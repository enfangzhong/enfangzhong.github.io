<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AnFrank</title>
  
  <subtitle>喜欢一个人，始于颜值，敬于才华，合于性格，久于善良，终于人品。W我真的好想你，在每一个雨季，你选择遗忘的，是我最不舍的。W不怕万人阻挡，只怕自己投降。W成熟是给你陌生人看的，逗比是给朋友看的，幼稚是给喜欢的人看的。W人生当苦无妨，良人当归即可。W逝者如斯乎，不舍昼夜。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://enfangzhong.github.io/"/>
  <updated>2019-09-22T07:49:13.751Z</updated>
  <id>http://enfangzhong.github.io/</id>
  
  <author>
    <name>AnFrank</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>个性化推荐算法实践第12章本课程回顾与总结</title>
    <link href="http://enfangzhong.github.io/2019/06/01/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E7%AC%AC12%E7%AB%A0%E6%9C%AC%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE%E4%B8%8E%E6%80%BB%E7%BB%93/"/>
    <id>http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第12章本课程回顾与总结/</id>
    <published>2019-06-01T14:03:11.000Z</published>
    <updated>2019-09-22T07:49:13.751Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="个性化推荐算法实践第12章本课程回顾与总结"><a href="#个性化推荐算法实践第12章本课程回顾与总结" class="headerlink" title="个性化推荐算法实践第12章本课程回顾与总结"></a>个性化推荐算法实践第12章本课程回顾与总结</h1><p>  推荐算法实战课程，课程是有问答专区，如果你有问题可以在问答专区提问，我会在每天固定时间解答课程，结合问答专区能够让你更快的掌握知识。</p><p>开始本章节的内容之前我们首先来回顾一下上一章节的内容，上一章节我们对之前所讲述过的排序部分的内容进行了总结与回顾。</p><p>本章节我们对课程所讲述过的全部内容进行总结与回顾，下面一起来看一下本章节的内容大纲。</p><p>一、个性化推荐算法离线架构</p><p>个性化推荐算法离线架构，无论是在个性化召回部分还是在个性化排序部分，我们都有一套离线处理，得到我们模型的流程，我们将这套流程的抽象一下给大家解析一下。</p><p>二、个性化推荐算法在线架构</p><p>无论是在个性化召回部分还是在个性化排序不分，我们都曾经将在线部分的架构呢，给大家讲解过，这里我们一起回顾一下。</p><p>三、本课程所讲述过的算法模型的内容回顾</p><h3 id="一、个性化推荐算法离线架构"><a href="#一、个性化推荐算法离线架构" class="headerlink" title="一、个性化推荐算法离线架构"></a>一、个性化推荐算法离线架构</h3><p>下面一起来看一下离线架构。</p><p><img src="/2019/06/01/个性化推荐算法实践第12章本课程回顾与总结/1566830509493.png" alt="1566830509493"></p><p>我们原始的日志呢，包含以下四个部分。</p><p>①用户的点击与用户的展示日志。</p><p>②我们记录用户信息的日志，这里包含用户的静态信息和统计的一些动态的信息，包括用户喜欢什么样的类型的内容等等。</p><p>③item的信息</p><p>④实时流式信息（Streaming）。这里的流式是指包含用户的一些实时行为。比如说用户订阅某个频道、用户将某些物品加入到购物车等等等等。</p><p>基于以上的日志，我们首先进行样本的筛选，我们将测试样本有噪声的样本的剔除掉得到我们的样本，得到样本之后呢，我们再进行特征选择。</p><p>在个性化召回部分，可能我们只是更多的需要我们的点击与展示数据。但是在我们的个性化排序部分，可能我们还需要用户的信息（user info），项目的信息（item info）以及上下文信息等等的一些信息。</p><p>基于我们的特征选择与样本选择之后呢，我们得到了我们的训练数据集以及测试数据，接下来无论是在召回部分还是在排序部分，我们都使用的训练数据集去训练模型，使用我们的测试数据去评估我们模型的表现。如果我们模型的表现达到了我们的要求，我们就将我们得到的模型文件的导出。</p><p>这里在个性化召回部分呢，可能是导出的直接用户的推荐结果，也可能是item相似度的列表亦或是我们的深度学习的模型文件。</p><p>在排序部分我们得到的模型文件的基本都是模型的实例化本身。像逻辑回归部分呢，其实我们只需要得到不同特征对应的参数即可。</p><h3 id="二、个性化推荐算法在线架构"><a href="#二、个性化推荐算法在线架构" class="headerlink" title="二、个性化推荐算法在线架构"></a>二、个性化推荐算法在线架构</h3><h5 id="1、Recall在线架构"><a href="#1、Recall在线架构" class="headerlink" title="1、Recall在线架构"></a>1、Recall在线架构</h5><p><img src="/2019/06/01/个性化推荐算法实践第12章本课程回顾与总结/1566831234657.png" alt="1566831234657"></p><p>说完了离线架构，下面我们来一起回顾一下在线架构的召回部分，大部分情况下我们召回部分得到的结果呢，是直接写入到KV存储当中了。用户访问我们的Server的时候呢，直接召回到自己对应的推荐结果，我们拿到推荐结果对应的id呢，从我们的Detail Server当中的获取Detail 传给我们的排序部分，但是呢在一些复杂场景情况下，我们比如训练了一些深度学习的模型，那么我们在用户访问我们的Server的时候呢，我们首先得需要拼接一下用户侧（user info）的特征，然后呢访问我们的Server得到用户的向量表示然后再进行召回。</p><h5 id="2、Rank在线架构"><a href="#2、Rank在线架构" class="headerlink" title="2、Rank在线架构"></a>2、Rank在线架构</h5><p>说完了个性化召回的在线架构的，下面我们来一起看一下排序部分在线架构。排序部分在线架构根据我们模型的不同的分为以下几种。</p><p><img src="/2019/06/01/个性化推荐算法实践第12章本课程回顾与总结/1566832185932.png" alt="1566832185932"></p><p>1、如果是像<strong>逻辑回归</strong>或者是<strong>GBDT</strong>这种浅层模型。我们这里的Rank Server可以直接将模型加载到内存当中，与我们的推荐引擎的进行服务的交互。用户访问我们的Server的时候，我们首先召回得到我们的候选集列表，对于每一个候选集列表，那我们去KV当中的拼一下我们的特征，就是获得我们用户侧（user info）的特征以及项目侧（item info）的特征包括一些上下文特征等等。我们将拼接的特征的传递给让给Rank Server。Rank Server用模型来进行下预测，将预测的结果呢再传递给我们的推荐引擎。推荐引擎的依据我们每一个item预测的得分的进行一下排序，这个顺序呢就是展示给用户的顺序。</p><p>2、如果我们这里采用像WD这样的深度学习模型的话，我们这里的Rank Server需要与我们的TF Server进行交互，这里的Rank Server相当于我们这里的请求的透传，并且返回的结果呢，也透传给我们的推荐引擎。</p><h3 id="三、本课程所讲述过的算法模型的内容回顾"><a href="#三、本课程所讲述过的算法模型的内容回顾" class="headerlink" title="三、本课程所讲述过的算法模型的内容回顾"></a>三、本课程所讲述过的算法模型的内容回顾</h3><p>回顾完了在线架构，我们来一起看一下本课程具体讲解了哪些算法与模型？</p><p>首先来看个性化召回部分，这里我们介绍了基于领域的CF、LFM、Person Rank 、Item2Vec、ContentBased。我们CF是在个性化推荐算法实战入门必修课里介绍的，入门必修课是一门免费的课程大家都可以看到。还有我们基于内容的推荐以及呢，我们这里基于深度学习的推荐Item2Vec。</p><p>我们在排序部分的介绍了浅层模型Rank LR（逻辑回归）,浅层模型组合GBDT。介绍了浅层模型不同模型的组合：LR+GBDT的混合模型。最后我们介绍了深度学习模型WD。以上的每一个算法或者模型呢，我们都是从物理意义数学公式推导，代码实战等几个方面来给大家介绍的，那么好了，本章节的内容到这里就全部结束了。本章节重点回顾了我们本次课程所讲述的全部内容。到这里本次个性化推荐算法实战课程的全部内容就结束了，非常感谢大家对于本课程的认真学习。祝大家在本次课程中学习一切顺利。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;个性化推荐算法实践第12章本课程回顾与总结&quot;&gt;&lt;a href=&quot;#个性化推荐算法实践第12章本课程回顾与总结&quot; class=&quot;headerlink&quot; title=&quot;个性化推荐算法实践第12章本课程回顾与总结&quot;&gt;&lt;/a&gt;个性化推荐算法实践第
      
    
    </summary>
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="个性化推荐算法实践总结" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="个性化推荐算法实践总结" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>个性化推荐算法实践第11章排序模型总结与回顾</title>
    <link href="http://enfangzhong.github.io/2019/06/01/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E7%AC%AC11%E7%AB%A0%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93%E4%B8%8E%E5%9B%9E%E9%A1%BE/"/>
    <id>http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第11章排序模型总结与回顾/</id>
    <published>2019-06-01T13:03:11.000Z</published>
    <updated>2019-09-22T07:48:10.875Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="个性化推荐算法实践第11章排序模型总结与回顾"><a href="#个性化推荐算法实践第11章排序模型总结与回顾" class="headerlink" title="个性化推荐算法实践第11章排序模型总结与回顾"></a>个性化推荐算法实践第11章排序模型总结与回顾</h1><ul><li>model在测试数据集效果回顾<br>1、逻辑回归模型、gbdt模型、gbdt模型与逻辑回归混合模型以及wd模型在测试数据集上的效果进行一下简单的回顾</li></ul><ul><li><p>LTR中特征维度浅析</p><p>2、我们会对工业界实际项目中建立排序模型所使用的特征进行一下简单的浅析。</p></li><li><p>工业界Rank技术展望<br>3、对工业界的排序技术进行一下展望</p></li></ul><h3 id="一、model在测试数据集效果回顾"><a href="#一、model在测试数据集效果回顾" class="headerlink" title="一、model在测试数据集效果回顾"></a>一、model在测试数据集效果回顾</h3><h5 id="1、效果回顾"><a href="#1、效果回顾" class="headerlink" title="1、效果回顾"></a>1、效果回顾</h5><p><img src="/2019/06/01/个性化推荐算法实践第11章排序模型总结与回顾/1566837900314.png" alt="1566837900314"></p><p>下面首先来回顾一下各模型在测试数据集上的表现。由于我们各Rank模型在线上实际使用时呢是对item进行打分，然后呢不同的item按照这个得分的展现给用户，所以呢我们这里更加关心的是模型对于我们item预测得分序的关系，所以呢也就是AUC我们这里从我们模型交叉验证得到的AUC以及模型在测试数据集表现的AUC两点的进行一下回顾。</p><p>我们在排序部分的重点介绍了四种模型分别是逻辑回归、GBDT、GBDT与逻辑回归的混合模型以及我们基于深度学习的WD模型。</p><p>首先呢从我们的模型交叉验证的效果来看一下GBDT与逻辑回归的混合模型的表现要好于GBDT。GBDT要好于我们的逻辑回归模型，由于我们的WD并没有采用我行交叉验证的方式呢，去评估，所以这里没有数据。好了，我们再来看一下训练好的模型呢，在测试数据集上的表现也就是看一下模型的泛化能力。这里同样的GBDT与逻辑回归的混合模型的是要好GBDT，GBDT是要比逻辑回归好，这里要注意一下，在实际的我们的项目中WD模型实际上是表现的最好，但是在这里限于我们的样本的数量了只有3万等等的一些限制。WD表现的要略低于GBDT。不过没有关系，如果大家有机会在实际项目海量的数据集当中了去实践一下，不同的Rank模型的话，大家一定能得到下面的结论，WD模型的表现是要好于GBDT与逻辑回归的混合模型。混合模型是要好于GBDT模型。GBDT是要好于我们的逻辑回归模型。但我实际工作中零一搭建个性化排序系统时得到的结论也是这样的。</p><h5 id="2、离线评估"><a href="#2、离线评估" class="headerlink" title="2、离线评估"></a>2、离线评估</h5><h6 id="模型交叉验证-model-cv"><a href="#模型交叉验证-model-cv" class="headerlink" title="模型交叉验证(model cv)"></a>模型交叉验证(model cv)</h6><p>在我们训练不同的排序模型，将不同的模型放到线上时，我们如何来评价离线的准入以及在线的收益呢？下面来看一下排序模型的评估，首先来看一下离线评估，第一点的，我们需要看一下模型交叉验证得到的指标，这些指标的，包括我们课上重点提到的AUC，以及准确召回等等的一些指标，这些指标我们需要明确它的物理意义，这样才能够帮助我们清晰地判断模型的效果。</p><h6 id="model-test-data-performance"><a href="#model-test-data-performance" class="headerlink" title="model test data performance"></a>model test data performance</h6><p>最终的我们还需要判断一下模型的泛化能力，也就是模型在测试数据集上的表现，比如模型在测试数据集上得到的AUC,得到的准确率等等，我们结合着不同的业务场景的也会有一些独自的评判标准，比如我们在信息流场景当中呢，我们可能更关注的是session的平均点击位置，这里简单的解释一下。我们每一个session展示了，比如说三条数据。</p><p><img src="/2019/06/01/个性化推荐算法实践第11章排序模型总结与回顾/1566838660693.png" alt="1566838660693"></p><p>在我们的测试数据上的原有的情况下，比如我们点击了这3条，经过我们训练模型对于这三个数据得重新打分之后呢，我们能否将已经击的这个第二条呢？学习到第一的位置，这样我们的平均点击位置的就更靠前了，这样的效果也就说明了是更好的。当然了，不同的业务场景还有一些其他的指标。</p><h5 id="3、在线评估"><a href="#3、在线评估" class="headerlink" title="3、在线评估"></a>3、在线评估</h5><h6 id="业务指标"><a href="#业务指标" class="headerlink" title="业务指标"></a>业务指标</h6><p>下面我们来看一下在线指标，首先呢是业务指标。比如说点击率，购买率，平均阅读时长，总的交易额度等等，我们根据不同的业务场景的制订了评价的指标，最终的结果呢也已在线AB测试得到的业务指标的生效为准。</p><h6 id="平均点击位置"><a href="#平均点击位置" class="headerlink" title="平均点击位置"></a>平均点击位置</h6><p>离线的评价指标的只是我们能否准入的一个衡量的标准，并不能决定我们在线实际效果的好坏，当然了这里还有一些辅助的评价指标，比如像之前我们介绍的平均点击位置。当然呢在线评估时，我们首先来看一下业务指标，其次呢是我们的辅助评价指标。好啦，在线离线的评估呢，我们已经说完了。</p><h3 id="二、LTR中特征维度浅析"><a href="#二、LTR中特征维度浅析" class="headerlink" title="二、LTR中特征维度浅析"></a>二、LTR中特征维度浅析</h3><h6 id="1、特征维度"><a href="#1、特征维度" class="headerlink" title="1、特征维度"></a>1、特征维度</h6><ul><li>特征维度</li></ul><p><img src="/2019/06/01/个性化推荐算法实践第11章排序模型总结与回顾/1566839442552.png" alt="1566839442552"></p><p>下面来看一下特征了有哪一些？我们的构建排序模型是常用的特征的有以下几个方面，我们来简单的介绍一下，用户侧的特征包含了用户的静态的属性，比如说年龄，性别，地域，还有一些简单的统计特征，比如该用户在我们平台上浏览过多少个商品？点击过多少个商品的购买过多少个商品的，购买过多少个商品啊？近30天浏览了多少商品的这种长短时的统计，最后还有一些用户侧的高维的特征，我们基于它的浏览点击购买历史的给他打上一些标签，比如说呢，她就喜欢某某品牌的香水某某品牌的鞋子等等。刚刚介绍用户特征侧时是以电商场景举例。对于其他的产品道理也是一样的，比如说信息流。我们只需要统计一下用户发现喜欢财经呢，还是喜欢体育，是喜欢娱乐呢？还是喜欢科技，甚至呢我们还可以给他打上标签儿，是科比的标签的还是鹿晗的标签等等？道理都是一样的。</p><p>商品侧的特征基础的特征包含商品的名称，商品的上线日期啊等等统计的特征的包含商品被购买的次数。商品的点击率呀，商品的购买率啊等等。一些高维的标签，那比如说这个商品的，他是深受90后欢迎啊，深受年轻女性的欢迎啊，我们的上下文的特征的有，当前是星期几呀？现在是几点呀？用户请求我们服务时所处的地理位置信息等等。用户和item的关系。比如说这个商品呢，是该用户两个月之前加入到购物车里头，一个月之前点击过的呀，半年之前购买过呀，等等的一些信息，还有我们的统计登录信息，比如说呢商品的上架的时间呢与购买率间的关系，比如说近一个月之内上架的商品打开的购买率是多少？近两个月等等我们统计出来显然的又增加了一维特征。好了我们曾经无数次说过特征与样本是决定我们最终这个整体表现的天花板，而我们采用不同的模型的只能去逼近这个天花板。像我刚才介绍这些不同维度特征时呢，我们是用电商场景举例的，但是呢实际的项目中呢，假如没有做过电商的场景，可以根据特征的大体由这五个维度来想到一些电商场景下应该有哪些特征？大家在自己的项目当中呢，或者自己解决实际问题过程中的，也一定要结合的实际去构造我们需要的特征。</p><h6 id="2、特征的数目"><a href="#2、特征的数目" class="headerlink" title="2、特征的数目"></a>2、特征的数目</h6><ul><li>特征的数目</li></ul><p>我们说过为了防止过拟合，那我们尽量要将特征与样本的数目来维持在1：100。举例，比如说我们这里有1000个训练样本，那么这里我选择了十个特征，这是没有问题的，但是呢，有的同学说我没有找到十个特征，我只找到了八个那也是没有问题的。</p><p>可能最终我们学习出来的效果不会很好。但是有的同学说我找到了50个特征，那么显然呢，这个模型呢就会过拟合。他在测试数据集上的表现的就会比较差，就是说它的泛化能力就不会很强。</p><h3 id="三、工业界Rank技术展望"><a href="#三、工业界Rank技术展望" class="headerlink" title="三、工业界Rank技术展望"></a>三、工业界Rank技术展望</h3><h6 id="1、多目标学习"><a href="#1、多目标学习" class="headerlink" title="1、多目标学习"></a>1、多目标学习</h6><p>我们知道了在信息流场景中的我们既想用户拿多点击，也就是点击率预估模型也想用户停留的阅读时长的要长一点。这样呢就是两个目标。之前的可能有很多方式呢，比如说训练两个模型，一个呢是点击率预估模型，一个是平均阅读时长预估模型，然后乘起来，比如说那像电商场景中的我们既想用户呢，他得购买率也就是说最终的转化率呢要高又想拿我们懂得交易额度也能高。有人呢对这种多目标问题的提出了一种将不同的目标的融合到一个网络里进行学习的方法。</p><h6 id="2、强化学习"><a href="#2、强化学习" class="headerlink" title="2、强化学习"></a>2、强化学习</h6><p>我们知道强化学习的是成功保证历史最大回报率的一种办法，现在呢这种算法呢，在游戏里应用的比较广泛也比较成熟，但排序的领域也有一些落地与尝试。希望大家的能够对这些较新的技术进行不断的追求，不断的学习。不断的探究，不断的尝试。那么本章节的内容到这里就全部结束了，本章节的重点是对之前多讲述过的排序部分的内容进行了总结回顾，下一章节我们将会对个性化推荐算法课程的内容来进行一下总结。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;个性化推荐算法实践第11章排序模型总结与回顾&quot;&gt;&lt;a href=&quot;#个性化推荐算法实践第11章排序模型总结与回顾&quot; class=&quot;headerlink&quot; title=&quot;个性化推荐算法实践第11章排序模型总结与回顾&quot;&gt;&lt;/a&gt;个性化推荐算法
      
    
    </summary>
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="排序模型总结" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="排序模型总结" scheme="http://enfangzhong.github.io/tags/%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep</title>
    <link href="http://enfangzhong.github.io/2019/06/01/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E7%AC%AC10%E7%AB%A0%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8BWideAndDeep/"/>
    <id>http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep/</id>
    <published>2019-06-01T12:03:11.000Z</published>
    <updated>2019-09-22T07:47:12.421Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep"><a href="#个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep" class="headerlink" title="个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep"></a>个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep</h1><p>欢迎来到本次个性化推荐算法实战课程，本课程是有问答专区，如果你有问题可以在问答专区提问，我会在每天固定时间解答，课程结合问答专区能够让您更快的掌握知识。</p><p>开始本章节的课程之前，我们首先来回顾一下上一章节的内容，上一章节我们重点讲述了树模型GBDT的数学原理，以及在测试数据集上代码实战了GBDT模型的训练，并且介绍了GBDT与逻辑回归的混合模型，那么本章节我们将重点介绍深度学习，在点击率预估方面的实战，分别介绍WD模型的数学原理，以及在测试数据集上代码实战WD模型的训练。</p><ul><li><p>背景介绍之深度学习</p></li><li><p>DNN网络结构与数学原理</p></li><li><p>WD(wide and deep)网络结构与数学原理</p></li></ul><p>下面我们首先来看一下本章节的内容大纲。1、背景知识介绍之深度学习，由于本章节所介绍的内容与之前我们所学习的浅层模型有较大的不同，所以我们首先介绍一下背景知识，什么是深度学习？2、DNN网络结构与数学原理，介绍完什么是深度学习，我们便选取一种具有代表性的网络DNN，来从它的数学原理如何进行参数学习来详解一下。3、WD网络结构与数学原理，WD模型便是我们所说的，使用深度学习来完成点击率预估实战所采用的模型，wd模型实际上是DNN与逻辑回归的混合模型，但是与之前我们所介绍过的GBDT与逻辑回归的混合模型不同，WD呢是联合训练的，所以我们要学习一下它的网络结构，并且详细的了解一下它的数学原理是如何做到联合训练的。</p><h3 id="一、背景介绍之什么是深度学习"><a href="#一、背景介绍之什么是深度学习" class="headerlink" title="一、背景介绍之什么是深度学习"></a>一、背景介绍之什么是深度学习</h3><h5 id="1-1什么是神经元"><a href="#1-1什么是神经元" class="headerlink" title="1.1什么是神经元?"></a>1.1什么是神经元?</h5><p>下面开始本小节的内容，本小节将重点介绍背景知识，什么是深度学习？深度学习，实际上是利用神经网络学习出一种非线性函数，该函数的输入是我们从训练数据中提取的特征，该函数的输出是训练数据所对应的label，那么说到这里，问题的核心就变成了什么是神经网络，在介绍神经网络之前，我们首先介绍一下它的组成成分，神经元，什么是神经元呢？下面来看一下，这里所说的神经元呢，实际上是指的是人工神经元，与我们生物上神经元的概念呢有所不同。但是呢，我们通过这个网络结构呢，实际上能够联想一下我们生物上的神经元，与该结构有极大的相似性。</p><p><img src="/2019/06/01/个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep/1566904703475.png" alt="1566904703475"></p><p>下面我们来看一下该结构，这里有几路输入，每一路输入了对应的输入的参数。也就是说我们最终的加权和是什么呢？我这里简单的写一下，加权和=$w_1<em>x_1+w_2</em>x_2+w_3<em>x_3+…+w_n</em>x_n$。加权和之后呢，还要做一下激活函数，这里的激活函数呢，主要是去线性化。常用的激活函数呢，我们在介绍逻辑回归模型的时候，也曾介绍过一种阶跃函数，大家应该还记得。这里的整体的网络结构呢与我们的逻辑回归模型的有一定的相似性。</p><h5 id="1-2激活函数"><a href="#1-2激活函数" class="headerlink" title="1.2激活函数"></a>1.2激活函数</h5><p>那么下面我们来看一下常用的激活函数有哪一些呢？</p><h6 id="1-2-1阶跃函数-sigmod"><a href="#1-2-1阶跃函数-sigmod" class="headerlink" title="1.2.1阶跃函数 sigmod"></a>1.2.1阶跃函数 sigmod</h6><p>第1种呢是我们比较熟悉的阶跃函数，该函数我们曾在逻辑回归里介绍过，该函数的取值范围呢是[0~1]，在x等于0时的函数值是0.5，他有一个特点那便是在x大于0的时候，很快函数值就趋向于1。x小于0的时候，函数值非常快的就趋向于0。</p><h6 id="1-2-2双曲正切"><a href="#1-2-2双曲正切" class="headerlink" title="1.2.2双曲正切"></a>1.2.2双曲正切</h6><p>第2种呢，是双曲正切，该函数的图像呢与阶跃函数几乎是一样的，只不过呢，它的取值范围呢是[-1,+1]，而且呢，该函数在取值为中间值变为最大值，以及取值为中间值变为最小值的速度呢，要比阶跃函数要快一些。</p><h6 id="1-2-3修正线性单元"><a href="#1-2-3修正线性单元" class="headerlink" title="1.2.3修正线性单元"></a>1.2.3修正线性单元</h6><p>第3种，修正线性单元，当输入大于零时，该函数的输出的是输入的本身，当输入小于零时，该函数的输出呢是0。在神经网络的参数学习中呢，如果我们采用之前讲述过的随机梯度下降的方法，修正线性单元函数能够更快的达到收敛。</p><h5 id="1-3-什么是神经网络？"><a href="#1-3-什么是神经网络？" class="headerlink" title="1.3 什么是神经网络？"></a>1.3 什么是神经网络？</h5><p><img src="/2019/06/01/个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep/1567153841330.png" alt="1567153841330"></p><p>好了介绍完了激活函数，下面我们来学习一下什么是神经网络。神经网络呢是由许多神经元组成的，我们先来简单的看一下网络结构，网络结构呢分为输入层，隐层以及输出层。</p><p>输出层呢，可以是一个也可以是多个。像我们做点击率预估呢，那便是一个。如果我们做分类呢是多分类的话，显然就是多个。有几个分类呢，也就有几个输出。输入指的是什么呢？输入指的是我们提取的特征，输入层呢，与隐层之间的采用的是全连接，也就是说我这里的隐层1与我输入层的每一个输入，都有参数相连。这里的隐层2呢，对于输入层的每一个输入呢，也都是有参数相连的。这里参数相连去加权求和之后呢，同样需要有激活函数来去线性化。</p><p>我们发现如果输入层与隐层不是全连接的话，而是一一连接的话，那么该网络也便回退到了我们之前讲述过的逻辑回归模型。为什么呢？很好理解，如果是一一连接的话，很像我们之前讲述过的w1与x1相乘，w2与x2相乘。</p><p>这样全连接呢，实际上相当于我们在逻辑回归模型里所做的特征交叉，但是呢，这里交叉的力度呢，会更强一点。</p><p>举个例子，如果我们这里只有三个输入特征，且并非全连接。对该隐层的第1个节点，我们让三个输入特征的前两个与它连接，实际上这就相当于完成了我们之前所做的两维特征的特征交叉。</p><p>而这里呢，由于每一个节点与之输入层都是全连接的，所以参数的规模呢要比之前大了非常多。我们来看一下一共有多少个参数呢？比如说我们的输入层一共有三个节点，隐层也是有三个节点，那么隐层的每一个节点，显然啦，与输入层之间的都是全连接，也就是都是三个参数，以及每一个隐层的节点呢，都需要一个偏执，所以这里总的参数呢，是3×3+3=12。而相同情景下逻辑回归的参数呢，只有三个，所以从参数量级上呢，神经网络还是要大很多的。</p><p>如果按当时我们训练逻辑回归模型所举的例子，输入特征100多维这里就按100维来算的话，隐层的节点如果有n个。那么这里的总参数便是（100n+n）也就是101n，而我们知道逻辑回归当中的这种情形下只有100个参数，所以呢，总体来讲参数的量级上呢，差了n倍，这个n呢是隐层的节点数目，也就是说隐藏的节点数目越多的话，参数的量级差距越大，神经网络能够学习到的隐含特征的也就越丰富。</p><h5 id="1-3深度学习与传统的机器学习有哪些流程的异同呢？"><a href="#1-3深度学习与传统的机器学习有哪些流程的异同呢？" class="headerlink" title="1.3深度学习与传统的机器学习有哪些流程的异同呢？"></a>1.3深度学习与传统的机器学习有哪些流程的异同呢？</h5><p>DL  DIFFS  ML</p><p><img src="/2019/06/01/个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep/1567157137223.png" alt="1567157137223"></p><p>介绍完了神经网络，下面我们来看一下深度学习与传统的机器学习有哪些流程的异同呢？首先呢，我们来看一下传统的机器学习。这里以我们所学习过的逻辑回归模型的训练为例，我们拿到训练数据之后呢，需要做特征工程，这里的特征工程呢，是指我们要挑选相应的特征，我们要做特征的离散化。归一化甚至呢，我们要区分连续特征以及离散特征不同的处理，最终呢，我们还要做特征的交叉等等，我们把特征工程处理好了之后呢，我们便得到了训练样本，训练样本呢，在传入的模型当中呢，去进行权重的学习，也就是参数的学习，最终呢，我们得到模型来完成结果的预测。</p><p>但是在深入学习中呢，我们只需要对输入的训练样本呢进行基础特征的抽取，而不再需要做繁琐的特征工程，繁琐的特征工程呢，相当于交由我们模型当中的多维参数来帮我们学习，这里呢，神经网络呢，就学习了这些复杂特征，进而呢在隐层之间呢，我们学习模型的参数，得到模型之后呢，我们便用来预测结果。好了本小节就全部结束了，本小节重点介绍了背景知识之什么是深度学习，下一小节我们将选取一种经典的深度神经网络DNN来介绍它的网络结构与数学原理。</p><h3 id="二、DNN网络结构与反向传播算法"><a href="#二、DNN网络结构与反向传播算法" class="headerlink" title="二、DNN网络结构与反向传播算法"></a>二、DNN网络结构与反向传播算法</h3><h5 id="2-1DNN网络结构"><a href="#2-1DNN网络结构" class="headerlink" title="2.1DNN网络结构"></a>2.1DNN网络结构</h5><p>开始本小节的课程之前，我们首先来回顾一下上一小节的内容，上一小节我们重点介绍的背景知识，什么是深度学习，那么本小节我们叫重点介绍，DNN网络结构以及DNN网络结构参数学习的数学原理，下面开始本小节的内容，下面我们来看一下DNN网络结构，DNN呢，实际上是深度神经网络与我们之前介绍过的，神经网络的结构呢，有相似性，也有不同的地方相似的地方呢。</p><p>相似的地方就是这里也分为三个大部分，第1大部分呢便是输入层，这是我们特征。多了的地方，第2大部分呢是隐层是我们基础特征的抽象到高阶特征，然后高阶特征之间参数不停的学习的过程。第3部分呢便是输出层，这里可以是一个节点，像我们在点击率预估问题中呢，便是一个输出，也可以是多个输出，像我们在多分类问题当中呢，便是多个输出。</p><p>但是这里与我们之前讲过的神经网络有不同的地方呢，便是我们的隐层呢，这里可以是多层，每一层的节点呢可以变得不同，好了这便是DNN网络结构。当然这里输入层与隐层，隐层与隐层，隐层与输出层之间的也都是全连接。</p><h5 id="2-2-DNN模型参数"><a href="#2-2-DNN模型参数" class="headerlink" title="2.2 DNN模型参数"></a>2.2 DNN模型参数</h5><h6 id="2-2-1-隐层的层数，每个隐层神经元的个数，以及激活函数"><a href="#2-2-1-隐层的层数，每个隐层神经元的个数，以及激活函数" class="headerlink" title="2.2.1 隐层的层数，每个隐层神经元的个数，以及激活函数"></a>2.2.1 隐层的层数，每个隐层神经元的个数，以及激活函数</h6><p>好了，下面让我们来看一下DNN模型当中有哪些重要的参数是值得我们注意的。首先呢，便是隐层的层数，每个隐层神经元的个数，以及激活函数。隐层的层数以及每个隐层神经元的个数呢，决定了网络的参数的量级，这里上一小节呢，我们曾经简单的举例过，三个维度特征的输入以及单隐层，三个隐层节点的话，它的参数呢是3×3+3，那么很明显我们这里可以以此类推，如果隐层与隐层之间的计算方式呢，也是这么计算，模型的参数的量级呢，是由隐层的层数以及每个隐层神经元的个数来决定的。当然了，还与我们输入特征的维度有直接的关系。激活函数的上一小节我们曾经介绍过三种，这里我们说过激活函数呢是决定我们参数收敛的快慢的。</p><h6 id="2-2-2-输入输出层的向量维度"><a href="#2-2-2-输入输出层的向量维度" class="headerlink" title="2.2.2 输入输出层的向量维度"></a>2.2.2 输入输出层的向量维度</h6><p>输入输出层的向量维度，如果是单维度的输出的话，像我们点击率预估这种问题就需要单维度的输出。如果是多维度的输出，像多分类问题呢，就需要多维度的输出。输入层的向量呢，是我们选定好基础特征之后呢，在完成，像字符串的哈希，然后做一个简单的Embedding或者说是我们这里将连续值呢进行分段离散等等的操作之后呢，我们得到的一个输入层的向量。</p><h6 id="2-2-3-不同层之间神经元的连接权重与偏移值B"><a href="#2-2-3-不同层之间神经元的连接权重与偏移值B" class="headerlink" title="2.2.3 不同层之间神经元的连接权重与偏移值B"></a>2.2.3 不同层之间神经元的连接权重与偏移值B</h6><p>我们需要学习的参数是什么呢？就是不同层之间神经元的连接权重，这里可能是输入层与隐层，隐层与隐层，隐层与输出层之间的连接权重，以及呢每一个节点上的偏移值，这是我们模型需要学习的参数。</p><h5 id="2-3-前向传播"><a href="#2-3-前向传播" class="headerlink" title="2.3 前向传播"></a>2.3 前向传播</h5><p><img src="/2019/06/01/个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep/1567182098732.png" alt="1567182098732"></p><p>下面我们来了解一下DNN模型的函数表达式，我们只要了解了网络的任何一个节点的输出值也便得到了模型的输出，因为模型的输出实际上就是输出层节点的输出值。</p><p>$a_{j}^{t}=f\left(\sum_{k} w_{j k}^{t} a_{k}^{t-1}+b_{j}^{t}\right)$<br>$z_{j}^{t}=\sum_{k} w_{j k}^{t} a_{k}^{t-1}+b_{j}^{t}$</p><p>下面我们来一起看一下公式，下面来解释一下公式，借助于一个简单的网络。t是指的这里的网络中的第t层，t-1呢是t层前面的一层。比如这一层，我们定义为第t层，那么这一层的前一层显然就是t-1层。</p><p>如果大家不好理解的话，可以想象一下，当t为0时也便是输入层。当t为1时，便是第一个隐层，当t为大T，便是输出层，这样也许就会好理解一点。a是指的每一个节点的激活值，这里的j表示的是第t个层上我们的这个第j个节点的激活值，这里我们可以把j想象成为1 2…..的节点。如果是1的话，那么便表示第t层上第1个节点的激活值，如果是2的话，并表示第t层上第2个节点的激活值。这里的w是指的第（t-1）层上第k个的节点指向第t层上j这个节点。这里的a同样是激活值，它表示的是第（t-1）层上第k个节点的激活值，b表示第t层上第j个节点的偏移值。如果这里我们求的是第1个节点的激活值的话，那么显然这里也便是第1个节点的偏移值好了。</p><p>下面我们以第t层上第1个节点的激活值求值来举例说明一下这个公式。这里呢，如果我们要求第t层上第1个节点的激活值，那么显然我们要依赖于第（t-1）层上，每一个节点的激活值。那么公式应该如下$w_{11}^{t}<em>a_{1}^{t-1}+w_{12}^{t}</em>a_{2}^{t-1}+w_{13}^{t}*a_{3}^{t-1}+b_{1}^t$，最终呢，我们还要加一个$b_{1}^t$偏执，这个偏执得到的加权求和呢。我们再过一下激活函数f，也便得到了我们的激活值。我们把加权求和没有经过激活函数的部分呢定义为$z_{j}^t$。$a_{j}^t$实际上也就是f(z)。</p><p>好了，经过我们的讲述呢，我们发现当模型的w与b，也就是所有的参数固定之后呢，我们输入层的特征输入之后，我们第1层的激活值是由我们的输入与w、d参数得到的。第2层呢是由我们第一隐层的激活值呢，与w、b参数达到的，我们这个过程呢是逐步向前去传播。我们把模型根据输入得到输出的过程呢，叫做前向传播。</p><h5 id="2-4-反向传播"><a href="#2-4-反向传播" class="headerlink" title="2.4 反向传播"></a>2.4 反向传播</h5><p>下面我们来学习一下DNN模型是如何学习我们的参数w与b的。</p><h6 id="2-4-1-Our-Target"><a href="#2-4-1-Our-Target" class="headerlink" title="2.4.1 Our Target"></a>2.4.1 Our Target</h6><p>$\frac{\partial L}{\partial w_{j k}^{t}} \quad \frac{\partial L}{\partial b_{j}^{t}}$</p><p>我们的目标是什么呢？我们的目标很简单，目标是求得损失函数，对模型中任意两层上两个节点连接的偏导，以及求得损失函数，对任意层上节点的偏置的偏导，如果我们得到了这两个偏导的话，我们发现我们就能将模型中的任意一个参数呢进行梯度下降，这样呢，经过数次迭代，我们最终就能将模型完成收敛，也便学习到了我们需要学的w与b。</p><h6 id="2-4-2-What-We-Have"><a href="#2-4-2-What-We-Have" class="headerlink" title="2.4.2 What We Have"></a>2.4.2 What We Have</h6><p>$\frac{\partial L}{\partial a_{j}^{T}} \quad \frac{\partial L}{\partial z_{j}^{T}}$</p><p>我们现在已知的是什么呢？我们现在已经知道了，是损失函数对于输出层节点激活值的偏导，这里的T表示的是输出层，为什么说我们已知道了，我们来详细的写一下公式，假使我们这里的loss函数呢是平方损失函数$(y-a_{j}^{T})^2$。对于每一个样本，我们的损失函数呢是这样的。我们发现以loss函数对于我们这里的输出层的输出激活值，去取偏导的话，很明显的，我们是能够得到答案的，也便是$-2（y-a_{j}^{T}）$。同样的这里我们知道了，loss函数对于最后一层节点输出激活值的偏导，也便知道了，我们这里的loss函数对于$z_{j}^T$的偏导，我们来推导一下。用一个链导法则。</p><p>$\frac{\partial L}{\partial a_{j}^{T}} \quad \frac{\partial a}{\partial z_{j}^{T}}$</p><p>a = f(z)</p><p>前一部分呢，是我们已经得到答案的，而后一部分呢，我们又曾经说过a呢，实际上等于我们的f（z）, 这里的f是激活函数，所以这一部分呢也很容易求的，所以我们说了我们知道了loss函数，对于最后一层节点激活值的偏导也便知道了loss函数对于z的偏导。</p><h6 id="2-4-3反向传播的推导"><a href="#2-4-3反向传播的推导" class="headerlink" title="2.4.3反向传播的推导"></a>2.4.3反向传播的推导</h6><script type="math/tex; mode=display">\frac{\partial L}{\partial b_{j}^{t}   }  =  \frac{\partial L}{\partial z_{j}^{t}  } * \frac{\partial z_{j}^{t}  }{\partial b_{j}^{t}   }=\frac{\partial L}{\partial z_{j}^{t}   }</script><p>$z_{j}^{t}=\sum_{k} \mathcal{W}_{j k}^{t} a_{k}^{t-1}+b_{j}^{t}$</p><p>$\frac{\partial L}{\partial w_{j k}^{t}}=\frac{\partial L}{\partial z_{j}^{t}} <em> \frac{\partial z_{j}^{t}}{\partial w_{j k}^{t}}=\frac{\partial L}{\partial z_{j}^{t}} </em> a_{k}^{t-1}$</p><p>下面我们来看一下是如何一步一步，通过我们已知的这些东西去进行推导的来看推导，这里我们的目标之一呢是求的loss函数，对于任意节点的偏移，它的偏导，好了这里我们应用一下链导法则，首先呢，我们对loss函数呢，求z值的偏导，既然呢对z值呢，求我们偏移的偏导。</p><p>大家应该对这个公式有印象，这个公式是前面我们说的前项传播的公式，所以这里我们看到z值对于b值的偏导呢，实际上是1，因为呢，在z对b偏转的过程中呢，前面这一部分呢相当于是常数。</p><p>$\frac{\partial L}{\partial b_{j}^{t}}=\frac{\partial L}{\partial z_{j}^{t}} * \frac{\partial z_{j}^{t}}{\partial b_{j}^{t}}=\frac{\partial L}{\partial z_{j}^{t}} $ </p><p>$\frac{\partial L}{\partial w_{j k}^{t}}=\frac{\partial L}{\partial z_{j}^{t}} <em> \frac{\partial z_{j}^{t}}{\partial w_{j k}^{t}}=\frac{\partial L}{\partial z_{j}^{t}} </em> a_{k}^{t-1}$<br>好了我们再来看一下，loss函数对于任意的网络中的w值，求偏导的整体过程，同样这里我们也采用链导法则，首先呢是loss函数对于z的偏导。z对于w的偏导。</p><p>$z_{j}^{t}=\sum_{k} \mathcal{W}_{j k}^{t} a_{k}^{t-1}+b_{j}^{t}$</p><p>我们看到z对于任意的w的偏导呢，显然呢，是这里的上一层k节点的激活值$a_{k}^{t-1}$，所以我们看到经过我们的推导呢，我们这里只需要知道loss函数对于任意节点的z值，它的的偏导我们也便得到了最终想要的答案，但是我们这里已知的是我们的loss函数对于我们输出节点的z值，所以说如果我们可以利用倒数第2层节点z值与输出层节点z值之间的关系，逐渐的将loss函数对于输入层节点的z值的偏导，传递的倒数第2层，进而呢由倒数第2层传递到倒数第3层，这样逐一的反向传播，我们便可以得到loss函数对于任意节点z值的偏导，继而我们便得到了loss函数，对于模型参数的偏导，也便完成了我们这里的学习。</p><h6 id="2-4-4-反向传播的核心部分"><a href="#2-4-4-反向传播的核心部分" class="headerlink" title="2.4.4 反向传播的核心部分"></a>2.4.4 反向传播的核心部分</h6><p>$\frac{\partial L}{\partial z_{j}^{t-1}}=\sum_{k} \frac{\partial L}{\partial z_{k}^{t}} \frac{\partial z_{k}^{t}}{\partial z_{j}^{t-1}}$<br>$z_{k}^{t}=\sum_{j} w_{k j}^{t} a_{j}^{t-1}+b_{k}^{t}$</p><p>$\frac{\partial z_{k}^{t}}{\partial z_{j}^{t-1}}=\frac{\partial z_{k}^{t}}{\partial a_{j}^{t-1}} \frac{\partial a_{j}^{t-1}}{\partial z_{j}^{t-1}}=w_{k j}^{t} \frac{\partial a_{j}^{t-1}}{\partial z_{j}^{t-1}}$</p><p><img src="/2019/06/01/个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep/1567185943400.png" alt="1567185943400"></p><p>好了下面我们来看推导的最核心的部分，我们说过现在问题的核心呢，变成了如何由损失函数对于输出层节点的。向前传播到倒数第2层，进而传播到倒数第3层，那么这里我们用一个普遍的公式，便是我们知道了第t层的z值的偏导，怎么由第t层z值的偏导呢去推导出第（t-1）层z值的偏导，如果我们知道了这个式子，问题也便解决了，普遍情况下的链导法的是损失函数对$z_k^t$求导，$z_k^t$对$z_j^{t-1}$求导即可。</p><p>为什么这里还有一个累加呢？我们借助于简单的网络来说明一下，我们看到这里上一层的任意节点，对下一层的每一个节点都有贡献，都有函数关系，所以我们在$z_k^t$对$z_j^{t-1}$，求偏导时，我们需要每一个节点都算一下偏导，所以这里出现了累加，好了我们将前向传播的式子呢，jk对调一下，我们之前讲前向传播时是求的第t层上第j个节点的a值或者z值。这里呢，我们是求第t层上第k个节点的z值，实际上我们只是把jk对调了一下即可。好了，我们这里呢，损失函数对第t层的z的偏导我们是知道的，因为呢，我们是从最后的T也就是输出层。往前传播的这里只需要求后一部分即可，我们采用链导法的，借助于激活值，我们看到呢后一部分是直接能够得到答案的，因为我们说过a等于f(z)，这里的f是激活函数。</p><p>同样呢，我们根据这个式子$\frac{\partial z_{k}^{t}}{\partial z_{j}^{t-1}}=\frac{\partial z_{k}^{t}}{\partial a_{j}^{t-1}} \frac{\partial a_{j}^{t-1}}{\partial z_{j}^{t-1}}=w_{k j}^{t} \frac{\partial a_{j}^{t-1}}{\partial z_{j}^{t-1}}$。前一部分呢，也是能够得到答案的，是这里的$w_{kj}^{t}$，既然这样的话，我们的整体流程也便窜了起来，由于我们这里损失函数对于z值偏等的过程中呢，是从后向前求的，所以我们这里的整体流程呢被称为反向传播。</p><h6 id="2-4-5方向传播的流程"><a href="#2-4-5方向传播的流程" class="headerlink" title="2.4.5方向传播的流程"></a>2.4.5方向传播的流程</h6><ul><li>对应输入x，设置合理的输入向量</li><li>前向传播逐层逐个神经元求解加权和与激活值</li><li>对于输出层求解输出层损失函数对于z值的偏导</li><li>反向传播逐层求解损失函数对z值的偏导</li><li>得到损失函数对于任意节点z值的偏导，也变得到了w与b的梯度</li></ul><p>好了，下面我们来看一下整体的反向传播流程是怎么样的，第1步对于从样本中获取的输入呢，我们经过哈希或Embedding等过程呢，设置好合理的输入向量维度，我们根据初始化的模型的参数w与b，逐层的前向传播，这样我们就得到了任意节点的加权和、z值与激活值a值。 然后我们对于输出层求解输出层损失函数对于z值的偏导，这我们是能够得到的，继而反向传播逐层求解损失函数对z值的偏导，这里我们是根据损失函数对$z^t$的偏导与损失函数对于$z^{t-1}$偏导之间的关系反向传播逐层得到的。最终呢，我们得到了损失函数，对于任意节点z值的偏导，也变得到了w与b的梯度，这里我们在推导的时候曾经详细的讲述过。</p><p>好了，我们得到了梯度之后呢，便完成了第1轮的迭代，之后呢，我们再逐次将反向传播的过程呢，不停的去进行，直到我们的参数收敛，模型也变训练好了。好了，本小节就到这里，本小节主要讲述了dnn的网络结构以及DNN模型求解的数学原理，下一小节我们将介绍wd模型的网络结构与数学原理。</p><h3 id="三、wide-and-deep的网络结构以及数学原理介绍"><a href="#三、wide-and-deep的网络结构以及数学原理介绍" class="headerlink" title="三、wide and deep的网络结构以及数学原理介绍"></a>三、wide and deep的网络结构以及数学原理介绍</h3><h5 id=""><a href="#" class="headerlink" title=" "></a> </h5><p>开始本小节的课程之前，我们首先来回顾一下上一小节的内容，上一小节我们重点介绍了dnn的网络结构以及数学原理，那么本小节我们将重点介绍wd的网络结构以及数学原理下面开始本小节的内容，下面来看一下本小节的内容会从哪几个方面展开。</p><p>1、wd的物理意义，首先会给大家介绍一下wd为什么会优于我们之前单独介绍过的逻辑回归模型，以及上一小节介绍过的深度神经网络。</p><p>2、wd的网络结构，会向大家展示一下wd模型是如果构造的。</p><p>3、wd的数学原理，我们会一起看一下反向传播算法在wd上是如何进行的。</p><h5 id="3-1-wd的物理意义"><a href="#3-1-wd的物理意义" class="headerlink" title="3.1 wd的物理意义"></a>3.1 wd的物理意义</h5><h6 id="论文：wide-amp-deep-learning-for-recommender-systems"><a href="#论文：wide-amp-deep-learning-for-recommender-systems" class="headerlink" title="论文：wide &amp; deep learning for recommender systems"></a>论文：wide &amp; deep learning for recommender systems</h6><p>好了下面我们来看一下wd的物理意义。wd出自于谷歌的论文，这篇论文我会在附件中提供给大家，如果大家需要可以读一下这篇论文呢，详述了wd的优点以及wd的网络结构，包括5个在wd上做的一些实验的结果好了下面我们来介绍一下wd为什么会优于我们之前单独介绍过的逻辑回归模型以及深度神经网络模型。</p><h6 id="Generalization-and-memorization-泛化与记忆"><a href="#Generalization-and-memorization-泛化与记忆" class="headerlink" title="Generalization and memorization 泛化与记忆"></a>Generalization and memorization 泛化与记忆</h6><p>首先呢，我们推荐系统当中呢有两个概念，泛化以及记忆，这里的泛化是指的我们推荐的多样性。记忆就比如说某人啊来到我们的推荐系统，一直点击宫斗类型的电视剧，那么这是我们的推荐系统也会一直给他推荐宫斗类型的电视剧，此过程我们称之为记忆，但是呢，这个人也有可能会喜欢历史类型的电视剧的多样性为泛化。我们如何将历史类型的电视剧的特征与该用户的特征在排序时呢，给他学到一个较高的参数呢，如果我们单纯的用逻辑回归的话，我们就需要组合特征，但是组合特征也有一个问题，如果我们的训练数据中没有该用户行为过历史题材电视剧的样本的话，我们的模型也是学不到组合特征对应的参数的，但是呢，深度神经网络DNN是可以的，我们曾经说过，深度神经网络呢可以将我们传入的基础特征进行高维的组合，在隐层当中呢，学出一些高维的特征，但是单独的深度神经网络也有一个问题，如果某用户的行为数据呢并不是十分的充分，那么我们学习的深度神经网络呢，可能会对该用户进行过于的泛化。推荐的结果呢，大多数是他不喜欢的好了，wd的便是能够结合逻辑回归的记忆能力，以及我们深度神经网络的泛化能力，很好的平衡了这两点。</p><h5 id="3-2-wd网络结构"><a href="#3-2-wd网络结构" class="headerlink" title="3.2 wd网络结构"></a>3.2 wd网络结构</h5><p><img src="/2019/06/01/个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep/1567188315016.png" alt="1567188315016"></p><p>下面我们来一起看一下wd的网络结构，wd的网络结构呢分为两大部分，一部分的是wide部分，这一部分和我们之前介绍过的逻辑回归模型呢，有极大的相似性，比如我们这里输入三个特征，那么它同样呢是将这三个特征呢与对应的参数组合$w1<em>x1+w2</em>x2+w3*x3$，第2部分呢是deep部分，deep部分与我们曾经介绍过的深度神经网络呢，几乎是一样的，这里也分为输入层以及隐藏，当然了，我们这里也是全连接的。</p><p>这里唯一与之前所介绍过的有所不同的是，wd模型的输出呢是将(wide侧的参数与特征的加权和)与(deep侧最后一个隐层的输出)，做相加在一起经过激活函数，最终得到我们模型的输出。</p><p>这样的结构呢，能够保证我们在每一次反向传播的过程中呢，不仅更新地deep侧的参数，同时呢也会更新wide侧的参数，这样也就保证了我们所说的联合训练。通常情况下，我们将离散特征以及离散特征的组合特征放入到wide侧，而我们将连续特征的放入到我们的深度神经网络侧，对于一些字符型的特征，我们通常是先做一下哈希，再做一下Embedding，然后传入到deep侧。</p><h5 id="3-3-模型的输出"><a href="#3-3-模型的输出" class="headerlink" title="3.3  模型的输出"></a>3.3  模型的输出</h5><p>好了，下面我们来一起看一下模型的输出。模型的输出，包含两部分。</p><p>第1部分的是我们的wide侧的输出，这里的x便是我们wide输入的特征，这里的cross便是特征的组合。</p><p>第2部分同样的我们这里的deep侧。倒数第2层节点的激活值也就是最后一个隐层的激活值，与我们输出节点与deep侧相连的w的乘积，再加上偏执。</p><p>最终2部分的加和呢，要过一下我们的激活函数，这里的激活函数呢是阶跃函数，但是在deep侧的隐层之间的参数学习是我们采用的激活函数是<strong>修正线性单元</strong>。</p><h5 id="3-4-WD-model的反向传播"><a href="#3-4-WD-model的反向传播" class="headerlink" title="3.4 WD model的反向传播"></a>3.4 WD model的反向传播</h5><p>大家一定要注意一下，好了下面我们来看一下反向传播是如何在wd上进行的。</p><h6 id="wide参数的学习过程"><a href="#wide参数的学习过程" class="headerlink" title="wide参数的学习过程"></a>wide参数的学习过程</h6><p>$\frac{\partial L}{\partial w_{w i d e j}}=\frac{\partial L}{\partial a^{T}} \frac{\partial a^{T}}{\partial z^{T}} \frac{\partial z^{T}}{\partial w_{w i d e j}}=\frac{\partial L}{\partial a^{T}} \sigma^{\prime}\left(z^{T}\right) x_{w i d e j}$</p><p>首先呢，我们先来看一下wide参数的学习过程，我们这里采用梯度下降的学习方法，所以呢，我们这里只需要求得，损失函数对于w和任意参数的偏导，也便能够进行参数学习，我们来看一下公式推导，这里采用链导法则。损失函数对w偏导，实际上也就是损失函数对输出层激活值的偏导在乘以激活值，对z值的偏导，在乘z值对参数w的偏大。</p><p>损失函数对输出层激活值的偏导，无论我们是采用平方分式函数，还是我们采用对数损失函数，我们曾经都详细的讲过，这一部分的值应该是多少，这里不太赘述。</p><p>激活函数的导数计算：我们知道a等于f（z），这里的f就是激活函数，所以呢，这一部分也便是激活函数的导数。我们把$\sigma^{\prime}\left(z^{T}\right) $放进去即可，</p><p>最后一部分的，由于我们输出层的输出。实际上我们说过是有两部分的由wide侧以及deep侧，显然呢，我们这里对于wide侧的参数的偏导与deep侧没有关系，也便是我们这里deep侧参数对应的特征。好了，该部分的推导那就讲到这里。</p><h6 id="WD-model的反向传播"><a href="#WD-model的反向传播" class="headerlink" title="WD model的反向传播"></a>WD model的反向传播</h6><p>$\frac{\partial L}{\partial z_{j}^{t-1}}=\sum_{k} \frac{\partial L}{\partial z_{k}^{t}} \frac{\partial z_{k}^{t}}{\partial a_{j}^{t-1}} \frac{\partial a_{j}^{t-1}}{\partial z_{j}^{t-1}}=\sum_{k} \frac{\partial L}{\partial z_{k}^{t}} w_{d e e p kj}^{t} \frac{\partial a_{j}^{t-1}}{\partial z_{j}^{t-1}}$<br>$z_{k}^{t}=\sum_{j} w_{d e e p k j}^{t} a_{j}^{t-1}+b_{k}^{t} \rightarrow t \neq T \quad z_{k}^{t}=\left(\sum_{j} w_{d e e p kj}^{t} a_{j}^{t-1}+b_{k}^{t}\right)+w_{\text {wide}} * X \rightarrow t=T$</p><p>下面我们来看一下deep侧参数的学习过程，与我们之前讲述过的dnn网络的反向传播是一样的，我们这里的核心呢都是损失函数，对于输出层z值的偏导，逐渐的前向传播，我们逐渐的得到，损失函数对倒数第2层z值的偏导逐次向前，这里唯一不同的是什么呢？</p><p>不同的是我们这里的前向传播的过程呢，如果当我们是最后一层时，我们这里会多出了一些wide侧的特征。</p><p>下面来看一下公式推导，这里我们在求上一层损失函数，对z值的偏导时，为什么这里会有一个累加呢？我们在上一小节曾经介绍过，这是因为（t+1）层上任意节点的激活值，都曾经被t层上第j节点贡献过，所以呢，我们需要累加。</p><p>好了，我们看到这3部分。</p><p>第1部分的，损失函数对于上一层节点的z值的偏导，这里我们是知道的，因为在最开始的时候呢，我们是知道，对输出层z值的偏导逐渐的向前传播，也便知道了t层。</p><p>第2部分，中间这一部分呢，根据下面我们两个式子，这两个式子我们分别发现的，虽然带我们的输出层的，我们有wide侧的特征，但是呢，并不影响我们这里求偏导，因为对于我们这里的偏导，wide侧的部分相当于常数，所以我们还是得到了相同的答案。</p><p>第3部分，最后一部分呢也是我们刚才说过的，a=f(z)。相当于呢，我们是激活函数的导数，也没有问题。我们得到了损失函数，对于任意层结点的z值的偏导。</p><p>$\frac{\partial L}{\partial b_{j}^{t}}=\frac{\partial L}{\partial z_{j}^{t}} * \frac{\partial z_{j}^{t}}{\partial b_{j}^{t}}=\frac{\partial L}{\partial z_{j}^{t}} $</p><p>$ \frac{\partial L}{\partial w_{j k}^{t}}=\frac{\partial L}{\partial z_{j}^{t}} <em> \frac{\partial z_{j}^{t}}{\partial w_{j k}^{t}}=\frac{\partial L}{\partial z_{j}^{t}} </em> a_{k}^{t-1}$</p><p>根据上一小节我们讲过的，此时我们便得到了损失函数，对于deep侧对于每一个偏执以及每一个w的偏导，这样我们便能够进行我们的梯度下降，进行参数学习了。好了，这就是wd模型的反向传播。</p><h5 id="3-5server架构"><a href="#3-5server架构" class="headerlink" title="3.5server架构"></a>3.5server架构</h5><p><img src="/2019/06/01/个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep/1567191012618.png" alt="1567191012618"></p><p>下面我们来看一下，我们得到了最终的模型之后呢，我们是如何与推荐引擎呢进行交互的，这里由于我们采用的是TensorFlow呢，实现我们的wd模型，这里我们必须搭一个TensorFlow serving来提供我们的深度学习的rank服务。</p><p>此时呢，推荐引擎呢，需要发动请求到rank server，rank server与TensorFlow serving呢，进行一次交互，将请求呢透传的TensorFlow serving，TensorFlow serving得到的结果呢，返回给我们这里的rank server。rank server再将结果呢透回给我们这里的推荐引擎，来完成每一个item对该user的得分，进而完成排序，我们之前讲述过的浅层模型是rank server可以直接将我们得到的模型文件的load的内存当中，然后利用API呢，去提供打分。</p><p>好了，本小节的内容就到这里，本小节重点介绍了wd的网络结构以及数学原理，那么下一小节我们将代码实在wd模型。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep&quot;&gt;&lt;a href=&quot;#个性化推荐算法实践第10章基于深度学习的排序模型WideAndDeep&quot; class=&quot;headerlink&quot; title=&quot;个性化推荐算法实践
      
    
    </summary>
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="WideAndDeep" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/WideAndDeep/"/>
    
      <category term="逻辑回归" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/WideAndDeep/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="神经网络" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/WideAndDeep/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="逻辑回归" scheme="http://enfangzhong.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="WideAndDeep" scheme="http://enfangzhong.github.io/tags/WideAndDeep/"/>
    
      <category term="神经网络" scheme="http://enfangzhong.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>个性化推荐算法实践第07章综述学习排序</title>
    <link href="http://enfangzhong.github.io/2019/06/01/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E7%AC%AC07%E7%AB%A0%E7%BB%BC%E8%BF%B0%E5%AD%A6%E4%B9%A0%E6%8E%92%E5%BA%8F/"/>
    <id>http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第07章综述学习排序/</id>
    <published>2019-06-01T09:03:11.000Z</published>
    <updated>2019-09-22T07:49:38.353Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="个性化推荐算法实践第07章综述学习排序"><a href="#个性化推荐算法实践第07章综述学习排序" class="headerlink" title="个性化推荐算法实践第07章综述学习排序"></a>个性化推荐算法实践第07章综述学习排序</h1><p>综述学习排序的思路，并介绍工业界排序架构以及本课程重点讲解的学习排序模型。</p><h3 id="一、什么是学习排序（Learn-To-Rank）？"><a href="#一、什么是学习排序（Learn-To-Rank）？" class="headerlink" title="一、什么是学习排序（Learn To Rank）？"></a>一、什么是学习排序（Learn To Rank）？</h3><p>说起学习排序，首先介绍一下排序，排序是在搜索场景以及推荐场景中应用的最为广泛的。</p><p>传统的排序方法是基于构造相关度函数，使相关度函数对于每一个文档进行打分，得分较高的文档，排的位置就靠前。但是，随着相关度函数中特征的增多，使调参变得极其的困难。所以后来便将排序这一过程引入机器学习的概念，也就变成这里介绍的学习排序。</p><p>那么这里介绍的排序都是学习排序中的<strong>Pointwise</strong>，指对于单独的文档进行预估点击率，将预估点击率最大的文档排到前面。</p><p>所以特征的选择与模型的训练是至关重要的。</p><p><strong>那么什么是学习排序呢？</strong></p><p>学习排序：将个性化召回的物品候选集根据物品本身的属性结合用户的属性，上下文等信息给出展现优先级的过程便是学习排序。</p><p>下面用一个例子进行展示：</p><p><img src="/2019/06/01/个性化推荐算法实践第07章综述学习排序/1564992344840.png" alt="1564992344840"></p><p>假设这里有一个用户A，基于他的历史行为给出了召回，可能是很多种召回算法，经过合并之后得到的6个item[a,b,c,d,e,f]。经过排序，最终将这6个item的优先级固定为c、a、f、d、b、e。</p><p>得到优先级的过程就是由排序得到的。分别根据item本身的属性，以及user当前的一些上下文和user固定的一些属性，得到此时最佳的顺序应该是将c给展示，这样以保证最后的点击率最高。</p><h3 id="二、排序在个性化推荐系统中的重要作用"><a href="#二、排序在个性化推荐系统中的重要作用" class="headerlink" title="二、排序在个性化推荐系统中的重要作用"></a>二、排序在个性化推荐系统中的重要作用</h3><p>之前介绍过，在个性化的算法中后端的主要流程是：召回—&gt;排序—&gt;策略调整。</p><p>我们说过，召回决定了推荐效果的天花板，那么排序就决定了逼近天花板的程度。</p><p>1、排序决定了最终的推荐效果</p><p>用户看到的顺序基本就是由排序这一步骤所决定的，如果用户在前面的位置就能够看到自己感兴趣的物品，那么用户就会在推荐系统总停留较长的时间；反之，如果需要用户几次刷新之后，才能得到自己想要的物品，那么用户下一次将不会在信任推荐效果，导致在推荐系统中停留的时间较短。</p><p>在工业界中，排序这一部分分为三个步骤：</p><p><img src="/2019/06/01/个性化推荐算法实践第07章综述学习排序/1564992495527.png" alt="1564992495527"></p><p>（1）prerank（预排序）</p><p>也就是排序之前的部分，由于排序的模型由浅层模型切换到深层模型的时候，耗时在不停的增加。比如之前召回可以允许有5000个物品去做浅层模型，比如说逻辑回归，就是训练出一组参数，那么整体的打分过程耗时很短。但是，如果当排序模型切换到深层模型，比如说DNN，那么整体需要请求一次新的深度学习的服务，那么这5000个item去请求的时间显然是不能承受的。所以要先有一个<strong>粗排</strong>。这个粗排会将这5000个召回的物品进行第一次排序，将候选集缩小到一定范围之内。这样使排序模型的总处理时间满足系统的性能要求。粗排往往以一些简单的规则为主，比如说使用<strong>后验CTR</strong>或者说对于新的物品使用入库时的<strong>预估CTR</strong>等等。</p><p>（2）Rank（主排序）</p><p>主排序部分就是重点部分，现在业界比较流行的还有一次重排（ReRank）。</p><p>主排序模型的分类：</p><p>a. 单一的浅层模型：浅层模型是相较于深度模型而言的，浅层模型的代表有LR(逻辑回归)、FM。</p><p>这一类模型在学习排序初期是非常受欢迎的，因为模型线上处理时间较短，所以它支持特征的维度就会非常的高。但是也存在很多问题：比如像LR模型，需要研发者具有很强的样本筛选以及特征处理能力，这个包含像特征的归一化、离散化、特征的组合等等。</p><p>所以，后期发展了浅层模型的组合。</p><p>b. 浅层模型的组合</p><p>这里比较著名的树模型的组合：GBDT组合，LR+GBDT等等。这一类模型不需要特征的归一化、离散化，能够较强的发现特征之间的规律，所以相较于单一的浅层模型具有一定的优势。</p><p>c. 深度学习模型</p><p>随着深度学习在工业界应用的不断成熟，以及像tensorflow等深度学习框架的开源，现在工业界大部分的主排序模型都已经切换到了深度学习模型。</p><p>（3）Rerank（重排序）</p><p>这个重排是将主排序的结果再放入一个类似于session model或者说是强化学习的一个模型里面去进行一个重排序，这种主要是突出了用户最近几次行为的session特征，将与最近几次session内用户行为相近的item给优先的展示，以便获取用户行为的连续性。</p><blockquote><h1 id="KDD2018-电商搜索场景中的强化排序学习：形式化、理论分析以及应用http-www-sohu-com-a-244970525-129720"><a href="#KDD2018-电商搜索场景中的强化排序学习：形式化、理论分析以及应用http-www-sohu-com-a-244970525-129720" class="headerlink" title="KDD2018 | 电商搜索场景中的强化排序学习：形式化、理论分析以及应用http://www.sohu.com/a/244970525_129720"></a>KDD2018 | 电商搜索场景中的强化排序学习：形式化、理论分析以及应用<a href="http://www.sohu.com/a/244970525_129720" target="_blank" rel="noopener">http://www.sohu.com/a/244970525_129720</a></h1></blockquote><p>由于单一item在重排模型的耗时要比主模型长很多，所以重排部分只是会影响主排序头部的一些结果，比如说top 50 的结果去进行一个重排。那么既然是这样的话，可以看到，最能影响结果的还是主排序模型。</p><h3 id="三、工业界推荐系统中排序架构解析"><a href="#三、工业界推荐系统中排序架构解析" class="headerlink" title="三、工业界推荐系统中排序架构解析"></a>三、工业界推荐系统中排序架构解析</h3><p>工业界中排序是如何落地的。</p><p><img src="/2019/06/01/个性化推荐算法实践第07章综述学习排序/1565100075461.png" alt="1565100075461"></p><p>算法的后端主流程是：召回之后排序。</p><p>召回完item之后，我们将item集合传给排序部分，排序部分会调用打分框架，得到每一个item在当前上下文下，对当前user的一个得分，进而根据得分决定展现顺序。</p><p>下面看一下打分框架内部的构成：</p><p>首先会将每一个item以及user去提取特征，注意这里提取的特征要与离线训练模型的特征保持一致。提取完特征之后，我们向排序服务发出请求，排序服务会返回给我们一个得分，推荐引擎会基于此得分完成排序。经过简单的策略调整之后，展现给用户。</p><p>这里需要特别注意的是，排序服务与离线训练好的排序模型之间的通信。</p><p>如果是单一的浅层模型，像LR，那么可以直接将训练好的模型参数存入内存。当排序服务需要对外提供服务的时候，直接加载内存中模型的参数即可。像FM以及GBDT等等，我们只需要离线训练好模型，将模型实例化到硬盘当中。在在线服务当中，由于这些模型都有相应的库函数，他们提供了模型的加载以及模型对外预测等一系列接口，所以便可以完成打分。</p><p>但是，对于像深度学习的话，我们在训练完成之后，我们还需要提供一个深度学习的服务供排序服务调用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;个性化推荐算法实践第07章综述学习排序&quot;&gt;&lt;a href=&quot;#个性化推荐算法实践第07章综述学习排序&quot; class=&quot;headerlink&quot; title=&quot;个性化推荐算法实践第07章综述学习排序&quot;&gt;&lt;/a&gt;个性化推荐算法实践第07章综述学
      
    
    </summary>
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="Learn To Rank" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/Learn-To-Rank/"/>
    
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="Learn To Rank" scheme="http://enfangzhong.github.io/tags/Learn-To-Rank/"/>
    
  </entry>
  
  <entry>
    <title>个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍</title>
    <link href="http://enfangzhong.github.io/2019/06/01/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E7%AC%AC06%E7%AB%A0%E4%B8%AA%E6%80%A7%E5%8C%96%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%E4%B8%8E%E5%9B%9E%E9%A1%BE/"/>
    <id>http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第06章个性化召回算法总结与回顾/</id>
    <published>2019-06-01T08:03:11.000Z</published>
    <updated>2019-09-22T07:41:44.328Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍"><a href="#个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍" class="headerlink" title="个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍"></a>个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍</h1><p>本章节重点总结前面几章节介绍过的个性化召回算法。并介绍如何从离线与在线两个大方面评估新增一种个性化召回算法时的收益。</p><h3 id="一、个性化召回算法的总结"><a href="#一、个性化召回算法的总结" class="headerlink" title="一、个性化召回算法的总结"></a>一、个性化召回算法的总结</h3><p>这里会将之前介绍过的几种算法进行归类，并简短介绍每一种个性化召回算法的核心原理；同时演示工业界中多种召回算法共存的架构。</p><p>下面看一下之前讲过的个性化召回算法的分类：</p><ol><li>基于邻域的：CF、LFM、基于图的推荐personal rank</li></ol><p>item-CF是item根据user的贡献，得到item的相似度矩阵，用户根据用户点击过的item的相似item来完成推荐。</p><p>user-CF是user根据item的贡献，得到user的相似度矩阵，用户将根据相似用户点击过的物品完成自己的推荐。</p><p>LFM是根据user-item矩阵，将矩阵分解，从而得到user与item的隐向量，将两个向量点乘得到的数值取top k 便完成了推荐。</p><p>psersonal rank是根据user与item的二分图，在这个二分图之间随机游走，便得到物品对固定用户的倾向度，把这个倾向度得分叫做PR值。那么取PR值的top k也就完成了用户的推荐。</p><ol><li>基于内容的：content-based算法</li></ol><p>content-base算法的主要流程是：</p><p>首先将item进行刻画，然后将user进行刻画，然后将user的刻画与item的刻画在线上推荐的时候串联起来。</p><ol><li>基于神经网络(Neural network)的：item2vec</li></ol><p>item2vec首先根据用户的行为得到由item得到的句柄，根据训练语料得到item embedding的向量，得到这个向量之后就能得到item的相似度矩阵。从而根据用户的历史点击推荐相似的item给用户，也就完成了推荐。</p><p>下面看一下工业界中推荐系统中多种召回并存的架构：</p><p>后端算法的核心逻辑：</p><p><img src="/2019/06/01/个性化推荐算法实践第06章个性化召回算法总结与回顾/1564988526824.png" alt="1564988526824"></p><p>首先是召回，召回之后是排序，排序之后是策略调整，然后就将结果返回给web层。</p><p>接下来看一下，具体在召回阶段是如何多种算法并存的。</p><p><img src="/2019/06/01/个性化推荐算法实践第06章个性化召回算法总结与回顾/1564988665653.png" alt="1564988665653"></p><p>比如这里的算法A，召回了两个item，分别是a、b；算法B召回了3个item，分别是a、c、d；同理算法C召回了4个item，分别是e、f、d、c。</p><p>那么每一种算法召回的数目是如何确定的呢？</p><p>这里有两种形式：</p><p>形式一：为了满足rank阶段的性能要求，这里指定召回阶段召回的数目，比如说50个，那么各种算法根据以往的表现来平分这50个，每一个算法有一个比例，比如说算法A是0.2，算法B是0.3，算法C是0.5。这样每一个算法也就有了自己召回的上限。 </p><p>形式二：rank阶段毫无性能压力，我们给算法A写了多少个推荐都能全部召回，其余算法也是相同的处理。</p><p>在召回完成之后，我们需要进行一个合并。合并完成之后，我们得到item a~f，将重复召回的进行去重，但是也会给item a标记上它同时是属于算法A和算法B召回的。召回完成之后，这些item进入排序阶段。</p><h3 id="二、个性化召回算法的评价"><a href="#二、个性化召回算法的评价" class="headerlink" title="二、个性化召回算法的评价"></a>二、个性化召回算法的评价</h3><p>在现有的个性化召回体系下，如果要新增一种个性化召回算法，需要知道这种个性化召回算法会对系统造成怎样的影响，是正向收益还是负向收益。所以需要从离线和在线两个方面对个性化召回算法进行评价。</p><p>离线评价准入：</p><p>也就是说，在我们新增一种个性化召回算法的时候，我们离线选取了一部分训练文件来训练个性化召回算法的模型。我们根据这个模型得到了一些推荐结果，同时有必要保留一些测试集。在测试集上评价推荐结果的可靠程度。这个可靠程度首先是要有一个预期，这个算法会给线上带来正向还是负向的收益。</p><p>当然，最终的结果仍然需要在线上生产环境中去评价真实的受益，也就是做A/B test。</p><p>如何在离线进行评价的？</p><p>评价方法：评测新增算法推荐结果在测试集上的表现。</p><p>这里用一个例子来具体说明：</p><p><img src="/2019/06/01/个性化推荐算法实践第06章个性化召回算法总结与回顾/1564989385352.png" alt="1564989385352"></p><p>如果新增了某种个性化召回算法，对于user A我们给出了推荐结果a、b、c。恰巧这里我们获得了user A在测试集上的展现数据，就是a、b、c、m，那么在这里我们发现有3个是重合的，也就是a、b、c，那么这3个就是分母，如果我们在得到了用户A在测试集上的点击数据，这个点击数据恰好是a、c。我们发现这里的推荐结果是a、b、c是分母，然后有两个被点击了，那么a、c就是分子，最后的点击率就是 2/3。</p><p>如果这个数据是高于基线的点击率的话，那么就可以将这种推荐算法放到线上做A/B test(A/B测试)。</p><p>当然了，我们知道线下的评价结果与线上真实环境中的结果是有差异的，但是这种方式是能够给我们一个最基础的、直观的评判，是否可以准入到线上。</p><p>这里简单解释一下什么是测试集？</p><p>举例：如果我们要使用itemCF这种个性化召回算法，那么我们首先需要计算item的相似度矩阵，我们这里以过去一周的用户的真实的展现与点击数据为依据来训练这个相似度矩阵。我们只使用周一到周五的数据来训练，周六、周日的数据便是这里的测试集。</p><p>在线评价收益：A/B test</p><p>线上的评价分为两步：</p><p>（1）定义指标：</p><p>这里需要根据不同的情况，比如说在信息流场景下，我们最关心的就是点击率，平均阅读时长等等指标；但是在电商系统中，我们可能更加关注的是转化率及总的交易额度。</p><p>总之要根据自己的产品，来找到最能够评价产品的核心指标。</p><p>（2）生产环境A/B test：</p><p>往往采用以划分user id尾号的形式，比如说分出1%的流量在原来的个性化召回体系框架上增加要实验的个性化召回算法。实验几天之后，与基线去比较核心指标的优劣。如果收益是正向的，我们就保留。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍&quot;&gt;&lt;a href=&quot;#个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍&quot; class=&quot;headerlink&quot; title=&quot;个性化推荐算法实践第06章个性化召回算
      
    
    </summary>
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="评估指标" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/"/>
    
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="评估指标" scheme="http://enfangzhong.github.io/tags/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/"/>
    
  </entry>
  
  <entry>
    <title>个性化推荐算法实践第05章基于内容的推荐方法ContentBased</title>
    <link href="http://enfangzhong.github.io/2019/06/01/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E7%AC%AC05%E7%AB%A0%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E6%8E%A8%E8%8D%90%E6%96%B9%E6%B3%95ContentBased/"/>
    <id>http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第05章基于内容的推荐方法ContentBased/</id>
    <published>2019-06-01T07:03:11.000Z</published>
    <updated>2019-09-22T07:41:14.466Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="个性化推荐算法实践第05章基于内容的推荐方法Content-Based"><a href="#个性化推荐算法实践第05章基于内容的推荐方法Content-Based" class="headerlink" title="个性化推荐算法实践第05章基于内容的推荐方法Content Based"></a>个性化推荐算法实践第05章基于内容的推荐方法Content Based</h1><p>本章节重点介绍一种基于内容的推荐方法content based。从content based算法的背景与主体流程进行介绍。并代码实战content based算法。</p><h1 id="第一部分：基于内容的推荐的理论知识部分"><a href="#第一部分：基于内容的推荐的理论知识部分" class="headerlink" title="第一部分：基于内容的推荐的理论知识部分"></a>第一部分：基于内容的推荐的理论知识部分</h1><h3 id="一、个性化召回算法Content-based背景介绍"><a href="#一、个性化召回算法Content-based背景介绍" class="headerlink" title="一、个性化召回算法Content based背景介绍"></a>一、个性化召回算法Content based背景介绍</h3><p>之前博文讲到的CF、LFM、Personal Rank都同属于基于领域的推荐。item2vec属于基于深度学习的推荐。</p><p>基于内容推荐不同于之前讲过的任何一种个性化召回算法，属于独立的分支。我们有必要去了解该算法出现的背景。</p><ol><li>思路简单，可解释性强</li></ol><p>任何一个推荐系统的初衷，都是推荐出用户喜欢的item。</p><p>基于内容的推荐，恰恰是根据用户的喜好之后，给予用户喜欢的物品。</p><p>eg：某一个用户经常点击体育类的新闻，那么在这个用户下一次访问这个网站系统的时候，自然而然的给用户推荐体育类型的新闻。那么对于推荐结果可解释性非常的强。</p><ol><li>用户推荐的独立性</li></ol><p>基于内容的推荐，推荐的结果只与该用户本身的行为有关系，其余用户的行为是影响不到该用户的推荐结果的。但是，联想一下，之前提到的无论是CF还是LFM、personal rank以及item2vec，其余用户的行为都会一定程度上，或多或少的干预到最后的推荐结果。</p><ol><li>问世较早，流行度高</li></ol><p>由于基于内容推荐思路的极简性，可解释性，所以它出现的非常早；并且，无论是在工业界，还是研究界，都最为一种基础的算法，流行度非常的高。</p><p>但是，任何事物都是有两面性的，基于内容的推荐不是说是完美的，它同样有一些非常明显的缺点。</p><p>（1）对于推荐的扩展性较差：也就是说，如果一个用户之前经常访问体育类型的新闻，那么在之后的推荐之中，倾向于在体育范围内不断地挖掘；<strong>很难完成跨领域的物品推荐。</strong></p><p>（2）<strong>需要积累一定量的用户的行为</strong>，才能够完成基于内容的推荐。</p><h3 id="二、Content-based算法的主体流程介绍"><a href="#二、Content-based算法的主体流程介绍" class="headerlink" title="二、Content-based算法的主体流程介绍"></a>二、Content-based算法的主体流程介绍</h3><p>实际上该算法的主体流程大部分不属于个性化推荐的范畴，应该从属于NLP或者用户画像的范畴。只有极小部分属于个性化推荐算法实战的范畴。</p><ol><li>item profile：对item的刻画</li></ol><p>针对于基于内容的推荐下，对item的刻画大体可以分为两大类：（1）关键词刻画；（2）类别的刻画。</p><p>比如，在信息流场景下，我们需要刻画出这篇新闻属于财经还是娱乐；那么在电商场景下也是一样的，我们需要刻画出这个物品它属于图书还是说属于母婴，具体的关键词上也会有这个图书是数据机器学习的还是人文情感的，这个物品是参与满减的，还是参与包邮的等等。</p><p>第一步完成内容的物品刻画之后，第二步需要对用户进行刻画。</p><ol><li>user profile</li></ol><p>传统范畴的用户画像是比较宽泛的，它不仅包含了用户的动态特征，还包含了它的一些静态特征。</p><p>而我们用在基于内容推荐里的更多的是聚焦在用户的长期、短期行为，进而通过行为的分析将用户感兴趣的topic、或者用户感兴趣的类别给予刻画。</p><p>那么，有了item的刻画，有了user的刻画，第三步就是在线上完成个性化推荐的过程。</p><ol><li>online recommendation</li></ol><p>给用户推荐他最感兴趣的一些topic，或者说一些类别。</p><p>假设某个用户经常点击明星新闻，当用户访问系统的时候，我们应该明星最新的新闻最及时的推荐给用户，这样点击率自然很高。</p><p>那么经过这三步流程的分析，可以发现：实际上，前两步更多的同属于NLP或者说是用户画像的范畴，第三步更多的是我们个性化推荐的内容实战范畴。</p><p>下面将每一部分的技术要点进行解析：</p><p>（1）item profile </p><p>a. Topic finding（Topic 发现）：首先要选定特征，这里的特征是title和内容主体的分词，那么得到词语的分词之后，针对于topic的发掘采用命名实体识别的方式。这个命名实体识别的方式可以去匹配关键词词表，那么得到了关键词之后，我们需要对这些关键词进行一定的排名，那么将排名最高的top 3 或者top 5给 item 完成label。</p><p>至于这里的排名，会使用一些算法和规则，算法诸如：TF-IDF，规则是基于自己的场景总结出来的修正错误case的一些规则。</p><p>b. Genre Classify（类别的划分）：首先选定好特征，这里同样是利用一些文本信息，比如说title，分词（正文中所有的去过标点，去过停用词）得到的词向量，这里词向量在浅层模型中可以直接one-hot编码，在深层模型中，首先可以先进行一个embedding，这里使用的分类模型主要是像LR、GBDT、CNN等等。</p><p>分类器的使用，是使用多种分类器，分别占不同的权重，然后对结果进行一个线性的加权，从而得到正确的分类。</p><p>以上是针对于文档的topic 发掘或者说是类别的分类进行的叙述。那么对于短视频，实际上现在引入了一些更多的特征，比如关键帧所对应图像的分类识别，以及音频所对应的语音识别后，文字的处理等一些有意义的尝试。</p><p>（2）user profile</p><p>a. Genre/Topic（类别的划分）（Topic 发现）： </p><p>一个层面是用户对哪些<strong>种类</strong>的新闻或者是物品感兴趣；另一个层面是对哪些<strong>关键词</strong>感兴趣。</p><p>现在多是基于统计的方式，业界也在做一些尝试，比如引入分类器等等。</p><p>b. Time Decay：</p><p>注意时间衰减，不同时期的行为所占权重是不同的。</p><p>最终，针对于某个用户最想想刻画得到的结果是，用户对于不同种类item的倾向性，eg,比如这个用户对于娱乐倾向性是0.7，对于财经的倾向性是0.3。</p><p>（3）线上推荐部分</p><p>a. find top k Genre/Topic ；</p><p>b. get the best n item for fix genre/topic</p><p>第一步，基于用户的刻画，找到用户最感兴趣的top k个分类，由于这top k个分类都是带有权重的，那么第二步，相应给每个分类得到n个最好的分类下的item.</p><p>这里有两点说明，</p><p>a. 由于权重的不同，从种类下召回的数目是不同的。比如某人对财经感兴趣，对娱乐也感兴趣，但是对娱乐感兴趣的程度更高。那么对娱乐召回的数目就要多于财经召回的数目。</p><p>b. best的理解：这里的best对于不是新item来讲，就是它的后验CTR；如果是新的item，在入库的时候，都会给出一个预估的CTR，那么就用这个预估的CTR来作为衡量的标准。</p><p>第二部分：基于内容的推荐的代码实战部分</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;个性化推荐算法实践第05章基于内容的推荐方法Content-Based&quot;&gt;&lt;a href=&quot;#个性化推荐算法实践第05章基于内容的推荐方法Content-Based&quot; class=&quot;headerlink&quot; title=&quot;个性化推荐算法实践
      
    
    </summary>
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="Content Based" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/Content-Based/"/>
    
      <category term="基于内容的推荐" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/Content-Based/%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E6%8E%A8%E8%8D%90/"/>
    
    
      <category term="个性化推荐算法 - Content Based - 基于内容的推荐" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95-Content-Based-%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E6%8E%A8%E8%8D%90/"/>
    
  </entry>
  
  <entry>
    <title>个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec</title>
    <link href="http://enfangzhong.github.io/2019/06/01/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E7%AC%AC04%E7%AB%A0%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95item2vec/"/>
    <id>http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/</id>
    <published>2019-06-01T06:03:11.000Z</published>
    <updated>2019-09-22T07:40:52.023Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec"><a href="#个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec" class="headerlink" title="个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec"></a>个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec</h1><p>本章节重点介绍一种基于深度学习的个性化召回算法item2vec。从item2vec的背景与物理意义以及算法的主流程进行介绍。并对该算法依赖的模型word2vec数学原理进行浅析。最后结合公开数据集代码实战item2vec算法。</p><p>本章节重点介绍基于深度学习的个性化召回算法item2vec。分为两部分：第一部分，item2vec背景和物理意义，论文依据，算法流程，以及item2vec的依据的模型word2vec的数学原理等等理论部分解析。第二部分，根据示例数据，item2vec的算法流程，编程实战，训练模型得到item对应的向量完成推荐并分析推荐结果。</p><h2 id="一、基于深度学习的个性化召回算法item2vec"><a href="#一、基于深度学习的个性化召回算法item2vec" class="headerlink" title="一、基于深度学习的个性化召回算法item2vec"></a>一、基于深度学习的个性化召回算法item2vec</h2><h3 id="1、个性化召回算法item2vec背景与物理意义"><a href="#1、个性化召回算法item2vec背景与物理意义" class="headerlink" title="1、个性化召回算法item2vec背景与物理意义"></a>1、个性化召回算法item2vec背景与物理意义</h3><h4 id="个性化召回算法item2vec背景"><a href="#个性化召回算法item2vec背景" class="headerlink" title="个性化召回算法item2vec背景"></a>个性化召回算法item2vec背景</h4><ol><li>Item2item的推荐方式效果显著：</li></ol><p>很多场景下item2item的推荐方式要优于user2item；</p><p>item2item的推荐方式：在获取item相似度矩阵之后，根据用户的最近的行为，根据行为过的item找到相似的item，完成推荐，如itemCF。</p><p>user2item：根据用户的基本属性和历史行为等基于一定的模型，算出最可能喜欢的item列表写在KV存储中；当用户访问系统的时候，将这些item列表推荐给用户，像userCF、LFM、personal rank算法等都是这种方式。</p><ol><li>NN model（神经网络）模型的特征抽象能力</li></ol><p>神经网络的特征抽象能力是要比浅层的模型特征抽象能力更强，主要有两方面原因：</p><p>（1）输入层与隐含层，隐层与输入层之间，所有的网络都是全连接的网络；</p><p>（2）激活函数的去线性化；</p><p>基于上述，基于神经网络的item2item的个性化召回算法item2vec也就在这个大背景下产生了。</p><p>3、依据的算法论文：<a href="http://www.researchgate.net/publication/298205072_Item2Vec_Neural_Item_Embedding_for_Collaborative_Filtering?ev=auth_pub" target="_blank" rel="noopener">Item2Vec: Neural Item Embedding for Collaborative Filtering</a></p><p>核心内容1：论文首先介绍了item2vec落地场景是类似于相关推荐的场景。也就是说用户点击了某item a APP,那么推荐一款类似的APP给用户。</p><p>核心内容2：介绍了item2vec所选的model，也就是word2vec算法原理，文中采用了负采样的训练方法进行介绍，因为我们知道word2vec还有一种也就所说的哈夫曼树的方式进行训练。那么我们后面也用负采样的方式进行介绍word2vec的算法原理。</p><p>核心内容3：论文抽象了算法的整体流程。</p><p>核心内容4：论文将给出了与之前流行的item2item算法进行结果对比。</p><h4 id="个性化召回算法item2vec物理意义"><a href="#个性化召回算法item2vec物理意义" class="headerlink" title="个性化召回算法item2vec物理意义"></a>个性化召回算法item2vec物理意义</h4><p>在介绍item2item之前，先介绍一下原型word2vec。</p><ol><li>word2vec</li></ol><p>根据所提供的语料，语料可以想象成一段一段的文字，将语料中的词embedding成词向量，embedding成词向量之间的距离远近可以表示成词与词之间的远近。（word2vec原理中详细介绍怎么样做到可以表征词与词距离的远近）</p><ol><li>item2item</li></ol><p>（1）将用户行为序列转换成item组成的句子。</p><p>解释：在系统中，无论是用户的评分系统，还是信息流场景下用户的浏览行为，或者是电商场景下用户的购买行为。在某一天内，用户会进行一系列的行为，那么将这一系列的行为抽象出来。每一个用户组成的item与item之间的这种序列的连接关系就变成了之前所说的文字组成的一段一段的句子，那么这里的每一个word相当于这里的item。</p><p>（2）模仿word2vec训练word embedding 过程，将item embedding。</p><p>word embedding的过程只需要提供语料，也就是一段一段的文字，那么训练得到的word embedding可以表征词语义的远近，那么同样希望表征item之间内涵的远近。</p><p>所以，可以将第一步构成的item语料放到word2vec中，也能够完成item embedding。embedding完成的向量同样可以表示item之间的隐语义的远近，也就是说，可以表示item之间的相似性。</p><p>以上就是item2vec的物理意义。</p><h4 id="个性化召回算法item2vec缺陷"><a href="#个性化召回算法item2vec缺陷" class="headerlink" title="个性化召回算法item2vec缺陷"></a>个性化召回算法item2vec缺陷</h4><p>（1）用户的行为序列时序性缺失：</p><p>在介绍物理意义的时候，说过将用户的行为转化成由item组成的句子，这里句子之间词与词之间的顺序与按照用户行为顺序进行排列和不按照用户行为顺序进行排列的结果是几乎一致的。</p><p>也就是用户的行为顺序性，模型是丢失的。</p><p>（2）用户行为序列中的item强度是无区分性的：</p><p>这里比如说，在信息流场景中，观看短视频的50%或者80%或者100%，在用item组成的句子当中，同样都是出现一次的，而不是说观看100%就会出现2次。</p><p>再如，在电商场景中，可能你购买一件商品，或者说你加了购物车，都会出现一次；而不会说，你购买了就会出现两次。加了购物车，在句子当中出现一次。在item cf算法中，item 相似度矩阵计算 ，当时引入了用户行为item的总数，用户行为item的时间这两个特征，来分别将公式进行升级。所以在item2vec算法中，是没有这个消息的。所以是item2vec的缺陷之一。</p><h3 id="2、item2vec算法应用的主流程"><a href="#2、item2vec算法应用的主流程" class="headerlink" title="2、item2vec算法应用的主流程"></a>2、item2vec算法应用的主流程</h3><p>（1）从log中抽取用户行为序列</p><p>按照每个用户“天”级别，行为过的item，构成一个完整的句子，这里的行为根据不同的推荐系统所指的不同。比如：在信息流场景下，用户点击就可以认为是行为；那么在评分系统中，我们可能需要评分大于几分；在电商系统中，可能希望用户购买，得到用户行为序列所构成的item句子。</p><p>（2）将用户序列当做语料训练word2vec得到item embedding</p><p>在训练过程中，word2vec代码不需要书写，但是有很多参数是需要我们具体设定的。</p><p>（3）得到item sim关系用于推荐</p><p>根据第二个步骤中得到每一个item所embedding的向量，可以计算每一个item最相似的top k个，然后将相似度矩阵离线写入到KV当中，当用户访问我们的推荐系统的时候，用户点击了哪些item，推出这些item所最相似的top k个给用户，就完成了推荐。</p><p>eg：</p><p><img src="/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/1564895184811.png" alt="1564895184811"></p><p>1、首先第一步：这是我们从推荐系统log中获得的，也就是说User A行为过item a、item b、item d，User B行为过item a、item c，User C行为过item b、item e。</p><p>2、继而我们需要将这些转化成句子。句子1就是a、b、d。句子2就是a、c。句子3就是b、e。这里我们已经没有了user之间的关系，只留下item组成的句子。</p><p>3、将句子放入到放入word2vec模型，这里就是一个输入（word2vec模型是一个三层的神经网络：输入层、隐含层、输出层，具体每一层的作用，公式原理后面详细介绍。）</p><p>经过word2vec模型的训练，会得到每一个item对应的embedding层向量。这里只写了item a [0.1,0.2,0.14……..0.3] ,item b [0.4,0.6,0.14……..0.3]。item c，item d,item e也是可以得到的。</p><p>4、得到item向量之后，就得到item的sim关系。基于item sim的关系，我们便完成了用户的推荐。</p><h3 id="3、item2vec依赖模型word2vec介绍（连续词袋模型和跳字模型-skip-gram-）"><a href="#3、item2vec依赖模型word2vec介绍（连续词袋模型和跳字模型-skip-gram-）" class="headerlink" title="3、item2vec依赖模型word2vec介绍（连续词袋模型和跳字模型(skip-gram)）"></a>3、item2vec依赖模型word2vec介绍（连续词袋模型和跳字模型(skip-gram)）</h3><p>item2vec依赖模型word2vec的数学原理详细介绍</p><p>word2vec model</p><p>是围绕负采样的训练方法进行介绍，因为我们知道word2vec还有一种也就所说的哈夫曼树的方式进行训练（这里就不做介绍了）。</p><p><strong>word2vec有两种形式：</strong>连续词袋模型和跳字模型(skip-gram)</p><ol><li><strong>CBOW（continuous bag of words）连续词袋模型</strong></li><li><strong>skip gram()</strong>跳字模型</li></ol><h2 id="二、item2vec依赖模型word2vec之CBOW数学原理介绍"><a href="#二、item2vec依赖模型word2vec之CBOW数学原理介绍" class="headerlink" title="二、item2vec依赖模型word2vec之CBOW数学原理介绍"></a>二、item2vec依赖模型word2vec之CBOW数学原理介绍</h2><h3 id="1、CBOW网络结构"><a href="#1、CBOW网络结构" class="headerlink" title="1、CBOW网络结构"></a>1、CBOW网络结构</h3><p><img src="https://img-blog.csdnimg.cn/20190331193124458.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY0MDU4Mw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>网络分为三层：输入层、投影层、输出层。</p><p>输入层：上下文；比如说这里有五个词W(t+2)、W(t+1)、W(t)、W(t-1)、W(t-2)。这里我们需要输入的训练数据是W(t)的上下文，即是W(t+2)、W(t+1)、W(t-1)、W(t-2)。 </p><p>投影层：将上下文的词输入的向量加起来；因为给每个词都初始化了一个向量。比如说我们需要让它的长度是16。那么刚开始的时候，这些词W(t+2)、W(t+1)、W(t-1)、W(t-2)，再包括了W(t)都有一个初始化的向量。把这些向量加到了投影层。</p><p>输出层：当前词；</p><p>投影层与输出层之间是全连接，如果输出的这个词是这里的W(t)的话，希望最大化的就是这个概率。而除了W(t)，词典（词典指训练语料包含的所有的词）中所有的其他的词，其余词的概率我们都希望最小那个概率。</p><p>这就引申出一个问题，如果其余词都是负样本的话，负样本太多，训练太慢。</p><p>所以，采用负采样（后面介绍）的方法。这里我们先不要关心负采样的具体是怎么做的，后面我们会详细的介绍</p><p>也就是说，有了正样本W(t)与它的上下文，负样本是通过负采样得到的，我们希望最小化负样本所对应的模型的输出。这里实际上，最后一层的输出不仅仅是只有W(t)，实际上是有字典中每一个词。因为每一个词都有一个与输出层之间的全连接，这也是模型需要训练的参数之一。</p><p>然后，模型需要得到的是每一个词，我们初始化的那个向量，让它训练，根据我们传入的训练样本，训练完成的向量，就是最后模型输出。我们就是依赖每一个词对应的向量，完成item相似度矩阵之间的运算。</p><p>与传统的监督模型有所不同，传统的模型在训练完成之后，需要将它保存，然后对外提供服务，当我们传入真实样本的时候，希望得到一个输出值；而这里不是。下面我们来看下一中形式。</p><h3 id="2、CBOW的数学公式"><a href="#2、CBOW的数学公式" class="headerlink" title="2、CBOW的数学公式"></a>2、CBOW的数学公式</h3><ol><li>问题抽象</li></ol><script type="math/tex; mode=display">g(w)=\prod_{u \in w \cup N E G(w)} p(u | \operatorname{Context}(w))</script><p>上式是想最大化的条件概率函数。</p><p>下面我来解释一下这个公式：</p><p>$\text { Context }(w)$：某一词W(t)的上下文的词，已知上下文，想预测中间词。</p><p>可能有的同学对于这个训练样本还不是很了解，下面我举一个最简单的例子，有一句话是我是中国人，那么经过分词之后呢，变成了（我    是    中国    人）这么四个字也就是w1,w2,w3,w4，如果我们选窗口是1的话，在这里第1组的训练样本变成了（我   是     中国  人），其中这个”是”就是公式这里的w，它的上下文呢，w(t+1)呢是’中国’，w（t-1）是”我”这个词。</p><p>我们知道了”中国”，”我”这个词。如果u是W的话（这个u是公式中W”是”的话），则是需要最大化条件概率；如果是W的负样本（NEG(w)）（除了”是”，以外的其他词），我们想要把这个概率最小化，最小化这个条件概率$p(u | \text { Context }(w))$，也就是最大化$1-p(u | \text { Context }(w))$条件概率。那么无论是u是w,或者u是我们选择的负采样的负样本都能统一起来了，实际上这个式子由两部分组成。</p><p>这里的条件概率是指：</p><script type="math/tex; mode=display">p(u | \text { Context }(w))=\sigma\left(X_{w}^{T} \theta^{u}\right)^{L^{w}(u)}\left(1-\sigma\left(X_{w}^{T} \theta^{u}\right)\right)^{\left(1-L^{w}(u)\right)}</script><p>这个式子由两部分组成：</p><p>（1）当u=w时，也就是说label $L^{w}(u)$=1，起作用的是前一部分，因为后面的指数变为零次幂（$1-L^{w}(u)$=0），零次幂的话，后面部分就等于1了。起作用的是前一部分，我们是想最大化的正样本的概率。解释一下这里的$X_{w}^{T}$和$\theta^{u}$：</p><p>$X_{w}^{T}$：CBOW的时候，投影层是将w对应的上下文的词向量加和。也就是我们这里举例子的”我”和”中国”对应的向量加和，便是这里的$X_{w}^{T}$。</p><p>$\theta^{u}$：隐含层（投影层）和输出层对应的词为u的时候，它们之间的全连接。</p><p>所以，想通过模型训练，让这两个参数$X_{w}^{T}$和$\theta^{u}$相乘得到的结果为1。（按照我们的距离就是16维的横竖向量相乘应该是一个常数，这里我们希望通过训练，让他最终相乘得到的数字是1）</p><p>（2）负采样部分，也就是后面那一部分，我们这里负采样选取的负样本，希望这一部分是0。也就是希望这一部分$1-\sigma\left(X_{w}^{T} \theta^{u}\right)$为1，也就是最大化$1-\sigma\left(X_{w}^{T} \theta^{u}\right)$这一部分。</p><ol><li>损失函数</li></ol><p>$\operatorname{Loss}=\log (g(w))$</p><p>这里采用对数损失函数，之前LFM采用的是平方损失函数。</p><p>公式带入：</p><script type="math/tex; mode=display">\text {Loss}=\sum\left(L^{w}(u) * \log \left(\sigma\left(x_{w}^{T} \theta^{u}\right)\right)+\left(1-L^{w}(u)\right) * \log \left(1-\sigma\left(x_{w}^{T} \theta^{u}\right)\right)\right)</script><p>取对数之后，简单讲解一下，之前的累乘，由于我们取对数，就变成了累加。而之前里面是两部分相乘，，由于我们取对数，就变成了两部分相加。而且对数里面的幂次，可以直接变成了这里的系数。</p><p>对$X_{w}^{T}$和$\theta^{u}$求偏导，求完偏导之后，使用梯度上升法不断迭代这里我们这里需要的参数$X_{w}^{T}$和$\theta^{u}$，继而便能够去迭代每一个词对应的词向量。</p><ol><li>梯度：</li></ol><p>$\frac{\partial L o s s}{\partial \theta^{u}}=\left(L^{w}(u)-\delta\left(x_{w}^{T} \theta^{u}\right)\right) x_{w}$               $\theta^{u}=\theta^{u}+\alpha * \frac{\partial L o s s}{\partial \theta^{u}}$</p><p>$\frac{\partial L o s s}{\partial x_{w}}=\left(L^{w}(u)-\delta\left(x_{w}^{T} \theta^{u}\right)\right) \theta^{u}$        $v(w_{context}) = v(w_{context}) + \sum_{u \in w \cup NEG(w)}\alpha *\frac{\partial L o s s}{\partial x_{w}}$</p><p>梯度公式1：</p><p>首先对$\theta^{u}$求偏导得到了上述的结果。我们先不在此处讲解推导过程。推导过程和排序部分的逻辑回归的部分完全一样。（推导过程也并不是很复杂，只需要记住链式求导法以及加上激活函数。sigmod函数的导数是等于他的本身乘以（1-他的本身），即是s(x)*(1-s(x))）。这两个小技巧比较容易得到。</p><p>公式中，$L^{w}(u)$是label，值是1或者0，如果当这里的词是中心词的时候就是1，如果这里的词是负采样中选取的负样本，那么就是0。$\delta\left(x_{w}^{T} \theta^{u}\right)$这个是模型的输出,实际上是投影层对应的向量$x_{w}^{T}$，并且乘以$\theta^{u}$向量得到的一个值，我们在用激活函数激活一下，也得到了一个零一之间的值。这里的$x_{w}$便是投影层上下文向量的加和。</p><p>由于我们之前看到的损失函数里$x_{w}$与$\theta^{u}$是对偶的，所以loss函数对$x_{w}$的偏导也便是与上面对偶的形式，只不过括号外面是乘以$\theta^{u}$。</p><p>梯度公式2：</p><p>既然分别都得到偏导之后呢，我们如果去更新呢。对于$\theta^{u}$我们根据学习率去对于$\theta^{u}$更新就可以了。但是对于$x_{w}$更新，我们看到由于这里损失函数对$x_{w}$求偏导呢，是与这里的$\theta^{u}$有关系的。这个u我们知道它有可能是中心词，也有可能是负采样所选出来的负样本，所以他是一系列的，我们将这一系列的词，或者说是正负样本对。学习完之后我们得到一个总的梯度。得到这个总的梯度之后呢，是$x_{w}^{T}$的梯度，也就是$x_{w}^{T}$可以去更新它自己。这里$x_{w}^{T}$是所有上下文词向量的加和。这里也采用了上下文的每一个词都共享这个梯度，来更新自己的向量。</p><p>这里就是$x_{w}^{T}$对于正负样本对他的梯度的加和。然后我们将上下文中的每一词对应的词向量都以这个梯度去更新。</p><h3 id="3、训练的主流程"><a href="#3、训练的主流程" class="headerlink" title="3、训练的主流程"></a>3、训练的主流程</h3><ol><li><p>选取中心词w以及负采样出NEG(w)</p><p>根据训练的语料，选取中心词w与上下文的词构成的正样本以及负采样选取出的负样本。</p></li><li><p>分别获得损失函数对于$X_w$和的$\theta^u$梯度</p></li></ol><p>$X_w$：隐含层(投影层)的向量，是上下层向量的一个累加和；</p><p>$\theta^u$：正负样本的每一个词都有一个$\theta^u$；</p><ol><li>更新$\theta^u$以及中心词对应的上下文context(w)中的每一个词的词向量。</li></ol><p>这里更新的时候需要注意：</p><p>以一个实例来说明：</p><p>中心词所对应的负样本，假使我们选了5个，加上中心词与上下文组成的正样本，这里一共有6个样本。在$X_w$的梯度的过程当中，实际上是6个梯度的加和，构成了它自己的梯度。在每一词所需要更新的$\theta^u$以及$X_w$的1/6的时候，首先先更新$X_w$的1/6。因为$X_w$是依赖于$\theta^u$的。如果这一次我们将$\theta^u$更新呢，再更新$X_w$的话，就错了。</p><p>故需要先更新$X_w$，在更新$\theta^u$。</p><h2 id="三、item2vec依赖模型word2vec之skip-gram数学原理介绍"><a href="#三、item2vec依赖模型word2vec之skip-gram数学原理介绍" class="headerlink" title="三、item2vec依赖模型word2vec之skip gram数学原理介绍"></a>三、item2vec依赖模型word2vec之skip gram数学原理介绍</h2><h3 id="1、skip-gram网络结构"><a href="#1、skip-gram网络结构" class="headerlink" title="1、skip gram网络结构"></a>1、skip gram网络结构</h3><p>我们首先来看一下它的网络结构，这里同样有三层构成(输入层、投影层、输出层)。</p><p><img src="/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/1564901411917.png" alt="1564901411917"></p><p>与CBOW网络结构不同的是，这里的投影层与输入层完成是一样的，也就是说，投影层是W(t)的输入向量。</p><p>这里的核心目标是说，当W(t)已知的情况下，去预测它的周围词，我们将这个条件概率最大化就是我们的目标。所以，这里也是有两组参数需要更新的：</p><p>（1）W(t)与词典中的每一个词所对应的这种全连接网络，这个参数需要更新；</p><p>（2）W(t)本身对应的初始化的向量。比如说我们想要把每个词映射成16维，这个向量也是需要更新的，</p><p>这里与之前有两点不同：</p><p>（1）这里每一次更新，只能更新W(t)一个词语对应的向量；而CBOW模型一次可以更新4个（4：针对上图）。也就是对应的这个上下文，如果我们上下文的窗口选的更长一点的话，可能会更新的更多一次。</p><p>（2）在对W(t)的每一个词进行上下文训练的时候，都需要对输出的词进行一次负采样，来构成训练的负样本。</p><h3 id="2、skip-gram的数学公式"><a href="#2、skip-gram的数学公式" class="headerlink" title="2、skip gram的数学公式"></a>2、skip gram的数学公式</h3><ol><li>问题抽象</li></ol><script type="math/tex; mode=display">G=\prod_{u \in \text { Contert }(w)} \prod_{z \in {u} \cup NEG(u)} p(z | w)</script><p>skip gram是已知中间词，去最大化它相邻词的概率。</p><p>举个栗子：”我   是  中国”为例子，这个w就是”是”这个词。这里的z就是”我”或者”中国”他们对应的正样本，以及通过负采样选取的负样本，最大化正样本的输出概率，并且最小化负样本的输出概率，也就是最大化(1-负样本）的输出概率。</p><p>与CBOW的不同：CBOW的时候，是选取一次负采样；而这里对于中间词的上下文的每一个词，也就是”我”或者”中国”,每一次都需要进行一个负采样。</p><p>下面看一下条件概率：</p><script type="math/tex; mode=display">p(z | w)=\left(\delta\left(v(w)^{T} \theta^{z}\right)\right)^{L^{u}(z)} *\left(1-\delta\left(v(w)^{T} \theta^{z}\right)\right)^{1-L^{u}(z)}</script><p>这个条件概率与之前的CBOW大体形式一样，也就是说当label $L^{w}(u)$=1的时候，我们还是希望最大化这个条件概率。当label $L^{w}(u)$=0的时候（看后半部分），我们需要最大化$(1-\delta\left(v(w)^{T} \theta^{z}\right))$,即最大化1减去模型输出。</p><p>这个条件概率与之前的CBOW，不同之处：</p><p>（1）隐含层（投影层）输出的是中间词对应的词向量；而CBOW是输出的所有中间词上下文词向量对应的和；</p><p>（2）这里的$\theta^{z}$：上下文的词，或者是上下文的词选出来的负样本的词与输出层之间的全连接；目标是中间词$v(w)^{T}$对应的向量以及$\theta^{z}$进行参数学习。进而得到中间词词向量的最佳表示。</p><ol><li>损失函数<script type="math/tex; mode=display">\text {Loss}=\sum_{u \in \text {Context}(w)} \sum_{z \in {u} \cup N E G(u)} L^{u}(z) * \log \left(\delta\left(v(w)^{T} \theta^{z}\right)\right)+\left(1-L^{u}(z)\right) * \log \left(1-\delta\left(v(w)^{T} \theta^{z}\right)\right)</script></li></ol><p>采用log损失函数。将上述的式子log一下，两个连乘，变成了两个连加（之前的乘变成了加）。幂次也可以放到log的前面。</p><p>但是，可以发现如果按照这个loss去对$v(w)^t$或者$\theta^z$求偏导，在每一轮迭代的时候，只能够对词向量$v(w)^t$进行一次迭代。这里需要进行上下文窗口次的负采样才能对一个词的词向量进行迭代。显然，效率有些低。</p><p>在真正的word2vec实现的时候，需要变换一下思路：</p><script type="math/tex; mode=display">\text {G}=\sum_{w^c \in \text {context}(w)} \sum_{u \in {w} \cup N E G(u)} p(u|w^c)</script><p>同样也是基于像CBOW一样的思想，已知上下文$(w^c)$的情况下，最大化中间词u。但是这里上下文$(w^c)$的每一个词都是独立的，不像CBOW是对上下文中所有的词向量进行累加。</p><p>下面重新看一下损失函数：</p><script type="math/tex; mode=display">\text {Loss}=\sum_{w^c \in \text {Context}(w)} \sum_{u \in {w} \cup N E G(u)} L^{w}(u) * \log \left(\delta\left(v(w^c)^{T} \theta^{z}\right)\right)+\left(1-L^{w}(u)\right) * \log \left(1-\delta\left(v(w^c)^{T} \theta^{u}\right)\right)</script><p>这个log损失函数，如果我们忽略掉前半部分的累加。我们只看后面这部分。如果把上下文中的单个的词，变成了$w^x$的话，这一部分的损失函数与上一节讲过的GBOW的损失函数就一样了。也就是说每一次不是用所有上下文的累加和向量来进行梯度的学习。而是对每一个词单独学习梯度，并进行单独更新。实际上梯度形式也比较容易。只是比上一节求出来的多了一次累加。</p><ol><li>Skip Gram训练主流程</li></ol><ul><li><p>对于中心词上下文词context(w)中的每一个词$w^c$，都需要选取一次负采样，也就是选取词w的正负样本，构造出正负样本；</p></li><li><p>计算loss对于theta以及$w^c$的偏导；($w^c$指的是我们举例的”我 是  中国”的”我”  或者”中国”)，计算偏导也是有顺序的，像CBOW首先更新loss对于$w^c$的偏导，因为这个偏导是最后我们需要更新词向量偏导的 1/n （n=负采样的数目 + 正样本 + 1）；</p></li></ul><ul><li>更新$w^c$对应的词向量；</li></ul><p>skip gram与CBOW相比，每一次负采样skip gram只能更新一个词对应的词向量；而CBOW在一次负采样，可以更新n（n指窗口）个词。</p><ol><li>负采样的算法</li></ol><p>假设词典（训练样本中所有的词）中有n个词，每一个词都会计算出一个长度，这个长度是一个0-1之间的长度，有的短，有的长，所有词的长度加起来的长度=1。</p><p><img src="/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/1564945861541.png" alt="1564945861541"></p><p>1、每一词的长度计算：</p><script type="math/tex; mode=display">\operatorname{len}(\text {word})=\frac{(\text {counter }(\text {word}))^{\alpha}}{\sum_{w \in D}(\text {counter }(w))^{\alpha}}</script><p>分子：该单词在所有语料中出现的次数；幂次：相当于做了一个平缓；源码中这个平缓是3/4。</p><p>分母：语料中（字典中）所有词出现的次数的累加和。</p><p>显而易见这个长度是一个0-1之间的长度。而且所有的词的长度累加便是1。这样每一个词都有自己一个值域。比如说这里的w1（词1）可能是0-0.05，w2可能是0.05-0.11。每一个词的值域都是采用前开后闭的区间。</p><p>2、然后，初始一个非常大的数，源码中采用10^8，将0-1进行等分。然后每一段都会对应一个词（w1,w2,….,wn）的值域。</p><p><img src="/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/1564946420820.png" alt="1564946420820"></p><p>eg：比如这里的m1属于了w1,m2也属于了w1,但是m4和m5属于w2。</p><p>在每次进行负采样的过程中，会随机一个0-M之间的数，随机完数字之后，也就知道了随机的哪一个词。eg：随机到了1，那么m1就对应了词1（w1），那么也就是词1。如果我们随机到了4和5，那么m4和m5对应的是w2，那么就是词2是负样本。</p><p>注意：如果随机到的词和中心词相同，那么就跳过这次，再进行一次随机。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec&quot;&gt;&lt;a href=&quot;#个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec&quot; class=&quot;headerlink&quot; title=&quot;个性化推荐算法实践
      
    
    </summary>
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="item2vec" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/item2vec/"/>
    
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="item2vec" scheme="http://enfangzhong.github.io/tags/item2vec/"/>
    
  </entry>
  
  <entry>
    <title>个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank</title>
    <link href="http://enfangzhong.github.io/2019/06/01/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E7%AC%AC03%E7%AB%A0%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95PersonalRank/"/>
    <id>http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/</id>
    <published>2019-06-01T05:03:11.000Z</published>
    <updated>2019-09-22T07:40:26.446Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="个性化推荐算法实践第03章基于图推荐的个性化推荐召回算法基于随机游走的Personal-Rank"><a href="#个性化推荐算法实践第03章基于图推荐的个性化推荐召回算法基于随机游走的Personal-Rank" class="headerlink" title="个性化推荐算法实践第03章基于图推荐的个性化推荐召回算法基于随机游走的Personal Rank"></a>个性化推荐算法实践第03章基于图推荐的个性化推荐召回算法基于随机游走的Personal Rank</h1><p>本章节重点介绍一种基于图的个性化推荐召回算法personal rank。从personal rank算法的理论知识与数学原理进行介绍。并结合公开数据集，代码实战personal rank算法的基础版本与矩阵升级版本。</p><p><strong>基于图的推荐—基于随机游走的personal rank算法实现</strong></p><p>博客第一部分：理论部分主要介绍该算法的背景、物理意义、数学公式推导，以及结合在大数据量实际推荐系统开发工作中为了满足训练速度等方面的要求，对数学公式的矩阵化升级。</p><p>博客第二大部分：主要介绍结合第一大部分的数学公式与虚拟log数据为大家编程实战该算法。</p><h3 id="1、个性化召回算法Personal-Rank背景与物理意义"><a href="#1、个性化召回算法Personal-Rank背景与物理意义" class="headerlink" title="1、个性化召回算法Personal Rank背景与物理意义"></a>1、个性化召回算法Personal Rank背景与物理意义</h3><p>1、首先介绍基于图的个性化召回算法—personal rank的背景。</p><p>（1）用户行为很容易表示为图</p><p>图这种数据结构有两个基本的概念—顶点和边。</p><p>在实际的个性化推荐系统中，无论是信息流场景、电商场景或者是O2O场景，用户无论是点击、购买、分享、评论等等的行为都是在user和item两个顶点之间搭起了一条连接边，构成了图的基本要素。</p><p>实际上这里user与item构成的图是二分图，后面会介绍二分图的概念以及结合具体的例子展示如何将用户行为转换为图。</p><p>（2）图推荐在个性化推荐领域效果显著</p><p>2、二分图<br>二分图又称为二部图，是图论中的一种特殊模型。设G=(V,E)是一个无向图，如果顶点V可分割为两个互不相交的子集（A,B），并且图中的每条边（i,j）所关联的两个顶点 i 和 j 分别属于这两个不同的顶点集（i in A, j in B）,则称图G为一个二分图。</p><p>（如果有一种无向图，它的定点可以分成两个独立的集合，并且互不相交，且所有的边关联顶点，都从属于这个集合。那么这样的图可以称为二分图。）</p><p>则，推荐系统中，user、item恰好满足两种独立的集合，并且用户行为总是从user顶点到item顶点集合，所以由推荐系统中user和item之间构成的图就是二分图。</p><p>接下来结合具体实例讲解如何将用户的行为转化为二分图。</p><p>假设某推荐系统中有4个用户：A B C D，以及从日志（log）中发现对如下item有过行为：</p><p><img src="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/1564726845935.png" alt="1564726845935"></p><p>即：user A 对 item a、b、d有过行为，userB 对 item a、c有过行为，userC对 item b、e有过行为，userD 对 item c、d有过行为。</p><p>首先将user、item分成两组不相交的集合，如下：</p><p><img src="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/1564726875995.png" alt="1564726875995"></p><p>然后，将所有user 对 item 有过行为的进行连线，就可以得到二分图，如下：</p><p><img src="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/1564727091910.png" alt="1564727091910"></p><p>此时问题也就抽象出来了，对于userA 来说，item c 和item e哪个更值得推荐？</p><p>这里共有5个item，其中userA 已经对item a、b、d有过行为，这里行为是指信息流产品中的点击或者电商产品中的购买等表示user对item喜欢的这种操作。</p><p>那么personal rank恰恰是这么一种算法，它能够结合用户行为构成的二分图，对于固定用户对item集合的重要程度给出排序，也就是说将user A 没有对item c 和item e有过行为，但是personal rank算法可以给出item c 和item e对于user A来说，哪个更值得推荐。</p><p>下面从物理意义的角度来分析一下，从二分图上如何分析出来item集合对user的重要程度。</p><p>3、物理意义<br>（1）两个顶点之间连通的路径数</p><p>如果要比较两个item顶点对固定user的重要程度，只需分别看一下user到两个item顶点的路径数，路径数越多的顶点越重要。</p><p>（2）两个顶点之间连通的路径长度</p><p>同样路径数的情况下，总路径长度越短的顶点越重要。</p><p>（3）两个顶点之间连通路径经过顶点的出度</p><p>这里解释一下出度的概念：出度是指顶点对外连接边的数目。如user A对item a、b、d有过行为，即为有条连接边，则A的出度为3。如果前两项都相同，则两个item对固定user 的重要程度则比较经过顶点所有的出度和，如果出度和越小则越重要。</p><p>结合刚才所举的具体二分图的例子，给大家介绍—对于user A来说，item c 和item e哪个更值得推荐？</p><h3 id="2、Personal-Rank算法example解析"><a href="#2、Personal-Rank算法example解析" class="headerlink" title="2、Personal Rank算法example解析"></a>2、Personal Rank算法example解析</h3><p> 例子分析</p><p><img src="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/1564729370493.png" alt="1564729370493"></p><p>1.分别有几条路径连通？</p><p>首先看A-c 之间有几条路径连通：分别是A-a-B-c，A-d-D-c 两条路径连通。</p><p>再来看A-e 之间有几条路径连通：A-b-C-e一条路径</p><p>从这一角度出发，可以知道 c 比 e 重要。</p><p>2.连通路径的长度分别是多少？</p><p>首先看A-c 之间有几条路径连通：分别是A-a-B-c，A-d-D-c ，长度都为3</p><p>再来看A-e 之间有几条路径连通：A-b-C-e长度为3</p><p>3.连通路径的经过顶点出度分别是多少?</p><p>首先看A-a-B-c这条路径：A出度是3，a出度是2，B出度是2，c出度是2</p><p>再看A-d-D-c这条路径：A出度是3，d出度是2，D出度是2，c出度是2</p><p>再看A-b-C-e这条路劲：A出度是3，b出度是2，C出度是2，e出度是1</p><p>实例中这里我们物理意义得到的结果。接下来使用程序来完成person Rank算法的时候同样可以得到相同的结论。</p><p>虽然 e 的出度和更小，但是由于1中 c 有两条路径，且1的优先级更高，所以还是应该推荐 c。</p><h3 id="3、Personal-Rank算法公式解析"><a href="#3、Personal-Rank算法公式解析" class="headerlink" title="3、Personal Rank算法公式解析"></a>3、Personal Rank算法公式解析</h3><p>personal rank是可以通过用户行为划分二分图为固定user得到item重要程度排序的一种算法。</p><p>1.算法的文字阐述</p><p>随机游走算法PersonalRank实现基于图的推荐对用户A进行个性化推荐，从用户A节点开始在用户-物品二分图random walk，以alpha的概率从A的出边中，等概率选择一条游走过去，到达该顶点后（举例顶点a），由alpha的概率继续从顶点a的出边中，等概率选择一条继续游走到下一个节点，或者（1-alpha）的概率回到顶点A，多次迭代。直到各顶点对于用户A的重要度收敛。</p><p><img src="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/1564729376053.png" alt="1564729376053"></p><p>后续我们在实现person rank算法的时候用不同的alpha值来做实验，熟悉是Google的pageRank算法的童鞋们可以发现PageRank与person rank算法有极大的相似性。只不过PageRank算法没有固定的起点。</p><p>2.算法的数学公式</p><script type="math/tex; mode=display">PR(v)=\left\{\begin{matrix}\alpha * \sum_{v^{\sim} \in i n(v)} \frac{P R\left(v^{\sim}\right)}{\left|o u t\left(v^{\sim}\right)\right|} \ldots\left(v !=v_{A}\right) & \\ (1-\alpha)+\alpha * \sum_{v^{\sim} \in i n(v)} \frac{P R\left(v^{\sim}\right)}{\left|o u t\left(v^{\sim}\right)\right|} \cdots\left(v=v_{A}\right) & \end{matrix}\right.</script><p>把不同item对user的重要程度描述为PR值。</p><p>为了便于理解，同样适用A作为固定起点。user A的PR值初始化为1，其余节点的PR值初始化为0。</p><p>这里使用 a 节点和 A 节点阐述公式的上半部分和公式的下半部分：</p><p>首先看公式的上部分，根据person rank的算法描述，节点a只可能是节点A与节点B,以alpha概率从他们的出边中等概率的选择了与节点a相互连的这条边。</p><p>具体来看，从user A出发有3条边，以3条边中等概率的选择了节点a连接的这条边，以1/3的概率选择连接节点a；user B以1/2的概率选择了连接节点a。</p><p>结合阐述看一下公式的上半部分：对于不是A节点的PR值，也就是 a 的PR值，那么首先要找到连接该顶点节点，同时分别计算他们PR值得几分之几贡献到要求节点的PR值。那么A将自己PR值得1/3贡献给了 a ，B将自己PR值得1/2贡献给了 a，分别求和，乘alpha，得到 a 的PR值。</p><p>接下来看下半部分：如果要求A节点本身的PR值，首先知道任意节点都会以（1-alpha）的概率回到本身，那么对于一些本来就与A节点相连的节点，比如这里的 a 节点或者 b 节点，它们除了以（1-alpha）的概率直接回到A以外；还可以以alpha的概率从自己的出边中等概率的选择与A相邻的这条边，比如这里的 a 节点，可以以1/2的概率选择回到A节点，所以就构成了下半部分的前后两个部分。</p><p>经过分析可以发现，personal rank算法求item对固定user的PR值，需要每次迭代在全图范围内迭代，时间复杂度在工业界实际算法落地的时候是不能接受的，所以要让尽可能多的user并行迭代。结合之前许多其他算法训练的工业界实现，很容易想到矩阵化实现，下面看personal rank算法的矩阵化实现。</p><p>3.算法抽象—矩阵式</p><p>$r=(1-\alpha) r_{0}+\alpha M^{T} r$</p><p>$M_{i j}=\frac{1}{|o u t(i)|} j \in \operatorname{out}(i) e l s e 0$</p><p>假设这里共有m个user，n个item。</p><p>R矩阵是m+n行，1列矩阵，表示其余顶点对该固定顶点的PR值。当然得到了这个，就得到了固定顶点下，其余所有顶点的重要程度排序，这里只需要排出m个user节点。只看n个item节点对该固定顶点的排序。也就得到了该固定顶点下推荐的item。</p><p>r0 是m+n行，1列的矩阵，负责选取某一节点是固定节点，它的数值只有1行唯一，其余行全为0。唯一的行，即为选取了该行对应的顶点为固定顶点。那么得到的就是该固定顶点下，其余节点对该固定节点的重要程度的排序。</p><p>M 是 m+n行 * m+n列的矩阵，也就是行包含了所有的节点，列也包含了所有的节点。 它是转移矩阵，数值定义如下：1.第一行第二列的数值距离，如果第一行对应的数值顶点由出边连接到了第二列的顶点，那么该值就为第一行顶点的出度的倒数；如果没有连接边，那么就是0。</p><p>我们很容易联想到，第一个式子包含了刚才所说的非矩阵化的personal rank的公式的上下两部分。</p><script type="math/tex; mode=display">\left(E-\alpha M^{T}\right) * r=(1-\alpha) r_{0}</script><p>上述公式是本部分中第一个公式，移项、合并同类项之后得到的。</p><script type="math/tex; mode=display">r=\left(E-\alpha M^{T}\right)^{-1}(1-\alpha) r_{0}</script><p>该公式是上一公式两个同时乘以$\left(E-\alpha M^{T}\right)$转置的之后得到的。</p><p>刚才说过，r0是m+n行，1列的矩阵，它能够选取固定的顶点，得到固定顶点的推荐结果。如果将r0变为（m+n）*（m+n）的矩阵，也就得到了所有顶点的推荐结果。</p><p>由于得到的推荐结果是考虑顶点之间的PR值的顺序关系，并非一个绝对数值，所以可以将$(1-\alpha) $舍去。所以$\left(E-\alpha M^{T}\right)^{-1}$即为所有顶点的推荐结果。每一列表示该顶点下，其余顶点对于该顶点的PR值。</p><p>但是，需要注意的是，每一个user能够行为的item毕竟是少数，所以这里的M矩阵是稀疏矩阵，$\left(E-\alpha M^{T}\right)^{-1}$同样也是稀疏矩阵。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;个性化推荐算法实践第03章基于图推荐的个性化推荐召回算法基于随机游走的Personal-Rank&quot;&gt;&lt;a href=&quot;#个性化推荐算法实践第03章基于图推荐的个性化推荐召回算法基于随机游走的Personal-Rank&quot; class=&quot;he
      
    
    </summary>
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="基于随机游走的Personal Rank" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E5%9F%BA%E4%BA%8E%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0%E7%9A%84Personal-Rank/"/>
    
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="基于随机游走的Personal Rank" scheme="http://enfangzhong.github.io/tags/%E5%9F%BA%E4%BA%8E%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0%E7%9A%84Personal-Rank/"/>
    
  </entry>
  
  <entry>
    <title>个性化推荐算法实践第02章基于邻域的个性化召回算法LFM</title>
    <link href="http://enfangzhong.github.io/2019/06/01/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E7%AC%AC02%E7%AB%A0%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95LFM/"/>
    <id>http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第02章基于邻域的个性化召回算法LFM/</id>
    <published>2019-06-01T04:03:11.000Z</published>
    <updated>2019-09-22T07:57:26.166Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="个性化推荐算法实践第02章基于邻域的个性化召回算法LFM"><a href="#个性化推荐算法实践第02章基于邻域的个性化召回算法LFM" class="headerlink" title="个性化推荐算法实践第02章基于邻域的个性化召回算法LFM"></a>个性化推荐算法实践第02章基于邻域的个性化召回算法LFM</h1><p>本章节重点介绍一种基于邻域的个性化召回算法，LFM。从LFM算法的理论知识与数学原理进行介绍。并结合公开数据集，代码实战LFM算法。</p><p>个性化召回算法LFM（latent factor model）算法综述</p><ul><li>包含了LFM背景，结合LFM的实例与求解方法。</li><li>以及LFM算法的应用场景。</li></ul><p>LFM（latent factor model）理论知识与公式推导</p><ul><li>LFM在建模后如何定义损失函数</li><li>如何迭代到模型收敛得到模型参数</li></ul><p>LFM（latent factor model）算法与CF算法的优缺点比较</p><ul><li>从理论的体系的完整度，离线训练复杂度</li><li>在线推荐的可解释性</li><li>完成LFM与CF算法的优缺点比较</li></ul><h3 id="个性化召回算法LFM（latent-factor-model）算法综述"><a href="#个性化召回算法LFM（latent-factor-model）算法综述" class="headerlink" title="个性化召回算法LFM（latent factor model）算法综述"></a>个性化召回算法LFM（latent factor model）算法综述</h3><p>对于基于邻域的机器学习算法来说，如果要给一个用户推荐商品，那么有两种方式。</p><p>一种是基于物品的，另一种是基于用户的。</p><p>基于物品的是，从该用户之前的购买商品中，推荐给他相似的商品。</p><p>基于用户的是，找出于该用户相似的用户，然后推荐给他相似用户购买的商品。</p><p>但是，推荐系统除了这两种之外，还有其他的方式。例如如果知道该用户的兴趣分类，可以给他推荐该类别的商品。</p><p>为了实现这一功能，我们需要根据用户的行为数据得到用户对于不同分类的兴趣，以及不同商品的类别归属。</p><p>LFM—隐语义模型，属于协同领域。</p><ol><li>LFM算法的背景<br>提到协同领域，很多人首先想到的就是item CF与user CF，那么这里说到的LFM与这两者又有什么区别呢？</li></ol><p>首先简单回忆一下item CF与user CF。</p><p>item CF：主体是item，首先考虑的是item层面。也就是说，可以根据目标用户喜欢的物品，寻找和这些物品相似的物品，再推荐给用户。</p><p>user CF：主体是user，首先考虑的是user层面。也就是说，可以先计算和目标用户兴趣相似的用户，之后再根据计算出来的用户喜欢的物品给目标用户推荐物品。</p><p>那么LFM呢？</p><p>LFM：先对所有的物品进行分类，再根据用户的兴趣分类给用户推荐该分类中的物品。</p><p>那LFM具体的意义是什么呢？</p><p>为了方便大家理解，这里通过item CF再进一步说明。</p><p>item CF算法，是将item进行划分，这样一旦item贡献的次数越多，就会造成两个item越相近。举个例子来说，就是当你看你喜欢的电视节目的时候，为了不错过精彩的内容，你广告部分也会看；这时，后台就会统计，你看了<strong>电视节目，也看了</strong>广告。这样就可能分析出<strong>电视节目与</strong>广告比较接近。</p><p>然而，事实上两者并不一样，如果我们知道两者属于不同的tag，我们不会将他们放到一起，进行降权处理。但是建立大量的tag体系就需要消耗大量的人力标注打标签，人力标注不适用。继而需要机器学习完成分类。在0-1搭建个性化推荐系统算法中显然是不切实际的。那么LFM算法就应运而生。</p><p>LFM是根据用户对item的点击与否，来获取user与item之间的关系，item与item之间的关系。</p><p>我的理解就是，LFM不仅会考虑item，也会考虑item。</p><p>2、什么是LFM算法<br>LFM算法输入：user对item的点击矩阵</p><p>LFM模型参数：每一个user的向量表示和每一个item的向量表示。</p><p>方式：用user矩阵和item矩阵的矩阵乘 拟合 user对item的点击矩阵</p><p>我们知道行向量乘以列向量是一个常数。完美情况下，user向量和item向量的相乘可以拟合点击矩阵的数值。我们可以看出本模型是从user对item的点击矩阵得到的user-item的向量。所以lfm也是矩阵分解的算法的一种。</p><p>LFM算法举例</p><p><img src="/2019/06/01/个性化推荐算法实践第02章基于邻域的个性化召回算法LFM/1564628104175.png" alt="1564628104175"></p><p>输入：user对item的点击矩阵（1代表点击、0代表未点击）</p><p>user对item的点击矩阵</p><div class="table-container"><table><thead><tr><th></th><th>item1</th><th>item2</th><th>item3</th></tr></thead><tbody><tr><td>user1</td><td>1</td><td>0</td><td>1</td></tr><tr><td>user2</td><td>0</td><td>1</td><td>0</td></tr><tr><td>user3</td><td>1</td><td>1</td><td>0</td></tr></tbody></table></div><p>输出：</p><p>​        user1:[0.123, 0.325, …, 0.623], …</p><p>​        item1:[0.214, 0.034, …, 0.241], …</p><p>user和item的向量维度是自定义的，用user与item作内积即可判断user对item的偏好</p><p>图片左边是点击矩阵，user对item的行为矩阵（点击表示为1,没有点击表示为0）。图片右边是模型收敛得到的是use和item的向量。对于这个维度可以是之前设定的。维度可以理解为有哪一些特征会影响uers对item的喜好程度。特征例如：item title,是否含有图片，<strong>小清新的</strong>、<strong>吉他伴奏的</strong>、<strong>摇滚</strong>等等这些特征会影响用户的偏好。这里比如来说统计出来有7个特征。那么在模型设置的时候就可以把维度设置为7，得到的向量显然就是user1，Item1都会表示7维度的向量。那么将user1，item1的转置乘起来的话应该是一个常数。如果将每一个user和item的向量乘起来可以和点击矩阵的常数无限接近的话。那么这个模型的效果也就越好。</p><p>3、 LFM算法的应用场景举例<br>（1）获取user的item推荐列表（计算用户toplike）</p><p>模型得到了user和item的向量，针对于用户没有被展现的item，我们可以计算出他的一个用户对item的倾向性得分。取top即toplike，后直接完成用户对item的喜爱度列表，写入离线即可完成对在线的推荐。</p><p>（2）获取item间的相似度列表（计算item的topsim）</p><p>得到item的向量可以用很多表示距离的公式包括cos等等，计算出每一个item的相似度矩阵，该矩阵可以用于线上推荐。当用户点击item之后，给其推荐与该item的topsim item。</p><p>（3）挖掘item间隐含topic挖掘（计算item的topic）</p><p>根据得到的item向量，可以用聚类的方法，如K-means，层次聚类等等，取出一些隐含的类别。也就是一些隐含的topic，能将item分成不同的簇，推荐时按簇推荐</p><h3 id="LFM（latent-factor-model）理论知识与公式推导"><a href="#LFM（latent-factor-model）理论知识与公式推导" class="headerlink" title="LFM（latent factor model）理论知识与公式推导"></a>LFM（latent factor model）理论知识与公式推导</h3><p>本小节主要从数学上重点介绍LFM，主要从建模方法、迭代、收敛等方面认识LFM算法</p><p>（1）LFM建模公式</p><script type="math/tex; mode=display">p(u, i)=p_{u}^{T} q_{i}=\sum_{f=1}^{F} p_{u f} q_{i f}</script><p>p(u,i)表示user-item对，如果user点击了item，那么p(u,i)=1，否则p(u,i)=0。模型的最终输出为user向量和item向量，即$p_u$和$q_i$。其中F表示维度，也就是上一小节阐述的user对item喜欢与否的影响因素的个数。</p><p>那么具体如何得到$p_u$和$q_i$呢？<br>我们用机器学习中监督学习的思想解决。在F设定好之后，$p_{u}$和$q_{i}$可以用随机数进行初始化，初始化之后，如何进行迭代呢？</p><p>这里采用的方法是梯度下降。分别从损失函数对user向量的偏导和损失函数对item向量的偏导，以及user向量对item向量的迭代来分别介绍。</p><p>（2）损失函数</p><p>LFM loss function</p><script type="math/tex; mode=display">l o s s=\sum_{(u, i) \in D}\left(p(u, i)-p^{L F M}(u, i)\right)^{2}</script><p>解释公式：p(u,i)是训练样本的label，也就是说如果user点击了item，那么p(u,i)=1，否则p(u,i)=0。后面项是模型预估的user对item喜好程度，也就是前面所说的模型产出的参数p_{u}和q_{i}转置的乘积。这里的D是所有的训练样本的集合。</p><p>可以看到如果模型预估的数值与label越接近的话，损失函数数值越小，反之则越大。这里为了防止过拟合，增加了正则化项。如下公式，前半部分是将模型对于user-item对的喜好程度，并进行展开，是前面建模公式中讲到的，得到如下：</p><script type="math/tex; mode=display">\operatorname{loss}=\sum_{(u, i) \in D}\left(p(u, i)-\sum_{f=1}^{F} p_{u f} q_{i f}\right)^{2}+\partial\left|p_{u}\right|^{2}+\partial\left|q_{i}\right|^{2}</script><p>这里$\partial$是正则化系数，是用来平衡平方损失与正则化项，这里采用的是L2正则化，正则化目的是为了让模型更加简单化，防止由于$p_{u}$和$q_{i}$过度拟合训练样本中的数据使模型的参数过度复杂，造成泛化能力减弱。</p><p>（3）LFM算法迭代</p><p>损失函数对$p_{uf}$和$q_{if}$的偏导：<br>$\frac{\partial l o s s}{\partial p_{u f}}=-2\left(p(u, i)-p^{L E M}(u, i)\right) q_{i f}+2 \partial p_{u f}$</p><p>$\frac{\partial l o s s}{\partial q_{i f}}=-2\left(p(u, i)-p^{L F M}(u, i)\right) p_{u f}+2 \partial q_{i f}$</p><p>损失函数对$p_{uf}$和$q_{if}$的偏导之后，我们应用梯度下降的方法可以看到：</p><p>$p_{u f}=p_{u f}-\beta \frac{\partial l o s s}{\partial p_{u f}}$</p><p>$q_{i f}=q_{i f}-\beta \frac{\partial l o s s}{\partial q_{i f}}$</p><p>其中，$\beta$是learning rate，即学习率。<br>编程时候也会按照上述思路来实现代码。</p><p>（4）影响因素</p><p>哪些参数的设定会影响最终的模型效果？</p><p>1.负样本的选取</p><p>比起正样本，负样本的数量是非常多的。因为，展现给用户的item比用户点击的item要多的多。我们要有一定的负采样规则，我们选取那些充分展现而用户没有点击的item作为负样本。</p><p>那么什么叫做充分展现？</p><p>充分展现是指，这个item在所有的用户中，已经有了比较高的展现次数。</p><p>然后，我们就可以用user没有点击过的物品中，按照该item在所有用户中的展现次数做排序（来降序），取一定数目的item作为负样本。这个一定数目只要保证对该user来说，它的正负样本均衡就可以。比如，一个用户点击了100个item，那么同样也取100个负样本来保证正负样本的均衡。</p><p>2.隐特征F、正则参数$\partial$、学习率（learning rate  $\beta$）</p><p>以上是对模型比较重要的三个参数。</p><p>其中正则参数$\partial$和学习率（learning rate  $\beta$）通常设置为0.01-0.05，隐特征个数F通常设置为10-32之间。</p><p>当然你可以在实验过程中，根据具体数据分布来固定一些参数，对另一些参数做差异化实验。</p><p>比如说我们固定了正则参数$\partial$和学习率（learning rate  $\beta$）都为0.01的情况下，我们来变隐特征F，将它从16-32-64等等取值来看最终的效果。上述的一些参数对模型的快速收敛和模型效果都是非常重要的。</p><h3 id="LFM（latent-factor-model）算法与CF算法的优缺点比较"><a href="#LFM（latent-factor-model）算法与CF算法的优缺点比较" class="headerlink" title="LFM（latent factor model）算法与CF算法的优缺点比较"></a>LFM（latent factor model）算法与CF算法的优缺点比较</h3><p>对于用户比较多的系统，用item CF比user CF更加具有可行性。</p><p>1.理论基础</p><p>LFM是比较传统的监督学习的打法，根据训练样本的label设定损失函数，利用最优化的方法使损失函数最小化。只不过这里的特征是隐特征，不像其他模型中的特征那样直观。</p><p>item CF是基于公式间求解相似度矩阵，相对来说，缺少了学习的过程。</p><p>所以，从理论基础的完备性上，LFM相对更好。</p><p>2.离线计算空间、时间复杂度</p><p>空间复杂度方面：item CF需要存储item sim表，需要的空间复杂度=物品数的平方；而LFM只需要存储user向量和item向量，所以空间复杂度=物品数目<em>隐类数+用户数目</em>隐类数。</p><p>相比较而言，显然LFM的空间复杂度更低。</p><p>时间复杂度方面：假设有M个用户，他们的平均点击序列长度为K，那么计算item相似度矩阵的时间复杂度=M<em>K</em>K；假设有D个样本，迭代N次，F是隐类的个数，那么训练LFM模型所需要的时间复杂度=D<em>F</em>N。由于LFM需要迭代，所以在耗时上略高于item CF，但是它们的耗时处于同一数量级。</p><p>3.在线推荐与推荐解释</p><p>item CF可以将item sim矩阵写入redis或者内存，线上基于用户实时点击去推荐，可以做到较好的响应用户的实时行为。</p><p>LFM由于得到user和item向量，在计算用户的toplike物品的时候，如果推荐系统的物品总量很大，那么就需要将每一个item做向量的点乘运算，复杂度是比较高的，离线计算相对来说比较耗时。得到用户的toplike物品表之后，也是写入redis或者内存之中，线上用户访问系统的时候，直接推荐计算好的toplike列表。但是这样就不能对用户的实时行为进行感知。现在像facebook也推出了一些向量召回的引擎，可以做到在线实时召回，但是依然也不能够在用户有了新行为之后立刻重新训练LFM模型得到新的向量。</p><p>所以，这这一方面，LFM效果略差。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;个性化推荐算法实践第02章基于邻域的个性化召回算法LFM&quot;&gt;&lt;a href=&quot;#个性化推荐算法实践第02章基于邻域的个性化召回算法LFM&quot; class=&quot;headerlink&quot; title=&quot;个性化推荐算法实践第02章基于邻域的个性化召回
      
    
    </summary>
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="LFM（latent factor model）算法" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/LFM%EF%BC%88latent-factor-model%EF%BC%89%E7%AE%97%E6%B3%95/"/>
    
      <category term="隐语义模型" scheme="http://enfangzhong.github.io/categories/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/LFM%EF%BC%88latent-factor-model%EF%BC%89%E7%AE%97%E6%B3%95/%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="个性化推荐算法" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="LFM（latent factor model）算法" scheme="http://enfangzhong.github.io/tags/LFM%EF%BC%88latent-factor-model%EF%BC%89%E7%AE%97%E6%B3%95/"/>
    
      <category term="隐语义模型" scheme="http://enfangzhong.github.io/tags/%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>程序员的浪漫爱心表白源码</title>
    <link href="http://enfangzhong.github.io/2019/05/13/Hexo%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2_%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%B5%AA%E6%BC%AB%E7%88%B1%E5%BF%83%E8%A1%A8%E7%99%BD%E6%BA%90%E7%A0%81/"/>
    <id>http://enfangzhong.github.io/2019/05/13/Hexo个人博客_程序员的浪漫爱心表白源码/</id>
    <published>2019-05-13T11:03:11.000Z</published>
    <updated>2019-07-27T16:53:53.104Z</updated>
    
    <content type="html"><![CDATA[<h1 id="程序员的浪漫爱心表白源码"><a href="#程序员的浪漫爱心表白源码" class="headerlink" title="程序员的浪漫爱心表白源码"></a>程序员的浪漫爱心表白源码</h1><hr><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>我们程序员在追求爱情方面也是非常浪漫的，某位同学利用自己所学的HTML5知识自制的HTML5爱心表白动画，画面非常温馨甜蜜，这样的创意很容易打动女孩，如果你是单身的程序员，也赶紧来制作自己的爱心表白动画吧。</p><blockquote><p>表白源码下载地址：<a href="https://github.com/enfangzhong/loveSource" target="_blank" rel="noopener">https://github.com/enfangzhong/loveSource</a></p><p>个人博客展示：<a href="https://enfangzhong.github.io/">https://enfangzhong.github.io/</a></p></blockquote><h2 id="在线演示"><a href="#在线演示" class="headerlink" title="在线演示"></a>在线演示</h2><ul><li><p>在我们rep中，我就展示了loveheart和love-ppt两个开源的表白动画。其他我就不一一全部上线演示了。</p></li><li><p>赶紧star吧。</p></li></ul><blockquote><p>loveheart演示地址：<a href="https://enfangzhong.github.io/love/">https://enfangzhong.github.io/love/</a></p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/13821160-f7b6477973f86e1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><blockquote><p>love-ppt演示地址：<a href="https://enfangzhong.github.io/loveshow/">https://enfangzhong.github.io/loveshow/</a></p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/13821160-773f036e969807a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/13821160-d23105c858bf3da4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="12套表白源码展示"><a href="#12套表白源码展示" class="headerlink" title="12套表白源码展示"></a>12套表白源码展示</h2><p><img src="https://upload-images.jianshu.io/upload_images/13821160-88080f6f0e22fd4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><blockquote><p>表白网页款式01源码</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/13821160-40576803f072c1a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><blockquote><p>表白网页款式02源码</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/13821160-a56318467e45b8b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>……….<br>……….<br>……….</p><blockquote><p>表白网页款式06源码</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/13821160-957555f77dfd686c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><blockquote><p>表白网页款式07源码</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/13821160-a23e93d0d508b27a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><blockquote><p>表白网页款式08源码</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/13821160-b6f8c34f46f3bf41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>……….<br>……….<br>……….</p><blockquote><p>表白网页款式11源码</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/13821160-745db3e4898218ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><blockquote><p>表白网页款式12源码</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/13821160-624163a77b9bc234.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>祝天下有情人终成眷属！show the love line with your Mrs.Right</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;程序员的浪漫爱心表白源码&quot;&gt;&lt;a href=&quot;#程序员的浪漫爱心表白源码&quot; class=&quot;headerlink&quot; title=&quot;程序员的浪漫爱心表白源码&quot;&gt;&lt;/a&gt;程序员的浪漫爱心表白源码&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; cl
      
    
    </summary>
    
      <category term="搭建个人博客" scheme="http://enfangzhong.github.io/categories/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="表白源码" scheme="http://enfangzhong.github.io/tags/%E8%A1%A8%E7%99%BD%E6%BA%90%E7%A0%81/"/>
    
      <category term="程序员表白源码" scheme="http://enfangzhong.github.io/tags/%E7%A8%8B%E5%BA%8F%E5%91%98%E8%A1%A8%E7%99%BD%E6%BA%90%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记_02决策树与随机森林</title>
    <link href="http://enfangzhong.github.io/2019/05/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_02%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>http://enfangzhong.github.io/2019/05/13/机器学习笔记_02决策树与随机森林/</id>
    <published>2019-05-13T09:03:11.000Z</published>
    <updated>2019-05-13T02:20:00.439Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习笔记-02决策树与随机森林"><a href="#机器学习笔记-02决策树与随机森林" class="headerlink" title="机器学习笔记_02决策树与随机森林"></a>机器学习笔记_02决策树与随机森林</h1><p>[TOC]</p><h1 id="从LR到决策树"><a href="#从LR到决策树" class="headerlink" title="从LR到决策树"></a>从LR到决策树</h1><h2 id="1、总体流程与核心问题"><a href="#1、总体流程与核心问题" class="headerlink" title="1、总体流程与核心问题"></a>1、总体流程与核心问题</h2><p>首先，在了解树模型之前，自然想到线性模型和树模型有什么区别呢？其中最重要的是，树形模型是一个一个特征进行处理，之前线性模型是所有特征给予权重相加得到一个新的值。决策树与逻辑回归的分类区别也在于此，逻辑回归是将所有特征通过sigmoid函数变换为概率后，通过大于某一概率阈值的划分为一类，小于某一概率阈值的为另一类；而决策树是对每一个特征做一个划分。另外逻辑回归只能找到线性分割（输入特征x与logit之间是线性的，除非对x进行多维映射），而决策树可以找到非线性分割。</p><p><strong>而树形模型更加接近人的思维方式，可以产生可视化的分类规则，产生的模型具有可解释性（可以抽取规则）。</strong>树模型拟合出来的函数其实是分区间的阶梯函数。</p><p><strong>决策树学习：采用自顶向下的递归的方法，基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处熵值为0（叶节点中的实例都属于一类）。</strong></p><p>  其次，需要了解几个重要的基本概念：根节点（最重要的特征）；父节点与子节点是一对，先有父节点，才会有子节点；叶节点（最终标签）。</p><h2 id="2、熵、信息增益、信息增益率"><a href="#2、熵、信息增益、信息增益率" class="headerlink" title="2、熵、信息增益、信息增益率"></a>2、熵、信息增益、信息增益率</h2><h3 id="信息熵-Information-Entropy"><a href="#信息熵-Information-Entropy" class="headerlink" title="信息熵(Information Entropy)"></a>信息熵(Information Entropy)</h3><p>　　信息熵是用来评估样本集合的纯度的一个参数，就是说，给出一个样本集合，这个样本集合中的样本可能属于好多不同的类别，也可能只属于一个类别，那么如果属于好多不同的类别的话，我们就说这个样本是不纯的，如果只属于一个类别，那么，我们就说这个样本是纯洁的。<br>　　而信息熵这个东西就是来计算一个样本集合中的数据是纯洁的还是不纯洁的。下面上公式：<br>　　 $Ent(D)=-\sum_{k=1}^{\left|y\right|}p_{k}log_{2}p_{k}$<br>　　下面解释一下公式的意思，其实很好理解，计算一个集合的纯度，就是把集合中每一个类别所占的比例$p_k$（k从1到 $\left | y \right |$，其中 $\left | y \right |$ 表示类别的个数）乘上它的对数，然后加到一起，然后经过计算之后，可以得到一个数据集的信息熵，然后根据信息熵，可以判断这个数据集是否纯粹。信息熵越小的话，表明这个数据集越纯粹。信息熵的最小值为0，此时数据集D中只含有一个类别。</p><h3 id="信息增益-Information-Gain"><a href="#信息增益-Information-Gain" class="headerlink" title="信息增益(Information Gain)"></a>信息增益(Information Gain)</h3><p>　　下面来介绍信息增益，所谓的信息增益，是要针对于具体的属性来讲的，比如说，数据集D中含有两个类别，分别是好人和坏人，那么，随便选择一个属性吧，比如说性别，性别这个属性中包含两个值，男人和女人，如果用男人和女人来划分数据集D的话，会得到两个集合，分别是$D_{man}$和$D_{woman}$。划分后的两个集合中各自有 好人和坏人，所以可以分别计算划分后两个集合的纯度，计算之后，把这两个集合的信息熵求加权平均$\frac{D_{man}}{D} Ent(D_{man})+\frac{D_{woman}}{D} Ent(D_{woman})$，跟之前没有划分的时候的信息熵$Ent(D)$相比较，用后者减去前者，得到的就是属性-性别对样本集D划分所得到的信息增益。可以通俗理解为，信息增益就是纯度提升值，用属性对原数据集进行划分后，得到的信息熵的差就是纯度的提升值。信息增益的公式如下： </p><p>$Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{\left | D^{v} \right |}{\left | D \right |}Ent(D^{v})$　　<br>　　先解释一下上式中的参数，D是数据集，a是选择的属性，a中一共有V个取值，用这个V取值去划分数据集D，分别得到数据集$D_1$到$D_V$，分别求这V个数据集的信息熵，并将其求加权平均。两者的差得到的就是信息增益。<br>　　那么这个信息增益有什么用呢？有用，可以根据信息增益值的大小来判断是否要用这个属性a去划分数据集D，如果得到的信息增益比较大，那么就说明这个属性是用来划分数据集D比较好的属性，否则则认为该属性不适合用来划分数据集D。这样有助于去构建决策树。<br>　　著名的<strong>算法ID3</strong>就是<strong>采用信息增益</strong>来作为判断是否用该属性划分数据集的标准。</p><h3 id="信息增益率-Information-Gain-Ratio"><a href="#信息增益率-Information-Gain-Ratio" class="headerlink" title="信息增益率(Information Gain Ratio)"></a>信息增益率(Information Gain Ratio)</h3><p>　　为什么要提出信息增益率这种评判划分属性的方法？信息增益不是就很好吗？其实不然，用信息增益作为评判划分属性的方法其实是有一定的缺陷的，书上说，信息增益准则对那些属性的取值比较多的属性有所偏好，也就是说，采用信息增益作为判定方法，会倾向于去选择属性取值比较多的属性。那么，选择取值多的属性为什么就不好了呢？举个比较极端的例子，如果将身份证号作为一个属性，那么，其实每个人的身份证号都是不相同的，也就是说，有多少个人，就有多少种取值，它的取值很多吧，让我们继续看，如果用身份证号这个属性去划分原数据集D，那么，原数据集D中有多少个样本，就会被划分为多少个子集，每个子集只有一个人，这种极端情况下，因为一个人只可能属于一种类别，好人，或者坏人，那么此时每个子集的信息熵就是0了，就是说此时每个子集都特别纯。这样的话，会导致信息增益公式的第二项$\sum_{v=1}^{V}\frac{\left | D^{v} \right |}{\left | D \right |}Ent(D^{v})$整体为0，这样导致的结果是，信息增益计算出来的特别大，然后决策树会用身份证号这个属性来划分原数据集D，其实这种划分毫无意义。因此，为了改变这种不良偏好带来的不利影响，提出了采用信息增益率作为评判划分属性的方法。<br>　　公式如下：<br>　　 $Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$<br>　　其中$IV(a)$的计算方式如下：<br>　　 $IV(a)=-\sum_{v=1}^{V}\frac{\left | D^v \right |}{\left | D \right |}log_2\frac{\left | D^v \right |}{\left | D \right |}$<br>　　$IV(a)$被称为是的“固有值”，这个$IV(a)$的公式是不是很熟悉啊，简直和信息熵的计算公式一毛一样，就是看属性a的纯度，如果a只含有少量的取值的话，那么a的纯度就比较高，否则的话，a的取值越多，a的纯度越低，$IV(a)$的值也就越大，因此，最后得到的信息增益率就越低。<br>　　采用信息增益率可以解决ID3算法中存在的问题(ID3会对那些属性的取值比较多的属性有所偏好，如西瓜的颜色有10种)，因此将采用信息增益率作为判定划分属性好坏的方法称为C4.5。<br>　　需要注意的是，<em>增益率准则对属性取值较少的时候会有偏好</em>，为了解决这个问题，C4.5并不是直接选择增益率最大的属性作为划分属性，而是之前先通过一遍筛选，先把信息增益低于平均水平的属性剔除掉，之后从剩下的属性中选择信息增益率最高的，这样的话，相当于两方面都得到了兼顾。 （结合信息增益与信息增益率使用）</p><p>采用信息增益率可以解决ID3算法中存在的问题，因此将采用信息增益率作为判定划分属性好坏的方法称为C4.5。需要注意的是，增益率准则对属性取值较少的时候会有偏好，为了解决这个问题，C4.5并不是直接选择增益率最大的属性作为划分属性，而是之前先通过一遍筛选，先把信息增益低于平均水平的属性剔除掉，之后从剩下的属性中选择信息增益率最高的，这样的话，相当于两方面都得到了兼顾。 </p><h3 id="基尼指数-gini-index-CART中使用"><a href="#基尼指数-gini-index-CART中使用" class="headerlink" title="基尼指数(gini index):CART中使用"></a>基尼指数(gini index):CART中使用</h3><p>定义：</p><ul><li>是一种不等性度量；</li><li>通常用来度量收入不平衡，可以用来度量任何不均匀分布；</li><li>是介于0~1之间的数，0-完全相等，1-完全不相等；</li><li>总体内包含的类别越杂乱，基尼指数就越大</li></ul><h4 id="基尼不纯度指标"><a href="#基尼不纯度指标" class="headerlink" title="基尼不纯度指标"></a>基尼不纯度指标</h4><p>在CART算法中, 基尼不纯度表示一个随机选中的样本在子集中被分错的可能性。基尼不纯度为这个样本被选中的概率乘以它被分错的概率。当一个节点中所有样本都是一个类时，基尼不纯度为零。<br>假设y的可能取值为{1, 2, …, m},令fifi是样本被赋予i的概率，则基尼指数可以通过如下计算： </p><p>$\begin{aligned} \operatorname{Gini}(D) &amp;=\sum_{k=1}^{|\mathcal{Y}|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\ &amp;=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2} \end{aligned}$</p><p>反映了从D中随机抽取两个样例，其类别标签不一致的概率。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器学习笔记-02决策树与随机森林&quot;&gt;&lt;a href=&quot;#机器学习笔记-02决策树与随机森林&quot; class=&quot;headerlink&quot; title=&quot;机器学习笔记_02决策树与随机森林&quot;&gt;&lt;/a&gt;机器学习笔记_02决策树与随机森林&lt;/h1&gt;&lt;p&gt;[TOC]&lt;/p&gt;

      
    
    </summary>
    
      <category term="机器学习" scheme="http://enfangzhong.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://enfangzhong.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="决策树" scheme="http://enfangzhong.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
      <category term="随机森林" scheme="http://enfangzhong.github.io/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    
      <category term="信息增益" scheme="http://enfangzhong.github.io/tags/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A/"/>
    
      <category term="信息增益率" scheme="http://enfangzhong.github.io/tags/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87/"/>
    
      <category term="基尼指数" scheme="http://enfangzhong.github.io/tags/%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记_01线性回归和逻辑回归</title>
    <link href="http://enfangzhong.github.io/2019/05/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_01%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://enfangzhong.github.io/2019/05/10/机器学习笔记_01线性回归和逻辑回归/</id>
    <published>2019-05-10T09:03:11.000Z</published>
    <updated>2019-05-11T08:52:47.705Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习笔记-01线性回归和逻辑回归"><a href="#机器学习笔记-01线性回归和逻辑回归" class="headerlink" title="机器学习笔记_01线性回归和逻辑回归"></a>机器学习笔记_01线性回归和逻辑回归</h1><p>[TOC]</p><h3 id="一、什么是机器学习"><a href="#一、什么是机器学习" class="headerlink" title="一、什么是机器学习"></a>一、什么是机器学习</h3><p>利用大量的数据样本，使得计算机通过不断的学习获得一个模型，用来对新的未知数据做预测。</p><ul><li><strong>有监督学习（分类、回归）</strong></li></ul><p>同时将数据样本和标签输入给模型，模型学习到数据和标签的映射关系，从而对新数据进行预测。</p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-35506d38cbd5e8cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li><p><strong>无监督学习（聚类）</strong><br>只有数据，没有标签，模型通过总结规律，从数据中挖掘出信息。<br><img src="https://upload-images.jianshu.io/upload_images/13821160-2d6b2857e3cd1378.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p><strong>强化学习</strong><br>强化学习会在没有任何标签的情况下，通过先尝试做出一些行为得到一个结果，通过这个结果是对还是错的反馈，调整之前的行为，就这样不断的调整，算法能够学习到在什么样的情况下选择什么样的行为可以得到最好的结果。</p></li></ul><p>就好比你有一只还没有训练好的小狗，每当它把屋子弄乱后，就减少美味食物的数量（惩罚），每次表现不错时，就加倍美味食物的数量（奖励），那么小狗最终会学到一个知识，就是把客厅弄乱是不好的行为。</p><p>【David Silve强化学习课程】：</p><ul><li>推荐David Silver的Reinforcement Learning Course</li><li>课件链接：<a href="https://github.com/enfangzhong/DavidSilverRLPPT" target="_blank" rel="noopener">https://github.com/enfangzhong/DavidSilverRLPPT</a></li></ul><ul><li><strong>机器学习基本术语与概念</strong><br><img src="https://upload-images.jianshu.io/upload_images/13821160-4ab12fca72407710.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li></ul><h3 id="二、线性回归"><a href="#二、线性回归" class="headerlink" title="二、线性回归"></a>二、线性回归</h3><p>利用大量的样本$D=\left(x_{i}, y_{i}\right)_{i=1}^{N}$，通过有监督的学习，学习到由x到y的映射f，利用该映射关系对未知的数据进行预估，因为y为连续值，所以是回归问题。<br><img src="https://upload-images.jianshu.io/upload_images/13821160-c7abfa8040768809.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li><p><strong>单变量情况</strong><br><img src="https://upload-images.jianshu.io/upload_images/13821160-b0faa2b1aace28ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p><strong>多变量情况</strong></p></li></ul><p>二维空间的直线，转化为高维空间的平面<br><img src="https://upload-images.jianshu.io/upload_images/13821160-0d11e9c8317bd70c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="2-1-线性回归的表达式"><a href="#2-1-线性回归的表达式" class="headerlink" title="2.1 线性回归的表达式"></a>2.1 线性回归的表达式</h4><p>机器学习是数据驱动的算法，数据驱动=数据+模型，模型就是输入到输出的映射关系。</p><p><strong>模型=假设函数（不同的学习方式）+优化</strong></p><h5 id="1-假设函数"><a href="#1-假设函数" class="headerlink" title="1. 假设函数"></a><strong>1. 假设函数</strong></h5><p>线性回归的假设函数（$\theta_{0}$表示截距项，$x_0=1$,方便矩阵表达）：</p><p>$f(x)=\theta_{0} x_{0}+\theta_{1} x_{1}+\theta_{2} x_{2} \ldots+\theta_{n} x_{n}$<br>向量形式（θ,x都是列向量）：<br>$f(x)=\theta^{T} x$<br><img src="https://upload-images.jianshu.io/upload_images/13821160-f8f5cbafb89b3e1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h5 id="2-优化方法"><a href="#2-优化方法" class="headerlink" title="2. 优化方法"></a><strong>2. 优化方法</strong></h5><p>监督学习的优化方法=损失函数+对损失函数的优化</p><h5 id="3-损失函数"><a href="#3-损失函数" class="headerlink" title="3. 损失函数"></a><strong>3. 损失函数</strong></h5><p><strong>如何衡量已有的参数$\theta$的好坏？</strong></p><p>利用损失函数来衡量，损失函数度量预测值和标准答案的偏差，不同的参数有不同的偏差，所以要通过最小化损失函数，也就是最小化偏差来得到最好的参数。<br>映射函数:<br>$h_{\theta}(x)$<br>损失函数：<br>$J\left(\theta_{0}, \theta_{1}, \ldots, \theta_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$</p><p>解释：因为有m个样本，所以要平均，分母的2是为了求导方便</p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-d443068c17710143.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>最小化损失函数（ loss function）：凸函数</p><h5 id="4-损失函数的优化"><a href="#4-损失函数的优化" class="headerlink" title="4. 损失函数的优化"></a><strong>4. 损失函数的优化</strong></h5><p>损失函数如右图所示，是一个凸函数，我们的目标是达到最低点，也就是使得损失函数最小。<br><img src="https://upload-images.jianshu.io/upload_images/13821160-292b6853d5dc92f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>多元情况下容易出现局部极值</p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-c63f4eea68c08364.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>求极值的数学思想，对公式求导=0即可得到极值，但是工业上计算量很大，公式很复杂，所以从计算机的角度来讲，求极值是利用<strong>梯度下降</strong>法。</p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-7443d50d51ca57ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>① 初始位置选取很重要</p><p>② 复梯度方向更新，二维情况下，函数变换最快的方向是斜率方向，多维情况下就成为梯度，梯度表示函数值增大的最快的方向，所以要在负梯度方向上进行迭代。</p><p>③ θ的更新公式如上图，每个参数 $\theta_1,\theta_2…$ 都是分别更新的</p><p><strong>高维情况：梯度方向就是垂直于登高线的方向</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-8d5ce93c1c479b77.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>参数更新示例：<br><img src="https://upload-images.jianshu.io/upload_images/13821160-de69bf224159c8b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对每个theta都进行更新：</p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-6bd3d85040ae207c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>学习率：</strong></p><p>① 学习率太大，会跳过最低点，可能不收敛<br>② 学习率太小收敛速度过慢<br><img src="https://upload-images.jianshu.io/upload_images/13821160-eb31e7887213a716.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h5 id="5-过拟合和欠拟合（underfitting-vs-overfitting）"><a href="#5-过拟合和欠拟合（underfitting-vs-overfitting）" class="headerlink" title="5. 过拟合和欠拟合（underfitting vs overfitting）"></a><strong>5. 过拟合和欠拟合（underfitting vs overfitting）</strong></h5><p><img src="https://upload-images.jianshu.io/upload_images/13821160-badb385bdff9b674.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>过拟合的原因：</strong><br>① 如果我们有很多的特征或模型很复杂，则假设函数曲线可以对训练样本拟合的非常好$\left(J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} \approx 0\right)$，学习能力太强了，但是丧失了一般性。从而导致对新给的待预测样本，预测效果差.<br>② 眼见不一定为实，训练样本中肯定存在噪声点，如果全都学习的话肯定会将噪声也学习进去。</p><p><strong>过拟合造成什么结果：</strong></p><p>过拟合是给参数的自由空间太大了，可以通过简单的方式让参数变化太快，并未学习到底层的规律，模型抖动太大，很不稳定，variance变大，对新数据没有泛化能力。</p><p>所有的模型都可能存在过拟合的风险：</p><ul><li>更多的参数，更复杂的模型，意味着有更强的能力， 但也更可能无法无天</li><li>眼见不一定为实，你看到的内容不一定是全部真实的数据分布，死记硬背不太好</li></ul><h5 id="6-利用正则化解决过拟合问题"><a href="#6-利用正则化解决过拟合问题" class="headerlink" title="6. 利用正则化解决过拟合问题"></a><strong>6. 利用正则化解决过拟合问题</strong></h5><p>正则化的作用：</p><p>① 控制参数变化幅度，对变化大的参数惩罚，不让模型“无法无天”</p><p>② 限制参数搜索空间</p><p>添加正则化的损失函数<br>$J\left(\theta_{0}, \theta_{1}, \ldots, \theta_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}-y^{(i)}\right)^{2}+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}\right.$</p><p>m：样本有m个<br>n：n个参数，对n个参数进行惩罚<br>λ：对误差的惩罚程度，λ 越大对误差的惩罚越大，容易出现过拟合，λ越小，对误差的惩罚越小，对误差的容忍度越大，泛化能力好。</p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-178c1c611a9baa50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h5 id="7-线性回归代码实例"><a href="#7-线性回归代码实例" class="headerlink" title="7. 线性回归代码实例"></a><strong>7. 线性回归代码实例</strong></h5><p><img src="https://upload-images.jianshu.io/upload_images/13821160-a9ce7b446e79d84e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="三、逻辑回归"><a href="#三、逻辑回归" class="headerlink" title="三、逻辑回归"></a>三、逻辑回归</h3><p>监督学习，解决二分类问题。</p><p>分类的本质：在空间中找到一个决策边界来完成分类的决策</p><p>逻辑回归：线性回归可以预测连续值，但是不能解决分类问题，我们需要根据预测的结果判定其属于正类还是负类。所以逻辑回归就是将线性回归的$(-\infty,+\infty)$结果，通过sigmoid函数映射到(0,1) 之间。<br>线性回归决策函数：$h_{\theta}(x)=\theta^{T} x$</p><p><strong>sigmoid函数：</strong><br>$g(z)=\frac{1}{1+e^{-z}}$</p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-94308262a2bd4d59.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>① 可以对$(-\infty,+\infty)$结果，映射到(0,1) 之间，作为概率。</p><p>② $x<0,$ sigmoid(x)$<\frac{1}{2}="" ;="" x="">0,$ sigmoid $(x)&gt;\frac{1}{2}$，可以将$ \frac{1}{2} $作为决策边界。</0,$></p><p>③ 数学特性好，求导容易：$g^{\prime}(z)=g(z) \cdot(1-g(z))$</p><h5 id="逻辑回归的决策函数"><a href="#逻辑回归的决策函数" class="headerlink" title="逻辑回归的决策函数"></a><strong>逻辑回归的决策函数</strong></h5><p>将线性回归决策函数通过sigmoid函数，获得逻辑回归的决策函数：$h_{\theta}(x)=g\left(\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}\right)=\frac{1}{1+e^{-\theta^T {x}}}$</p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-88532ed9a1685b87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="3-1-逻辑回归的损失函数"><a href="#3-1-逻辑回归的损失函数" class="headerlink" title="3.1 逻辑回归的损失函数"></a>3.1 逻辑回归的损失函数</h4><p>线性回归的损失函数为平方损失函数，如果将其用于逻辑回归的损失函数，则其数学特性不好，有很多局部极小值，难以用梯度下降法求最优。<br>$J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \frac{1}{2}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$</p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-cdf9addb70bd5a96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-0b6d1fe731b0b756.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>逻辑回归损失函数：对数损失函数</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-21d98340590f161d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>解释：如果一个样本为正样本，那么我们希望将其预测为正样本的概率p越大越好，也就是决策函数的值越大越好，则logp越大越好，逻辑回归的决策函数值就是样本为正的概率；</p><p>如果一个样本为负样本，那么我们希望将其预测为负样本的概率越大越好，也就是(1-p)越大越好，即log(1-p)越大越好。</p><p><strong>为什么要用log：</strong></p><p>样本集中有很多样本，要求其概率连乘，概率为(0,1)间的数，连乘越来越小，利用log变换将其变为连加，不会溢出，不会超出计算精度。</p><p><strong>逻辑回归损失函数：</strong><br><img src="https://upload-images.jianshu.io/upload_images/13821160-6344d206b32e3ff8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-0a3c7f18d7da5746.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>$\frac{\partial J(\theta)}{\partial \theta_{j}}=\frac{1}{m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}\right]$</p><h4 id="3-2-逻辑回归实现多分类"><a href="#3-2-逻辑回归实现多分类" class="headerlink" title="3.2 逻辑回归实现多分类"></a>3.2 逻辑回归实现多分类</h4><p><img src="https://upload-images.jianshu.io/upload_images/13821160-e1a911c78ec981b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li><p>一对一(one vs one)<br>一对一分类，每两个类之间构建一个分类器，共需要$\frac{N(N-1)}{2} $ 个分类器</p></li><li><p>一对多（one vs rest）<br>一对多分类器，每个分类器判断是三角形还是不是三角形，共需要N个分类器。</p></li></ul><h4 id="3-3-逻辑回归代码实现"><a href="#3-3-逻辑回归代码实现" class="headerlink" title="3.3 逻辑回归代码实现"></a>3.3 逻辑回归代码实现</h4><p><img src="https://upload-images.jianshu.io/upload_images/13821160-effbaad8b507c739.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-98a0baf55abffa97.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-9be3cbfb874309d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-fba5c8c8fbba9422.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-d23d01f413a344d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>四、LR的特点<br>可解释性高，工业中可控度高。<br><img src="https://upload-images.jianshu.io/upload_images/13821160-2740c90b17518171.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/13821160-686751f431a93f56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="五、-为什么逻辑回归比线性回归好"><a href="#五、-为什么逻辑回归比线性回归好" class="headerlink" title="五、 为什么逻辑回归比线性回归好"></a>五、 为什么逻辑回归比线性回归好</h3><p>虽然逻辑回归能够用于分类，不过其本质还是线性回归。它仅在线性回归的基础上，在特征到结果的映射中加入了一层sigmoid函数（非线性）映射，即先把特征线性求和，然后使用sigmoid函数来预测。</p><p>这主要是由于线性回归在整个实数域内敏感度一致，而分类范围，需要在[0,1]之内。而逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，其回归方程与回归曲线如下图所示。逻辑曲线在z=0时，十分敏感，在z&gt;&gt;0或z&lt;&lt;0处，都不敏感，将预测值限定为(0,1)。</p><p>LR在线性回归的实数范围输出值上施加sigmoid函数将值收敛到0~1范围, 其目标函数也因此从差平方和函数变为对数损失函数, 以提供最优化所需导数（sigmoid函数是softmax函数的二元特例, 其导数均为函数值的f*(1-f)形式）。请注意, LR往往是解决二元0/1分类问题的, 只是它和线性回归耦合太紧, 不自觉也冠了个回归的名字(马甲无处不在). 若要求多元分类,就要把sigmoid换成大名鼎鼎的softmax了。</p><p>首先逻辑回归和线性回归首先都是广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。</p><p>逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。</p><h3 id="六、-LR和SVM的关系"><a href="#六、-LR和SVM的关系" class="headerlink" title="六、 LR和SVM的关系"></a>六、 LR和SVM的关系</h3><p>1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）</p><p>2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。</p><p>区别：</p><p>1、LR是参数模型，SVM是非参数模型。</p><p>2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。</p><p>3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。</p><p>4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。</p><p>5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。</p><p>博客中的PPT笔记地址：<a href="https://github.com/enfangzhong/ML_Material" target="_blank" rel="noopener">https://github.com/enfangzhong/ML_Material</a><br>（感谢七月在线机器学习课程。仅供学习交流,严禁用于商业用途）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器学习笔记-01线性回归和逻辑回归&quot;&gt;&lt;a href=&quot;#机器学习笔记-01线性回归和逻辑回归&quot; class=&quot;headerlink&quot; title=&quot;机器学习笔记_01线性回归和逻辑回归&quot;&gt;&lt;/a&gt;机器学习笔记_01线性回归和逻辑回归&lt;/h1&gt;&lt;p&gt;[TOC]&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://enfangzhong.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://enfangzhong.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://enfangzhong.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="逻辑回归" scheme="http://enfangzhong.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>SQL内连接、左外连接、右外连接、交叉连接区别</title>
    <link href="http://enfangzhong.github.io/2018/10/17/SQL%E5%86%85%E8%BF%9E%E6%8E%A5%E3%80%81%E5%B7%A6%E5%A4%96%E8%BF%9E%E6%8E%A5%E3%80%81%E5%8F%B3%E5%A4%96%E8%BF%9E%E6%8E%A5%E3%80%81%E4%BA%A4%E5%8F%89%E8%BF%9E%E6%8E%A5%E5%8C%BA%E5%88%AB/"/>
    <id>http://enfangzhong.github.io/2018/10/17/SQL内连接、左外连接、右外连接、交叉连接区别/</id>
    <published>2018-10-17T15:08:51.000Z</published>
    <updated>2019-05-10T13:36:19.402Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内连接、左外连接、右外连接、交叉连接区别"><a href="#内连接、左外连接、右外连接、交叉连接区别" class="headerlink" title="内连接、左外连接、右外连接、交叉连接区别"></a>内连接、左外连接、右外连接、交叉连接区别</h2><p>在之前，我对MSSQL中的内连接和外连接所得出的数据集不是很清楚。这几天重新温习了一下SQL的书本，现在的思路应该是很清楚了，现在把自己的理解发出来给大家温习下。希望和我一样对SQL的连接语句不太理解的朋友能够有所帮助。（发这么菜的教程，各位大大们别笑话偶了，呵:D ） 有两个表A和表B。表A结构如下： Aid：int；标识种子，主键，自增ID Aname：varchar 数据情况，即用select * from A出来的记录情况如下图1所示：</p><p><img src="/2018/10/17/SQL内连接、左外连接、右外连接、交叉连接区别/20181017111507739.png" alt=""><br>图1:A表数据</p><p>表B结构如下： Bid：int；标识种子，主键，自增ID Bnameid：int 数据情况，即用select * from B出来的记录情况如下图2所示：<br> <img src="/2018/10/17/SQL内连接、左外连接、右外连接、交叉连接区别/20181017111601913.png" alt=""></p><p>图2:B表数据</p><p>为了把Bid和Aid加以区分，不让大家有误解，所以把Bid的起始种子设置为100。有SQL基本知识的人都知道，两个表要做连接，就必须有个连接字段，从上表中的数据可以看出，在A表中的Aid和B表中的Bnameid就是两个连接字段。下图3说明了连接的所有记录集之间的关系：<br> <img src="/2018/10/17/SQL内连接、左外连接、右外连接、交叉连接区别/20181017111618065.png" alt=""><br>图3:连接关系图 </p><p>现在我们对内连接和外连接一一讲解。 1.内连接：利用内连接可获取两表的公共部分的记录，即图3的记录集C 语句如下：Select <em> from A JOIN B ON A.Aid=B.Bnameid 运行结果如下图4所示：其实select </em> from A,B where A.Aid=B.Bnameid与Select * from A JOIN B ON A.Aid=B.Bnameid的运行结果是一样的。<br> <img src="/2018/10/17/SQL内连接、左外连接、右外连接、交叉连接区别/20181017111637904.png" alt=""><br>图4:内连接数据</p><p> 2.外连接：外连接分为两种，一种是左连接（Left JOIN）和右连接（Right JOIN）<br> (1)左连接（Left JOIN）：即图3公共部分记录集C＋表A记录集A1。<br> 语句如下：select * from A Left JOIN B ON A.Aid=B.Bnameid<br>运行结果如下图5所示：<br> <img src="/2018/10/17/SQL内连接、左外连接、右外连接、交叉连接区别/20181017111658117.png" alt=""></p><p>图5:左连接数据</p><p>说明：在语句中，A在B的左边，并且是Left Join，所以其运算方式为：A左连接B的记录=图3公共部分记录集C＋表A记录集A1<br>在图3中即记录集C中的存在的Aid为：2 3 6 7 8<br>图1中即表A所有记录集A中存在的Aid为：1 2 3 4 5 6 7 8 9<br>表A记录集A1中存在的Aid=(图1中即A表中所有Aid)-(图3中即记录集C中存在的Aid)，最终得出为：1 4 5 9<br>由此得出图5中A左连接B的记录=图3公共部分记录集C＋表A记录集A1, 最终得出的结果图5中可以看出Bnameid及Bid非NULL的记录都为图3公共部分记录集C中的记录；Bnameid及Bid为NULL的Aid为1 4 5 9的四笔记录就是表A记录集A1中存在的Aid。</p><p>(2)右连接（Right JOIN）：即图3公共部分记录集C＋表B记录集B1。</p><p>语句如下：select * from A Right JOIN B ON A.Aid=B.Bnameid   运行结果如下图6所示：<br> <img src="/2018/10/17/SQL内连接、左外连接、右外连接、交叉连接区别/20181017111728233.png" alt=""><br>图6:右连接数据 </p><p>说明： 在语句中，A在B的左边，并且是Right Join，所以其运算方式为：A右连接B的记录=图3公共部分记录集C＋表B记录集B1<br>在图3中即记录集C中的存在的Aid为：2 3 6 7 8<br>图2中即表B所有记录集B中存在的Bnameid为：2 3 6 7 8 11<br>表B记录集B1中存在的Bnameid=(图2中即B表中所有Bnameid)-(图3中即记录集C中存在的Aid)，最终得出为：11<br>由此得出图6中A右连接B的记录=图3公共部分记录集C＋表B记录集B1, 最终得出的结果图6中可以看出Aid及Aname非NULL的记录都为图3公共部分记录集C中的记录；Aid及Aname为NULL的Aid为11的记录就是表B记录集B1中存在的Bnameid。    </p><p>交叉连接：两张表联合没有条件情况下，条数 = 图1 * 图2</p><p>交叉连接不带WHERE子句，它返回被连接的两个表所有数据行的笛卡尔积，返回结果集合中的数据行数等于第一个表中符合查询条件的数据行数乘以第二个表中符合查询条件的数据行数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内连接、左外连接、右外连接、交叉连接区别&quot;&gt;&lt;a href=&quot;#内连接、左外连接、右外连接、交叉连接区别&quot; class=&quot;headerlink&quot; title=&quot;内连接、左外连接、右外连接、交叉连接区别&quot;&gt;&lt;/a&gt;内连接、左外连接、右外连接、交叉连接区别&lt;/h2&gt;&lt;
      
    
    </summary>
    
      <category term="JavaWeb" scheme="http://enfangzhong.github.io/categories/JavaWeb/"/>
    
    
      <category term="SQL内连接" scheme="http://enfangzhong.github.io/tags/SQL%E5%86%85%E8%BF%9E%E6%8E%A5/"/>
    
      <category term="左外连接" scheme="http://enfangzhong.github.io/tags/%E5%B7%A6%E5%A4%96%E8%BF%9E%E6%8E%A5/"/>
    
      <category term="右外连接" scheme="http://enfangzhong.github.io/tags/%E5%8F%B3%E5%A4%96%E8%BF%9E%E6%8E%A5/"/>
    
      <category term="交叉连接" scheme="http://enfangzhong.github.io/tags/%E4%BA%A4%E5%8F%89%E8%BF%9E%E6%8E%A5/"/>
    
  </entry>
  
  <entry>
    <title>Git个人总结笔记</title>
    <link href="http://enfangzhong.github.io/2018/09/06/Git%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://enfangzhong.github.io/2018/09/06/Git个人总结笔记/</id>
    <published>2018-09-06T03:16:28.000Z</published>
    <updated>2019-05-10T13:33:55.340Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Git个人总结笔记"><a href="#Git个人总结笔记" class="headerlink" title="Git个人总结笔记"></a>Git个人总结笔记</h1><h2 id="初始化一个Git仓库"><a href="#初始化一个Git仓库" class="headerlink" title="初始化一个Git仓库"></a>初始化一个Git仓库</h2><p>使用git init命令。</p><h2 id="添加文件到Git仓库"><a href="#添加文件到Git仓库" class="headerlink" title="添加文件到Git仓库"></a>添加文件到Git仓库</h2><p>分两步：</p><ul><li>使用命令git add <file>，注意，可反复多次使用，添加多个文件；</file></li><li>使用命令git commit -m <message>，完成。</message></li></ul><h2 id="查看git仓库状态"><a href="#查看git仓库状态" class="headerlink" title="查看git仓库状态"></a>查看git仓库状态</h2><p>要随时掌握工作区的状态，使用git status命令。</p><p>如果git status告诉你有文件被修改过，用git diff可以查看修改内容。</p><h2 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h2><p>HEAD指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用命令git reset —hard commit_id。</p><ul><li><p>穿梭前，用git log可以查看提交历史，以便确定要回退到哪个版本。<br>如：git reset —hard HEAD^</p></li><li><p>要重返未来，用git reflog查看命令历史，以便确定要回到未来的哪个版本。<br>如：git reset —hard commit_id</p></li></ul><p>git diff HEAD — readme.txt：<br>查看工作区和版本库里面最新版本的区别</p><h2 id="撤销修改"><a href="#撤销修改" class="headerlink" title="撤销修改"></a>撤销修改</h2><p>git checkout — readme.txt<br>把readme.txt文件在工作区的修改全部撤销</p><ul><li><p>一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；</p></li><li><p>一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。</p></li></ul><p>就是让这个文件回到最近一次<code>git commit</code>或<code>git add</code>时的状态</p><p>git reset HEAD <file><br>可以把暂存区的修改撤销掉（unstage），重新放回工作区</file></p><ul><li>场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout — file。</li><li>场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD <file>，就回到了场景1，第二步按场景1操作。</file></li><li>场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考版本回退一节，不过前提是没有推送到远程库。</li></ul><p>git rm用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失最近一次提交后你修改的内容。<br>git rm —cached <file>…</file></p><h2 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h2><p>创建SSH Key：<br>$ ssh-keygen -t rsa -C “youremail@example.com”<br>github添加SHH</p><p>本地仓库进行远程同步<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:michaelliao/learngit.git</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><p>第一次push加上-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来</p><p>以后只要本地作了提交，就可以通过命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin master</span><br></pre></td></tr></table></figure></p><p>远程仓库tips:</p><ul><li><p>要关联一个远程库，使用命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@server-name:path/repo-name.git；</span><br></pre></td></tr></table></figure></li><li><p>关联后，使用命令git push -u origin master第一次推送master分支的所有内容；</p></li><li><p>此后，每次本地提交后，只要有必要，就可以使用命令git push origin master推送最新修改；</p></li></ul><h2 id="远程仓库克隆"><a href="#远程仓库克隆" class="headerlink" title="远程仓库克隆"></a>远程仓库克隆</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:michaelliao/gitskills.git</span><br></pre></td></tr></table></figure><h2 id="分支管理（重要）"><a href="#分支管理（重要）" class="headerlink" title="分支管理（重要）"></a>分支管理（重要）</h2><p>master指向最新的提交<br>HEAD指向的就是当前分支</p><p>只有master分支的时候：<br>每次提交，master分支都会向前移动一步，这样，随着你不断提交，master分支的线也越来越长</p><p>当我们创建新的分支，例如dev时，Git新建了一个指针叫dev，指向master相同的提交，再把HEAD指向dev，就表示当前分支在dev上：</p><p><img src="/2018/09/06/Git个人总结笔记/20180906012830969.png" alt=""></p><p>从现在开始，对工作区的修改和提交就是针对dev分支了，比如新提交一次后，dev指针往前移动一步，而master指针不变<br><img src="/2018/09/06/Git个人总结笔记/20180906013006354.png" alt=""><br>假如我们在dev上的工作完成了，就可以把dev合并到master上。Git怎么合并呢？最简单的方法，就是直接把master指向dev的当前提交，就完成了合并：<br><img src="/2018/09/06/Git个人总结笔记/20180906013104230.png" alt=""><br>合并完分支后，甚至可以删除dev分支。删除dev分支就是把dev指针给删掉，删掉后，我们就剩下了一条master分支：<br><img src="/2018/09/06/Git个人总结笔记/20180906013153959.png" alt=""></p><p>下面开始实战。<br>首先，我们创建dev分支，然后切换到dev分支：<br>git checkout -b dev</p><p>git checkout命令加上-b参数表示创建并切换，相当于以下两条命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git branch dev</span><br><span class="line">git checkout dev</span><br></pre></td></tr></table></figure></p><p>git branch命令查看当前分支</p><p>把dev分支的工作成果合并到master分支上:<br>git merge dev</p><p>合并后删除分支：<br>git branch -d dev</p><p>总结：<br>Git鼓励大量使用分支：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">查看分支：git branch</span><br><span class="line"></span><br><span class="line">创建分支：git branch &lt;name&gt;</span><br><span class="line"></span><br><span class="line">切换分支：git checkout &lt;name&gt;</span><br><span class="line"></span><br><span class="line">创建+切换分支：git checkout -b &lt;name&gt;</span><br><span class="line"></span><br><span class="line">合并某分支到当前分支：git merge &lt;name&gt;</span><br><span class="line"></span><br><span class="line">删除分支：git branch -d &lt;name&gt;</span><br></pre></td></tr></table></figure></p><p>本地库上使用命令git remote add把它和码云的远程库关联<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@gitee.com:liaoxuefeng/learngit.git</span><br></pre></td></tr></table></figure></p><p>git remote -v查看远程库信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br><span class="line">origin    git@github.com:michaelliao/learngit.git (fetch)</span><br><span class="line">origin    git@github.com:michaelliao/learngit.git (push)</span><br></pre></td></tr></table></figure></p><p>删除已关联的名为origin的远程库：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote rm origin</span><br></pre></td></tr></table></figure></p><p>关联GitHub的远程库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add github git@github.com:michaelliao/learngit.git</span><br></pre></td></tr></table></figure></p><p>关联码云的远程库：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add gitee git@gitee.com:liaoxuefeng/learngit.git</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br><span class="line">gitee    git@gitee.com:liaoxuefeng/learngit.git (fetch)</span><br><span class="line">gitee    git@gitee.com:liaoxuefeng/learngit.git (push)</span><br><span class="line">github    git@github.com:michaelliao/learngit.git (fetch)</span><br><span class="line">github    git@github.com:michaelliao/learngit.git (push)</span><br></pre></td></tr></table></figure><p>如果要推送到GitHub，使用命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push github master</span><br></pre></td></tr></table></figure></p><p>如果要推送到码云，使用命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push gitee master</span><br></pre></td></tr></table></figure></p><h2 id="为开源项目贡献代码"><a href="#为开源项目贡献代码" class="headerlink" title="为开源项目贡献代码"></a>为开源项目贡献代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:enfangzhong/bootstrap.git</span><br></pre></td></tr></table></figure><p>添加推特公司bootstrap项目远程仓库<br>git remote add upstream git@github.com:twbs/bootstrap.git</p><p>查看建立连接的远程仓库<br><img src="/2018/09/06/Git个人总结笔记/20180906023141140.png" alt=""></p><p>1、首先拉取推特公司最新代码<br>git pull upstream master</p><p>2、自己创建分支<br>git checkout -b feature/add_sth</p><p>然后去修改你自己代码</p><p>git status查看状态</p><p>git add ./</p><p>git commit -m “add sth”</p><p>3.切换到主分支，继续拉取网上最新代码</p><p>git checkout master<br>git pull upstream master</p><p>4.切换到分支，进行测试，<br>git checkout feature/add_sth</p><p>5.合并分支<br>git rebase master</p><p>git push origin feature/add_sth</p><p><img src="/2018/09/06/Git个人总结笔记/20180906030018530.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Git个人总结笔记&quot;&gt;&lt;a href=&quot;#Git个人总结笔记&quot; class=&quot;headerlink&quot; title=&quot;Git个人总结笔记&quot;&gt;&lt;/a&gt;Git个人总结笔记&lt;/h1&gt;&lt;h2 id=&quot;初始化一个Git仓库&quot;&gt;&lt;a href=&quot;#初始化一个Git仓库&quot; cla
      
    
    </summary>
    
      <category term="JavaWeb" scheme="http://enfangzhong.github.io/categories/JavaWeb/"/>
    
    
      <category term="Git" scheme="http://enfangzhong.github.io/tags/Git/"/>
    
      <category term="GiT撤销" scheme="http://enfangzhong.github.io/tags/GiT%E6%92%A4%E9%94%80/"/>
    
  </entry>
  
  <entry>
    <title>JaveEE请求转发和重定向的区别</title>
    <link href="http://enfangzhong.github.io/2018/09/05/JaveEE%E8%AF%B7%E6%B1%82%E8%BD%AC%E5%8F%91%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://enfangzhong.github.io/2018/09/05/JaveEE请求转发和重定向的区别/</id>
    <published>2018-09-05T06:32:29.000Z</published>
    <updated>2019-05-12T04:37:34.476Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、请求转发和重定向"><a href="#一、请求转发和重定向" class="headerlink" title="一、请求转发和重定向"></a>一、请求转发和重定向</h2><h3 id="请求转发："><a href="#请求转发：" class="headerlink" title="请求转发："></a>请求转发：</h3><p>request.getRequestDispatcher(URL地址).forward(request, response)</p><p>处理流程：</p><ul><li>客户端发送请求，Servlet做出业务逻辑处理。</li><li>Servlet调用forword()方法，服务器Servlet把目标资源返回给客户端浏览器。<br><img src="https://i.loli.net/2018/09/06/5b9128febacfe.png" alt=""></li></ul><h3 id="重定向："><a href="#重定向：" class="headerlink" title="重定向："></a>重定向：</h3><p>response.sendRedirect(URL地址)</p><p>处理流程：</p><ul><li>客户端发送请求，Servlet做出业务逻辑处理。</li><li>Servlet调用response.sendReadirect()方法，把要访问的目标资源作为response响应头信息发给客户端浏览器。</li><li>客户端浏览器重新访问服务器资源xxx.jsp，服务器再次对客户端浏览器做出响应。<br><img src="https://i.loli.net/2018/09/06/5b9129b0968aa.png" alt=""><br>重定向<br>以上两种情况，你都需要考虑Servlet处理完后，数据如何在jsp页面上呈现。图例是请求、响应的流程，没有标明数据如何处理、展现。</li></ul><h2 id="二、转发和重定向的路径问题"><a href="#二、转发和重定向的路径问题" class="headerlink" title="二、转发和重定向的路径问题"></a>二、转发和重定向的路径问题</h2><p>1）使用相对路径在重定向和转发中没有区别<br>2）重定向和请求转发使用绝对路径时，根/路径代表了不同含义<br>重定向response.sendRedirect(“xxx”)是服务器向客户端发送一个请求头信息，由客户端再请求一次服务器。/指的Tomcat的根目录,写绝对路径应该写成”/当前Web程序根名称/资源名” 。如”/WebModule/login.jsp”,”/bbs/servlet/LoginServlet”<br>转发是在服务器内部进行的，写绝对路径/开头指的是当前的Web应用程序。绝对路径写法就是是”/login.jsp”或”/servlet/LoginServlet”。</p><p><strong>总结：</strong>以上要注意是区分是从服务器外的请求，还在是内部转发，从服务器外的请求，从Tomcat根写起(就是要包括当前Web的根)；是服务器内部的转发，很简单了，因为在当前服务器内，/写起指的就是当前Web的根目录。</p><h2 id="三、转发和重定向的区别"><a href="#三、转发和重定向的区别" class="headerlink" title="三、转发和重定向的区别"></a>三、转发和重定向的区别</h2><ul><li><p>request.getRequestDispatcher()是容器中控制权的转向，在客户端浏览器地址栏中不会显示出转向后的地址；服务器内部转发，整个过程处于同一个请求当中。<br>response.sendRedirect()则是完全的跳转，浏览器将会得到跳转的地址，并重新发送请求链接。这样，从浏览器的地址栏中可以看到跳转后的链接地址。不在同一个请求。重定向，实际上客户端会向服务器端发送两个请求。<br>所以转发中数据的存取可以用request作用域：request.setAttribute(), request.getAttribute()，重定向是取不到request中的数据的。只能用session。</p></li><li><p>forward()更加高效，在可以满足需要时，尽量使用RequestDispatcher.forward()方法。（思考一下为什么？）</p></li><li><p>RequestDispatcher是通过调用HttpServletRequest对象的getRequestDispatcher()方法得到的，是属于请求对象的方法。<br>sendRedirect()是HttpServletResponse对象的方法，即响应对象的方法，既然调用了响应对象的方法，那就表明整个请求过程已经结束了，服务器开始向客户端返回执行的结果。</p></li><li><p>重定向可以跨域访问，而转发是在web服务器内部进行的，不能跨域访问。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、请求转发和重定向&quot;&gt;&lt;a href=&quot;#一、请求转发和重定向&quot; class=&quot;headerlink&quot; title=&quot;一、请求转发和重定向&quot;&gt;&lt;/a&gt;一、请求转发和重定向&lt;/h2&gt;&lt;h3 id=&quot;请求转发：&quot;&gt;&lt;a href=&quot;#请求转发：&quot; class=&quot;he
      
    
    </summary>
    
      <category term="JavaWeb" scheme="http://enfangzhong.github.io/categories/JavaWeb/"/>
    
    
      <category term="JaveEE" scheme="http://enfangzhong.github.io/tags/JaveEE/"/>
    
      <category term="java框架" scheme="http://enfangzhong.github.io/tags/java%E6%A1%86%E6%9E%B6/"/>
    
      <category term="java编程" scheme="http://enfangzhong.github.io/tags/java%E7%BC%96%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Redis学习笔记</title>
    <link href="http://enfangzhong.github.io/2018/09/05/Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://enfangzhong.github.io/2018/09/05/Redis学习笔记/</id>
    <published>2018-09-05T02:32:35.000Z</published>
    <updated>2019-05-10T13:39:05.599Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis简介"><a href="#Redis简介" class="headerlink" title="Redis简介"></a>Redis简介</h1><h3 id="关于关系型数据库和nosql数据库"><a href="#关于关系型数据库和nosql数据库" class="headerlink" title="关于关系型数据库和nosql数据库"></a>关于关系型数据库和nosql数据库</h3><blockquote><p>  关系型数据库是基于关系表的数据库，最终会将数据持久化到磁盘上，而nosql数据<br>  库是基于特殊的结构，并将数据存储到内存的数据库。从性能上而言，nosql数据库<br>  要优于关系型数据库，从安全性上而言关系型数据库要优于nosql数据库，所以在实<br>  际开发中一个项目中nosql和关系型数据库会一起使用，达到性能和安全性的双保证。</p></blockquote><h3 id="为什么要使用Redis"><a href="#为什么要使用Redis" class="headerlink" title="为什么要使用Redis"></a>为什么要使用Redis</h3><h3 id="redis在Linux上的安装"><a href="#redis在Linux上的安装" class="headerlink" title="redis在Linux上的安装"></a>redis在Linux上的安装</h3><ol><li><p>安装redis编译的c环境，yum install gcc-c++</p></li><li><p>将redis-2.6.16.tar.gz上传到Linux系统中</p></li><li><p>解压到/usr/local下 tar -xvf redis-2.6.16.tar.gz -C /usr/local</p></li><li><p>进入redis-2.6.16目录 使用make命令编译redis</p></li><li><p>在redis-2.6.16目录中 使用make PREFIX=/usr/local/redis install命令安装<br>redis到/usr/local/redis中</p></li><li><p>拷贝redis-2.6.16中的redis.conf到安装目录redis中</p></li><li><p>启动redis 在bin下执行命令redis-server redis.conf</p></li><li><p>如需远程连接redis，需配置redis端口6379在linux防火墙中开发</p></li></ol><blockquote><p>  /sbin/iptables -I INPUT -p tcp —dport 6379 -j ACCEPT</p><p>  /etc/rc.d/init.d/iptables save</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/37168456521de2f12b6616af58929fc4.png" alt=""></p><p>启动后看到如上欢迎页面，但此窗口不能关闭，窗口关闭就认为redis也关闭了(类<br>似Tomcat通过bin下的startup.bat的方式)</p><p>解决方案：可以通过修改配置文件 配置redis后台启动，即服务器启动了但不会<br>穿件控制台窗口</p><p>将redis.conf文件中的daemonize从false修改成true表示后台启动</p><p>使用命令查看6379端口是否启动ps -ef | grep redis</p><p><img src="/2018/09/05/Redis学习笔记/c7bdc26e311ca4aad1c8f0dbbde470cf.png" alt=""></p><h1 id="使用java去操作Redis"><a href="#使用java去操作Redis" class="headerlink" title="使用java去操作Redis"></a>使用java去操作Redis</h1><h1 id="Redis的常用命令"><a href="#Redis的常用命令" class="headerlink" title="Redis的常用命令"></a>Redis的常用命令</h1><blockquote><p>  redis是一种高级的key-value的存储系统</p><p>  其中的key是字符串类型，尽可能满足如下几点：</p></blockquote><ol><li><p>key不要太长，最好不要操作1024个字节，这不仅会消耗内存还会降低查找 效率</p></li><li><p>key不要太短，如果太短会降低key的可读性</p></li><li><p>在项目中，key最好有一个统一的命名规范（根据企业的需求）</p></li></ol><blockquote><p>  其中value 支持五种数据类型：</p></blockquote><ol><li><p>字符串型 string</p></li><li><p>字符串列表 lists</p></li><li><p>字符串集合 sets</p></li><li><p>有序字符串集合 sorted sets</p></li><li><p>哈希类型 hashs</p></li></ol><blockquote><p>  我们对Redis的学习，主要是对数据的存储，下面将来学习各种Redis的数据类型的<br>  存储操作：</p></blockquote><h3 id="存储字符串string"><a href="#存储字符串string" class="headerlink" title="存储字符串string"></a>存储字符串string</h3><blockquote><p>  字符串类型是Redis中最为基础的数据存储类型，它在Redis中是二进制安全的，这<br>  便意味着该类型可以接受任何格式的数据，如JPEG图像数据或Json对象描述信息等。<br>  在Redis中字符串类型的Value最多可以容纳的数据长度是512M</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/d719e7ea50fbb0ddd14a95366cb28382.png" alt=""></p><ol><li><p><strong>set key value</strong>：设定key持有指定的字符串value，如果该key存在则进行覆盖<br>操作。总是返回”OK”</p></li><li><p><strong>get key</strong>：获取key的value。如果与该key关联的value不是String类型，redis<br>将返回错误信息，因为get命令只能用于获取String value；如果该key不存在，返<br>回null。</p></li></ol><p><img src="/2018/09/05/Redis学习笔记/122f9d8b0eb285eb9b9adcce1830fb55.png" alt=""></p><ol><li><strong>getset key value</strong>：先获取该key的值，然后在设置该key的值。</li></ol><p><img src="/2018/09/05/Redis学习笔记/f235bea179383cbf7acba3331c2539bc.png" alt=""></p><blockquote><p>  4）<strong>incr key</strong>：将指定的key的value原子性的递增1.如果该key不存在，其初始值<br>  为0，在incr之后其值为1。如果value的值不能转成整型，如hello，该操作将执<br>  行失败并返回相应的错误信息。</p><p>  5）<strong>decr key</strong>：将指定的key的value原子性的递减1.如果该key不存在，其初始值<br>  为0，在incr之后其值为-1。如果value的值不能转成整型，如hello，该操作将执<br>  行失败并返回相应的错误信息。</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/550c428204f4d04aa54544d301edf00d.png" alt=""></p><blockquote><p>  6）<strong>incrby key increment</strong>：将指定的key的value原子性增加increment，如果该<br>  key不存在，器初始值为0，在incrby之后，该值为increment。如果该值不能转成<br>  整型，如hello则失败并返回错误信息</p><p>  7）<strong>decrby key decrement</strong>：将指定的key的value原子性减少decrement，如果<br>  该key不存在，器初始值为0，在decrby之后，该值为decrement。如果该值不能<br>  转成整型，如hello则失败并返回错误信息</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/09a5661b72a8d30fe1aaa56947a0ba24.png" alt=""></p><p><img src="/2018/09/05/Redis学习笔记/ad6001d7f725828dd13223f512d98ea4.png" alt=""></p><blockquote><p>  8）<strong>append key value</strong>：如果该key存在，则在原有的value后追加该值；如果该<br>  key 不存在，则重新创建一个key/value</p></blockquote><h3 id="存储lists类型"><a href="#存储lists类型" class="headerlink" title="存储lists类型"></a>存储lists类型</h3><blockquote><p>  在Redis中，List类型是按照插入顺序排序的字符串链表。和数据结构中的普通链表<br>  一样，我们可以在其头部(left)和尾部(right)添加新的元素。在插入时，如果该键并不<br>  存在，Redis将为该键创建一个新的链表。与此相反，如果链表中所有的元素均被移<br>  除，那么该键也将会被从数据库中删除。List中可以包含的最大元素数量是<br>  4294967295。</p></blockquote><p>从元素插入和删除的效率视角来看，如果我们是在链表的两头插入或删除元素，这将<br>会是非常高效的操作，即使链表中已经存储了百万条记录，该操作也可以在常量时间<br>内完成。然而需要说明的是，如果元素插入或删除操作是作用于链表中间，那将会是<br>非常低效的。相信对于有良好数据结构基础的开发者而言，这一点并不难理解。</p><p><img src="/2018/09/05/Redis学习笔记/a95668510b8f0b7d098dc493b9a06fd0.png" alt=""></p><ol><li><p><strong>lpush key value1 value2…</strong>：在指定的key所关联的list的头部插入所有的<br>values，如果该key不存在，该命令在插入的之前创建一个与该key关联的空链<br>表，之后再向该链表的头部插入数据。插入成功，返回元素的个数。</p></li><li><p><strong>rpush key value1、value2…</strong>：在该list的尾部添加元素</p></li><li><p><strong>lrange key start end</strong>：获取链表中从start到end的元素的值，start、end可<br>为负数，若为-1则表示链表尾部的元素，-2则表示倒数第二个，依次类推…</p></li></ol><p><img src="/2018/09/05/Redis学习笔记/c817bb72ebb9a912fc98728fd2555703.png" alt=""></p><ol><li><strong>lpushx key value</strong>：仅当参数中指定的key存在时（如果与key管理的list中没<br>有值时，则该key是不存在的）在指定的key所关联的list的头部插入value。</li></ol><blockquote><p>  5）<strong>rpushx key value</strong>：在该list的尾部添加元素</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/f4168cd95ab247bffb96c7f4e53ca49c.png" alt=""></p><blockquote><p>  6）<strong>lpop key</strong>：返回并弹出指定的key关联的链表中的第一个元素，即头部元素。</p><p>  7）<strong>rpop key</strong>：从尾部弹出元素。</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/55bf55e5817df95473e372562a0c6f6d.png" alt=""></p><p><img src="/2018/09/05/Redis学习笔记/8ed2b9e5d38588b65001722cba9e596f.png" alt=""></p><blockquote><p>  8）<strong>rpoplpush resource destination</strong>：将链表中的尾部元素弹出并添加到头部</p><p>  9）<strong>llen key</strong>：返回指定的key关联的链表中的元素的数量。</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/d828c877fc1d70d29c2e2874af7d3fb7.png" alt=""></p><blockquote><p>  10）<strong>lset key index<br>  value</strong>：设置链表中的index的脚标的元素值，0代表链表的头元<br>  素，-1代表链表的尾元素。</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/e12c33b42ab106d631e248ee90c0bb05.png" alt=""></p><blockquote><p>  11）<strong>lrem key count<br>  value</strong>：删除count个值为value的元素，如果count大于0，从头向尾遍历并删除count个值为value的元素，如果count小于0，则从尾向头遍历并删除。如果count等于0，则删除链表中所有等于value的元素。</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/3bdb15c54c9a766fb7a9607efea446b3.png" alt=""></p><blockquote><p>  12）<strong>linsert key before|after pivot<br>  value</strong>：在pivot元素前或者后插入value这个 元素。</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/d3fbb6c9ed79291ca94214f2a67d00ff.png" alt=""></p><h3 id="存储sets类型"><a href="#存储sets类型" class="headerlink" title="存储sets类型"></a>存储sets类型</h3><blockquote><p>  在Redis中，我们可以将Set类型看作为没有排序的字符集合，和List类型一样，我<br>  们也可以在该类型的数据值上执行添加、删除或判断某一元素是否存在等操作。需要<br>  说明的是，这些操作的时间是常量时间。Set可包含的最大元素数是4294967295。</p><p>  和List类型不同的是，Set集合中不允许出现重复的元素。和List类型相比，Set类<br>  型在功能上还存在着一个非常重要的特性，即在服务器端完成多个Sets之间的聚合计<br>  算操作，如unions、intersections和differences。由于这些操作均在服务端完成，<br>  因此效率极高，而且也节省了大量的网络IO开销</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/a95668510b8f0b7d098dc493b9a06fd0.png" alt=""></p><blockquote><p>  1）sadd key value1、value2…：向set中添加数据，如果该key的值已有则不会<br>  重复添加</p><p>  2）smembers key：获取set中所有的成员</p><p>  3）scard key：获取set中成员的数量</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/a41c3bb595584719ad1f9a1f02ff08cd.png" alt=""></p><blockquote><p>  4）sismember key member：判断参数中指定的成员是否在该set中，1表示存<br>  在，0表示不存在或者该key本身就不存在</p><p>  5）srem key member1、member2…：删除set中指定的成员</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/1701297c2cd81dc7d0bda3721f10a301.png" alt=""></p><p>6）srandmember key：随机返回set中的一个成员</p><p><img src="/2018/09/05/Redis学习笔记/0b16524ef5e1db1fa4dba8226512454b.png" alt=""></p><blockquote><p>  7）sdiff sdiff key1 key2：返回key1与key2中相差的成员，而且与key的顺序有<br>  关。即返回差集。</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/ffa48e0a498de079ab56848dda375900.png" alt=""></p><blockquote><p>  8）sdiffstore destination key1 key2：将key1、key2相差的成员存储在<br>  destination上</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/b600dad7d4568d70d774422aef6d32d1.png" alt=""></p><blockquote><p>  9）sinter key[key1,key2…]：返回交集。</p><p>  10）sinterstore destination key1 key2：将返回的交集存储在destination上</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/9a48658e5c6f7fe08177669386a8e7f1.png" alt=""></p><blockquote><p>  11）sunion key1、key2：返回并集。</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/8847e09f463cee91443b06034ad63748.png" alt=""></p><blockquote><p>  12）sunionstore destination key1 key2：将返回的并集存储在destination上</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/994d5312aeb7e2fab285748c1ed4dab6.png" alt=""></p><h3 id="存储sortedset"><a href="#存储sortedset" class="headerlink" title="存储sortedset"></a>存储sortedset</h3><blockquote><p>  Sorted-Sets和Sets类型极为相似，它们都是字符串的集合，都不允许重复的成员出<br>  现在一个Set中。它们之间的主要差别是Sorted-Sets中的每一个成员都会有一个分<br>  数(score)与之关联，Redis正是通过分数来为集合中的成员进行从小到大的排序。然<br>  而需要额外指出的是，尽管Sorted-Sets中的成员必须是唯一的，但是分数(score)<br>  却是可以重复的。</p><p>  在Sorted-Set中添加、删除或更新一个成员都是非常快速的操作，其时间复杂度为<br>  集合中成员数量的对数。由于Sorted-Sets中的成员在集合中的位置是有序的，因此，<br>  即便是访问位于集合中部的成员也仍然是非常高效的。事实上，Redis所具有的这一<br>  特征在很多其它类型的数据库中是很难实现的，换句话说，在该点上要想达到和Redis<br>  同样的高效，在其它数据库中进行建模是非常困难的。</p><p>  例如：游戏排名、微博热点话题等使用场景。</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/a95668510b8f0b7d098dc493b9a06fd0.png" alt=""></p><blockquote><p>  1）<strong>zadd key score member score2 member2 …</strong> ：将所有成员以及该成员的<br>  分数存放到sorted-set中</p><p>  2）<strong>zcard key</strong>：获取集合中的成员数量</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/a9531d6a0b55abcb785b42e3e226a429.png" alt=""></p><blockquote><p>  3）<strong>zcount key min max</strong>：获取分数在[min,max]之间的成员</p><p>  zincrby key increment member：设置指定成员的增加的分数。</p><p>  zrange key start end<br>  [withscores]：获取集合中脚标为start-end的成员，[withscores]参数表明返回的成员包含其分数。</p><p>  zrangebyscore key min max [withscores] [limit offset<br>  count]：返回分数在[min,max]的成员并按照分数从低到高排序。[withscores]：显示分数；[limit<br>  offset count]：offset，表明从脚标为offset的元素开始并返回count个成员。</p><p>  zrank key member：返回成员在集合中的位置。</p><p>  zrem key member[member…]：移除集合中指定的成员，可以指定多个成员。</p><p>  zscore key member：返回指定成员的分数</p></blockquote><h3 id="存储hash"><a href="#存储hash" class="headerlink" title="存储hash"></a>存储hash</h3><blockquote><p>  Redis中的Hashes类型可以看成具有String Key和String Value的map容器。所<br>  以该类型非常适合于存储值对象的信息。如Username、Password和Age等。如果<br>  Hash中包含很少的字段，那么该类型的数据也将仅占用很少的磁盘空间。每一个Hash<br>  可以存储4294967295个键值对。</p></blockquote><p><img src="/2018/09/05/Redis学习笔记/e8320268e156af3a42887d284af00fb1.png" alt=""></p><p></p><p>1）<strong>hset key field value</strong>：为指定的key设定field/value对（键值对）。</p><p>2）<strong>hgetall key：</strong>获取key中的所有filed-vaule</p><p><img src="/2018/09/05/Redis学习笔记/be52e27b0b7cb4e15cf9e125a268316f.png" alt=""></p><p>3）<strong>hget key field</strong>：返回指定的key中的field的值</p><p><img src="/2018/09/05/Redis学习笔记/347c176b3e916155debc4cd8494d9e42.png" alt=""></p><p>4）<strong>hmset key fields</strong>：设置key中的多个filed/value</p><p>5）<strong>hmget key fileds</strong>：获取key中的多个filed的值</p><p>6）<strong>hexists key field</strong>：判断指定的key中的filed是否存在</p><p>7）<strong>hlen key</strong>：获取key所包含的field的数量</p><p>8）<strong>hincrby key field increment</strong>：设置key中filed的值增加increment，如：age<br>增加20</p><h1 id="Redis的通用操作-见文档"><a href="#Redis的通用操作-见文档" class="headerlink" title="Redis的通用操作(见文档)"></a>Redis的通用操作(见文档)</h1><h1 id="Redis的特性-见文档"><a href="#Redis的特性-见文档" class="headerlink" title="Redis的特性(见文档)"></a>Redis的特性(见文档)</h1><h1 id="Redis的事务-见文档"><a href="#Redis的事务-见文档" class="headerlink" title="Redis的事务(见文档)"></a>Redis的事务(见文档)</h1><h1 id="Redis的持久化-见文档"><a href="#Redis的持久化-见文档" class="headerlink" title="Redis的持久化(见文档)"></a>Redis的持久化(见文档)</h1><p>总结：</p><ol><li><p>nosql</p></li><li><p>redis安装——linux（重点）</p></li><li><p>jedis（重点）</p></li><li><p>redis的数据操作类型 5中 (了解) —- string和hash</p></li><li><p>redis的其他</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis简介&quot;&gt;&lt;a href=&quot;#Redis简介&quot; class=&quot;headerlink&quot; title=&quot;Redis简介&quot;&gt;&lt;/a&gt;Redis简介&lt;/h1&gt;&lt;h3 id=&quot;关于关系型数据库和nosql数据库&quot;&gt;&lt;a href=&quot;#关于关系型数据库和nosql数据
      
    
    </summary>
    
      <category term="JavaWeb" scheme="http://enfangzhong.github.io/categories/JavaWeb/"/>
    
    
      <category term="Redis" scheme="http://enfangzhong.github.io/tags/Redis/"/>
    
      <category term="非关系型数据库" scheme="http://enfangzhong.github.io/tags/%E9%9D%9E%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="数据库" scheme="http://enfangzhong.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Java开发中的23种设计模式详解</title>
    <link href="http://enfangzhong.github.io/2018/09/03/Java%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%8423%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/"/>
    <id>http://enfangzhong.github.io/2018/09/03/Java开发中的23种设计模式详解/</id>
    <published>2018-09-03T04:58:00.000Z</published>
    <updated>2019-05-10T12:20:46.341Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Java开发中的23种设计模式详解"><a href="#Java开发中的23种设计模式详解" class="headerlink" title="Java开发中的23种设计模式详解"></a>Java开发中的23种设计模式详解</h1><h2 id="java的设计模式大体上分为三大类："><a href="#java的设计模式大体上分为三大类：" class="headerlink" title="java的设计模式大体上分为三大类："></a>java的设计模式大体上分为三大类：</h2><ul><li>创建型模式（5种）：工厂方法模式，抽象工厂模式，单例模式，建造者模式，原型模式。</li><li>结构型模式（7种）：适配器模式，装饰器模式，代理模式，外观模式，桥接模式，组合模式，享元模式。</li><li>行为型模式（11种）：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。</li></ul><h2 id="设计模式遵循的原则有6个："><a href="#设计模式遵循的原则有6个：" class="headerlink" title="设计模式遵循的原则有6个："></a>设计模式遵循的原则有6个：</h2><p><strong>1、开闭原则（Open Close Principle）</strong></p><p>　　<strong>对扩展开放，对修改关闭</strong>。</p><p><strong>2、里氏代换原则（Liskov Substitution Principle）</strong></p><p>　　只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。</p><p><strong>3、依赖倒转原则（Dependence Inversion Principle）</strong></p><p>　　这个是开闭原则的基础，<strong>对接口编程</strong>，依赖于抽象而不依赖于具体。</p><p><strong>4、接口隔离原则（Interface Segregation Principle）</strong></p><p>　　使用多个隔离的借口来降低耦合度。</p><p><strong>5、迪米特法则（最少知道原则）（Demeter Principle）</strong></p><p>　　一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。</p><p><strong>6、合成复用原则（Composite Reuse Principle）</strong></p><p>　　原则是尽量使用合成/聚合的方式，而不是使用继承。继承实际上破坏了类的封装性，超类的方法可能会被子类修改。</p><h2 id="1-工厂模式（Factory-Method）"><a href="#1-工厂模式（Factory-Method）" class="headerlink" title="1. 工厂模式（Factory Method）"></a>1. 工厂模式（Factory Method）</h2><p>　　常用的工厂模式是静态工厂，利用static方法，作为一种类似于常见的工具类Utils等辅助效果，一般情况下工厂类不需要实例化。</p><p>　　</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">interface food&#123;&#125;</span><br><span class="line"></span><br><span class="line">class A implements food&#123;&#125;</span><br><span class="line">class B implements food&#123;&#125;</span><br><span class="line">class C implements food&#123;&#125;</span><br><span class="line"></span><br><span class="line">public class StaticFactory &#123;</span><br><span class="line"></span><br><span class="line">    private StaticFactory()&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    public static food getA()&#123;  return new A(); &#125;</span><br><span class="line">    public static food getB()&#123;  return new B(); &#125;</span><br><span class="line">    public static food getC()&#123;  return new C(); &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Client&#123;</span><br><span class="line">    //客户端代码只需要将相应的参数传入即可得到对象</span><br><span class="line">    //用户不需要了解工厂类内部的逻辑。</span><br><span class="line">    public void get(String name)&#123;</span><br><span class="line">        food x = null ;</span><br><span class="line">        if ( name.equals(&quot;A&quot;)) &#123;</span><br><span class="line">            x = StaticFactory.getA();</span><br><span class="line">        &#125;else if ( name.equals(&quot;B&quot;))&#123;</span><br><span class="line">            x = StaticFactory.getB();</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            x = StaticFactory.getC();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-抽象工厂模式（Abstract-Factory）"><a href="#2-抽象工厂模式（Abstract-Factory）" class="headerlink" title="2. 抽象工厂模式（Abstract Factory）"></a>2. 抽象工厂模式（Abstract Factory）</h2><p>　　一个基础接口定义了功能，每个实现接口的子类就是产品，然后定义一个工厂接口，实现了工厂接口的就是工厂，这时候，接口编程的优点就出现了，我们可以新增产品类（只需要实现产品接口），只需要同时新增一个工厂类，客户端就可以轻松调用新产品的代码。</p><p>　　抽象工厂的灵活性就体现在这里，无需改动原有的代码，毕竟对于客户端来说，静态工厂模式在不改动StaticFactory类的代码时无法新增产品，如果采用了抽象工厂模式，就可以轻松的新增拓展类。</p><p>　　实例代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">interface food&#123;&#125;</span><br><span class="line"></span><br><span class="line">class A implements food&#123;&#125;</span><br><span class="line">class B implements food&#123;&#125;</span><br><span class="line"></span><br><span class="line">interface produce&#123; food get();&#125;</span><br><span class="line"></span><br><span class="line">class FactoryForA implements produce&#123;</span><br><span class="line">    @Override</span><br><span class="line">    public food get() &#123;</span><br><span class="line">        return new A();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">class FactoryForB implements produce&#123;</span><br><span class="line">    @Override</span><br><span class="line">    public food get() &#123;</span><br><span class="line">        return new B();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">public class AbstractFactory &#123;</span><br><span class="line">    public void ClientCode(String name)&#123;</span><br><span class="line">        food x= new FactoryForA().get();</span><br><span class="line">        x = new FactoryForB().get();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-单例模式（Singleton）"><a href="#3-单例模式（Singleton）" class="headerlink" title="3. 单例模式（Singleton）"></a>3. 单例模式（Singleton）</h2><p> 　　在内部创建一个实例，构造器全部设置为private，所有方法均在该实例上改动，在创建上要注意类的实例化只能执行一次，可以采用许多种方法来实现，如Synchronized关键字，或者利用内部类等机制来实现。</p><p>　　</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class Singleton &#123;</span><br><span class="line">    private Singleton()&#123;&#125;</span><br><span class="line"></span><br><span class="line">    private static class SingletonBuild&#123;</span><br><span class="line">        private static Singleton value = new Singleton();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public Singleton getInstance()&#123;  return  SingletonBuild.value ;&#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4-建造者模式（Builder）"><a href="#4-建造者模式（Builder）" class="headerlink" title="4.建造者模式（Builder）"></a>4.建造者模式（Builder）</h2><p>　　在了解之前，先假设有一个问题，我们需要创建一个学生对象，属性有name,number,class,sex,age,school等属性，如果每一个属性都可以为空，也就是说我们可以只用一个name,也可以用一个school,name,或者一个class,number，或者其他任意的赋值来创建一个学生对象，这时该怎么构造？</p><p>　　难道我们写6个1个输入的构造函数，15个2个输入的构造函数…….吗？这个时候就需要用到Builder模式了。给个例子，大家肯定一看就懂：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">public class Builder &#123;</span><br><span class="line"></span><br><span class="line">    static class Student&#123;</span><br><span class="line">        String name = null ;</span><br><span class="line">        int number = -1 ;</span><br><span class="line">        String sex = null ;</span><br><span class="line">        int age = -1 ;</span><br><span class="line">        String school = null ;</span><br><span class="line"></span><br><span class="line">　　　　　//构建器，利用构建器作为参数来构建Student对象</span><br><span class="line">        static class StudentBuilder&#123;</span><br><span class="line">            String name = null ;</span><br><span class="line">            int number = -1 ;</span><br><span class="line">            String sex = null ;</span><br><span class="line">            int age = -1 ;</span><br><span class="line">            String school = null ;</span><br><span class="line">            public StudentBuilder setName(String name) &#123;</span><br><span class="line">                this.name = name;</span><br><span class="line">                return  this ;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            public StudentBuilder setNumber(int number) &#123;</span><br><span class="line">                this.number = number;</span><br><span class="line">                return  this ;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            public StudentBuilder setSex(String sex) &#123;</span><br><span class="line">                this.sex = sex;</span><br><span class="line">                return  this ;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            public StudentBuilder setAge(int age) &#123;</span><br><span class="line">                this.age = age;</span><br><span class="line">                return  this ;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            public StudentBuilder setSchool(String school) &#123;</span><br><span class="line">                this.school = school;</span><br><span class="line">                return  this ;</span><br><span class="line">            &#125;</span><br><span class="line">            public Student build() &#123;</span><br><span class="line">                return new Student(this);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        public Student(StudentBuilder builder)&#123;</span><br><span class="line">            this.age = builder.age;</span><br><span class="line">            this.name = builder.name;</span><br><span class="line">            this.number = builder.number;</span><br><span class="line">            this.school = builder.school ;</span><br><span class="line">            this.sex = builder.sex ;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main( String[] args )&#123;</span><br><span class="line">        Student a = new Student.StudentBuilder().setAge(13).setName(&quot;LiHua&quot;).build();</span><br><span class="line">        Student b = new Student.StudentBuilder().setSchool(&quot;sc&quot;).setSex(&quot;Male&quot;).setName(&quot;ZhangSan&quot;).build();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-原型模式（Protype）"><a href="#5-原型模式（Protype）" class="headerlink" title="5. 原型模式（Protype）"></a>5. 原型模式（Protype）</h2><p>原型模式就是讲一个对象作为原型，使用clone()方法来创建新的实例。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public class Prototype implements Cloneable&#123;</span><br><span class="line"></span><br><span class="line">    private String name;</span><br><span class="line"></span><br><span class="line">    public String getName() &#123;</span><br><span class="line">        return name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setName(String name) &#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected Object clone()   &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            return super.clone();</span><br><span class="line">        &#125; catch (CloneNotSupportedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;finally &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main ( String[] args)&#123;</span><br><span class="line">        Prototype pro = new Prototype();</span><br><span class="line">        Prototype pro1 = (Prototype)pro.clone();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此处使用的是浅拷贝，关于深浅拷贝，大家可以另行查找相关资料。</p><h2 id="6-适配器模式（Adapter）"><a href="#6-适配器模式（Adapter）" class="headerlink" title="6.适配器模式（Adapter）"></a>6.适配器模式（Adapter）</h2><p>适配器模式的作用就是在原来的类上提供新功能。主要可分为3种：</p><p>类适配：创建新类，继承源类，并实现新接口，例如<br>class  adapter extends oldClass  implements newFunc{}<br>对象适配：创建新类持源类的实例，并实现新接口，例如<br>class adapter implements newFunc { private oldClass oldInstance ;}<br>接口适配：创建新的抽象类实现旧接口方法。例如<br>abstract class adapter implements oldClassFunc { void newFunc();}<br>7.装饰模式（Decorator）<br> 给一类对象增加新的功能，装饰方法与具体的内部逻辑无关。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">interface Source&#123; void method();&#125;</span><br><span class="line">public class Decorator implements Source&#123;</span><br><span class="line"></span><br><span class="line">    private Source source ;</span><br><span class="line">    public void decotate1()&#123;</span><br><span class="line">        System.out.println(&quot;decorate&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void method() &#123;</span><br><span class="line">        decotate1();</span><br><span class="line">        source.method();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="8-代理模式（Proxy）"><a href="#8-代理模式（Proxy）" class="headerlink" title="8.代理模式（Proxy）"></a>8.代理模式（Proxy）</h2><p>客户端通过代理类访问，代理类实现具体的实现细节，客户只需要使用代理类即可实现操作。</p><p>这种模式可以对旧功能进行代理，用一个代理类调用原有的方法，且对产生的结果进行控制。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">interface Source&#123; void method();&#125;</span><br><span class="line"></span><br><span class="line">class OldClass implements Source&#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void method() &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Proxy implements Source&#123;</span><br><span class="line">    private Source source = new OldClass();</span><br><span class="line"></span><br><span class="line">    void doSomething()&#123;&#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void method() &#123;</span><br><span class="line">        new Class1().Func1();</span><br><span class="line">        source.method();</span><br><span class="line">        new Class2().Func2();</span><br><span class="line">        doSomething();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="9-外观模式（Facade）"><a href="#9-外观模式（Facade）" class="headerlink" title="9.外观模式（Facade）"></a>9.外观模式（Facade）</h2><p>为子系统中的一组接口提供一个一致的界面，定义一个高层接口，这个接口使得这一子系统更加容易使用。这句话是百度百科的解释，有点难懂，但是没事，看下面的例子，我们在启动停止所有子系统的时候，为它们设计一个外观类，这样就可以实现统一的接口，这样即使有新增的子系统subSystem4,也可以在不修改客户端代码的情况下轻松完成。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public class Facade &#123;</span><br><span class="line">    private subSystem1 subSystem1 = new subSystem1();</span><br><span class="line">    private subSystem2 subSystem2 = new subSystem2();</span><br><span class="line">    private subSystem3 subSystem3 = new subSystem3();</span><br><span class="line">    </span><br><span class="line">    public void startSystem()&#123;</span><br><span class="line">        subSystem1.start();</span><br><span class="line">        subSystem2.start();</span><br><span class="line">        subSystem3.start();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public void stopSystem()&#123;</span><br><span class="line">        subSystem1.stop();</span><br><span class="line">        subSystem2.stop();</span><br><span class="line">        subSystem3.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="10-桥接模式（Bridge）"><a href="#10-桥接模式（Bridge）" class="headerlink" title="10.桥接模式（Bridge）"></a>10.桥接模式（Bridge）</h2><p>这里引用下<a href="http://www.runoob.com/design-pattern/bridge-pattern.html的例子。Circle类将DrwaApi与Shape类进行了桥接，代码：" target="_blank" rel="noopener">http://www.runoob.com/design-pattern/bridge-pattern.html的例子。Circle类将DrwaApi与Shape类进行了桥接，代码：</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">interface DrawAPI &#123;</span><br><span class="line">    public void drawCircle(int radius, int x, int y);</span><br><span class="line">&#125;</span><br><span class="line">class RedCircle implements DrawAPI &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void drawCircle(int radius, int x, int y) &#123;</span><br><span class="line">        System.out.println(&quot;Drawing Circle[ color: red, radius: &quot;</span><br><span class="line">                + radius +&quot;, x: &quot; +x+&quot;, &quot;+ y +&quot;]&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">class GreenCircle implements DrawAPI &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void drawCircle(int radius, int x, int y) &#123;</span><br><span class="line">        System.out.println(&quot;Drawing Circle[ color: green, radius: &quot;</span><br><span class="line">                + radius +&quot;, x: &quot; +x+&quot;, &quot;+ y +&quot;]&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">abstract class Shape &#123;</span><br><span class="line">    protected DrawAPI drawAPI;</span><br><span class="line">    protected Shape(DrawAPI drawAPI)&#123;</span><br><span class="line">        this.drawAPI = drawAPI;</span><br><span class="line">    &#125;</span><br><span class="line">    public abstract void draw();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Circle extends Shape &#123;</span><br><span class="line">    private int x, y, radius;</span><br><span class="line"></span><br><span class="line">    public Circle(int x, int y, int radius, DrawAPI drawAPI) &#123;</span><br><span class="line">        super(drawAPI);</span><br><span class="line">        this.x = x;</span><br><span class="line">        this.y = y;</span><br><span class="line">        this.radius = radius;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void draw() &#123;</span><br><span class="line">        drawAPI.drawCircle(radius,x,y);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//客户端使用代码</span><br><span class="line">Shape redCircle = new Circle(100,100, 10, new RedCircle());</span><br><span class="line">Shape greenCircle = new Circle(100,100, 10, new GreenCircle());</span><br><span class="line">redCircle.draw();</span><br><span class="line">greenCircle.draw();</span><br></pre></td></tr></table></figure><h2 id="11-组合模式（Composite）"><a href="#11-组合模式（Composite）" class="headerlink" title="11.组合模式（Composite）"></a>11.组合模式（Composite）</h2><p> 组合模式是为了表示那些层次结构，同时部分和整体也可能是一样的结构，常见的如文件夹或者树。举例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">abstract class component&#123;&#125;</span><br><span class="line"></span><br><span class="line">class File extends  component&#123; String filename;&#125;</span><br><span class="line"></span><br><span class="line">class Folder extends  component&#123;</span><br><span class="line">    component[] files ;  //既可以放文件File类，也可以放文件夹Folder类。Folder类下又有子文件或子文件夹。</span><br><span class="line">    String foldername ;</span><br><span class="line">    public Folder(component[] source)&#123; files = source ;&#125;</span><br><span class="line">    </span><br><span class="line">    public void scan()&#123;</span><br><span class="line">        for ( component f:files)&#123;</span><br><span class="line">            if ( f instanceof File)&#123;</span><br><span class="line">                System.out.println(&quot;File &quot;+((File) f).filename);</span><br><span class="line">            &#125;else if(f instanceof Folder)&#123;</span><br><span class="line">                Folder e = (Folder)f ;</span><br><span class="line">                System.out.println(&quot;Folder &quot;+e.foldername);</span><br><span class="line">                e.scan();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="12-享元模式（Flyweight）"><a href="#12-享元模式（Flyweight）" class="headerlink" title="12.享元模式（Flyweight）"></a>12.享元模式（Flyweight）</h2><p>使用共享对象的方法，用来尽可能减少内存使用量以及分享资讯。通常使用工厂类辅助，例子中使用一个HashMap类进行辅助判断，数据池中是否已经有了目标实例，如果有，则直接返回，不需要多次创建重复实例。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">abstract class flywei&#123; &#125;</span><br><span class="line"></span><br><span class="line">public class Flyweight extends flywei&#123;</span><br><span class="line">    Object obj ;</span><br><span class="line">    public Flyweight(Object obj)&#123;</span><br><span class="line">        this.obj = obj;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class  FlyweightFactory&#123;</span><br><span class="line">    private HashMap&lt;Object,Flyweight&gt; data;</span><br><span class="line"></span><br><span class="line">    public FlyweightFactory()&#123; data = new HashMap&lt;&gt;();&#125;</span><br><span class="line"></span><br><span class="line">    public Flyweight getFlyweight(Object object)&#123;</span><br><span class="line">        if ( data.containsKey(object))&#123;</span><br><span class="line">            return data.get(object);</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            Flyweight flyweight = new Flyweight(object);</span><br><span class="line">            data.put(object,flyweight);</span><br><span class="line">            return flyweight;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Java开发中的23种设计模式详解&quot;&gt;&lt;a href=&quot;#Java开发中的23种设计模式详解&quot; class=&quot;headerlink&quot; title=&quot;Java开发中的23种设计模式详解&quot;&gt;&lt;/a&gt;Java开发中的23种设计模式详解&lt;/h1&gt;&lt;h2 id=&quot;java的
      
    
    </summary>
    
      <category term="JavaWeb" scheme="http://enfangzhong.github.io/categories/JavaWeb/"/>
    
    
      <category term="java框架" scheme="http://enfangzhong.github.io/tags/java%E6%A1%86%E6%9E%B6/"/>
    
      <category term="java编程" scheme="http://enfangzhong.github.io/tags/java%E7%BC%96%E7%A8%8B/"/>
    
      <category term="设计模式" scheme="http://enfangzhong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>将hexo博客同时部署发布托管到github和coding</title>
    <link href="http://enfangzhong.github.io/2018/08/30/Hexo%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2_%E5%90%8C%E6%97%B6%E9%83%A8%E7%BD%B2%E5%8F%91%E5%B8%83%E6%89%98%E7%AE%A1%E5%88%B0github%E5%92%8Ccoding/"/>
    <id>http://enfangzhong.github.io/2018/08/30/Hexo个人博客_同时部署发布托管到github和coding/</id>
    <published>2018-08-29T16:12:11.000Z</published>
    <updated>2019-05-10T12:20:01.736Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前我们把hexo托管在github，但是毕竟github是国外的，访问速度上还是有点慢，所以想也部署一套在国内的托管平台，之前查资料听说gitcafe,但是听说gitcafe已经被coding收购了，所以就决定部署到coding。 查询了多方资料，终于鼓捣出了本地一次部署，同时更新到github以及coding。<br><img src="https://i.loli.net/2018/08/30/5b86d9304789a.png" alt=""></p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>_config.yml配置<br>想要同时部署到2个平台，就要修改博客根目录下面的_config.yml文件中的deploy如下<br>根据Hexo官方文档需要修改成下面的形式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  message: [message]</span><br><span class="line">  repo:</span><br><span class="line">    github: &lt;repository url&gt;,[branch]</span><br><span class="line">    gitcafe: &lt;repository url&gt;,[branch]</span><br></pre></td></tr></table></figure></p><p>所以我的是这样：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo:</span><br><span class="line">      github: git@github.com:enfang/enfang.github.io.git,master</span><br><span class="line">      coding: git@git.coding.net:enfang/enfang.git,master</span><br></pre></td></tr></table></figure></p><p>我这边提交采用的SSH密钥，这个方法有个好处，提交的时候不用输入用户名和密码。如果你习惯用http的方式，只要将地址改成相应的http地址即可。</p><p>coding上创建一个新项目<br>这里只介绍coding上面如何创建项目，以及把本地hexo部署到coding上面，还不懂如何创建hexo的请看我之前的系类文章。首先我们创建一个项目，创建后进入项目的代码模块，获取到这个项目的ssh地址，我的是<a href="https://git.coding.net/enfang/enfang.git" target="_blank" rel="noopener">https://git.coding.net/enfang/enfang.git</a><br><img src="https://i.loli.net/2018/08/30/5b86d929e4b68.png" alt=""></p><p>同步本地hexo到coding上<br>把获取到了ssh配置在上面的_config.yml文件中的deploy下，如果是第一次使用coding的话，需要设置SSH公钥，生成的方法可以参考coding帮助中心<br>如果你看过我第一篇文章里面介绍过秘钥生成。<br>coding上的<a href="http://enfang.coding.me/2018/08/26/hexo+github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%8F%8A%E7%BE%8E%E5%8C%96%E6%9B%B4%E6%96%B0/#more" target="_blank" rel="noopener">第一篇文章</a><br>github上的<a href="https://enfangzhong.github.io/2018/08/26/hexo+github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%8F%8A%E7%BE%8E%E5%8C%96%E6%9B%B4%E6%96%B0/#more">第一篇文章</a></p><p>我这里直接使用之前部署github时已经生成的公钥。<br><img src="https://i.loli.net/2018/08/30/5b86d92990647.png" alt=""></p><p>本地打开 id_rsa.pub 文件，复制其中全部内容，填写到SSH_RSA公钥key下的一栏，公钥名称可以随意起名字。完成后点击“添加”，然后输入密码或动态码即可添加完成。<br><img src="https://i.loli.net/2018/08/30/5b86d92a0c55e.png" alt=""></p><p>添加后，测试公钥是否添加成功，在git bash命令输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@git.coding.net</span><br></pre></td></tr></table></figure></p><p>如果得到下面提示就表示公钥添加成功了：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Coding.net Tips : [Hello ! You&apos;ve conected to Coding.net by SSH successfully! ]</span><br></pre></td></tr></table></figure></p><p>最后使用部署命令就能把博客同步到coding上面：</p><p>hexo deploy -g<br><img src="https://i.loli.net/2018/08/30/5b86d92a3da71.png" alt=""></p><p>pages服务方式部署<br>部署博客方式有两种，第一种就是pages服务的方式，也推荐这种方式，因为可以绑定域名，而第二种演示的方式必须升级会员才能绑定自定义域名。pages方式也很简单<br>就是在source/需要创建一个空白文件，至于原因，是因为 coding.net需要这个文件来作为以静态文件部署的标志。就是说看到这个Staticfile就知道按照静态文件来发布。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd source/</span><br><span class="line">touch Staticfile  #名字必须是Staticfile</span><br></pre></td></tr></table></figure></p><p>分支选择master，因为前面配置的分支是master,因此开启之后，也需要是master。然后看起之后就可访问了。</p><p><strong>注意：</strong></p><p>如果你的项目名称跟你coding的用户名一样，比如我的用户是叫enfang,博客项目名也叫enfang<br>那直接访问 enfang.coding.me就能访问博客，否则就要带上项目名：enfang.coding.me/项目名 才能访问<br>推荐项目名跟用户名一样，这样就可以省略项目名了<br><img src="https://i.loli.net/2018/08/30/5b86d92bb1a82.png" alt=""></p><p>总结<br>到此为止，终于可以实现一次部署，github和coding两个网站同时更新。访问速度也是唰唰唰的快，忙乎了两天终于搭好了独立博客。希望对还在搭建hexo独立博客的小伙伴有帮助。<br>本人博客效果<br><img src="https://i.loli.net/2018/08/30/5b86d932b57bd.png" alt=""><br><a href="https://enfangzhong.github.io/">效果展示</a></p><p><strong>欢迎访问我的博客</strong><br><a href="https://enfangzhong.github.io/">Git托管博客效果</a> </p><p><a href="http://enfang.coding.me/" target="_blank" rel="noopener">Coding托管博客效果</a></p><p><a href="https://itjyg.gitee.io" target="_blank" rel="noopener">码云托管博客效果</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;之前我们把hexo托管在github，但是毕竟github是国外的，访问速度上还是有点慢，所以想也部署一套在国内的托管平台，之前查资料听说g
      
    
    </summary>
    
      <category term="搭建个人博客" scheme="http://enfangzhong.github.io/categories/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="个人博客" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="github博客" scheme="http://enfangzhong.github.io/tags/github%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="hexo博客" scheme="http://enfangzhong.github.io/tags/hexo%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="博客优化" scheme="http://enfangzhong.github.io/tags/%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客添加在线联系功能</title>
    <link href="http://enfangzhong.github.io/2018/08/29/Hexo%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2_%E6%B7%BB%E5%8A%A0%E5%9C%A8%E7%BA%BF%E8%81%94%E7%B3%BB%E5%8A%9F%E8%83%BD/"/>
    <id>http://enfangzhong.github.io/2018/08/29/Hexo个人博客_添加在线联系功能/</id>
    <published>2018-08-29T13:03:11.000Z</published>
    <updated>2019-05-10T12:19:28.127Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hexo博客添加在线联系功能"><a href="#Hexo博客添加在线联系功能" class="headerlink" title="Hexo博客添加在线联系功能"></a>Hexo博客添加在线联系功能</h1><p>Hexo博客如何添加在线联系功能呢,发现了一个不错的网站可以提供在线联系的服务，当有用户在网页上给你留言后会通过邮件或者微信通知你，可以及时的解答用户的疑问。</p><p>最终的效果可以参考我博客的右下角,有个聊天的按钮,效果如下所示:<br><img src="https://i.loli.net/2018/08/29/5b867bfc05324.png" alt=""><br>配置方法如下:<br>首先到DaoVoice上注册一个账号,注册完成后会得到一个app_id，获取appid的步骤如下图所示:<br><img src="https://i.loli.net/2018/08/29/5b867bfc134be.png" alt=""><br>以next主题为例,打开/themes/next/layout/_partials/head.swig文件添加如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if theme.daovoice %&#125;</span><br><span class="line">  &lt;script&gt;</span><br><span class="line">  (function(i,s,o,g,r,a,m)&#123;i[&quot;DaoVoiceObject&quot;]=r;i[r]=i[r]||function()&#123;(i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset=&quot;utf-8&quot;;m.parentNode.insertBefore(a,m)&#125;)(window,document,&quot;script&quot;,(&apos;https:&apos; == document.location.protocol ? &apos;https:&apos; : &apos;http:&apos;) + &quot;//widget.daovoice.io/widget/0f81ff2f.js&quot;,&quot;daovoice&quot;)</span><br><span class="line">  daovoice(&apos;init&apos;, &#123;</span><br><span class="line">      app_id: &quot;&#123;&#123;theme.daovoice_app_id&#125;&#125;&quot;</span><br><span class="line">    &#125;);</span><br><span class="line">  daovoice(&apos;update&apos;);</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure></p><p>接着打开主题配置文件_config.yml，添加如下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Online contact </span><br><span class="line">daovoice: true</span><br><span class="line">daovoice_app_id: 这里输入前面获取的app_id</span><br></pre></td></tr></table></figure></p><p>最后执行hexo clean &amp;&amp; hexo g &amp;&amp; hexo s就能看到效果了。</p><p>需要注意的是,next主题下聊天的按钮会和其他按钮重叠到一起，可以到聊天设置，修改下按钮的位置:<br><img src="https://i.loli.net/2018/08/29/5b867bfc0a0c8.png" alt=""></p><p>最后到右上角选择管理员，微信绑定,可以绑定你的微信号，关注公众号后打开小程序，就可以实时收发消息，有新的消息也会通过微信通知，设置页面如下:<br><img src="https://i.loli.net/2018/08/29/5b867bfc055c2.png" alt=""><br>效果展示: <a href="https://enfangzhong.github.io/">酱油哥博客</a></p><p><strong>欢迎访问我的博客</strong><br><a href="https://enfangzhong.github.io/">Git托管博客效果</a> </p><p><a href="http://enfang.coding.me/" target="_blank" rel="noopener">Coding托管博客效果</a></p><p><a href="https://itjyg.gitee.io" target="_blank" rel="noopener">码云托管博客效果</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hexo博客添加在线联系功能&quot;&gt;&lt;a href=&quot;#Hexo博客添加在线联系功能&quot; class=&quot;headerlink&quot; title=&quot;Hexo博客添加在线联系功能&quot;&gt;&lt;/a&gt;Hexo博客添加在线联系功能&lt;/h1&gt;&lt;p&gt;Hexo博客如何添加在线联系功能呢,发现了一
      
    
    </summary>
    
      <category term="搭建个人博客" scheme="http://enfangzhong.github.io/categories/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="个人博客" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="github博客" scheme="http://enfangzhong.github.io/tags/github%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="hexo博客" scheme="http://enfangzhong.github.io/tags/hexo%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="博客优化" scheme="http://enfangzhong.github.io/tags/%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>markdown的使用方法</title>
    <link href="http://enfangzhong.github.io/2018/08/27/Hexo%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2_markdown%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>http://enfangzhong.github.io/2018/08/27/Hexo个人博客_markdown的使用方法/</id>
    <published>2018-08-27T13:03:11.000Z</published>
    <updated>2019-05-10T12:22:03.283Z</updated>
    
    <content type="html"><![CDATA[<h1 id="markDown的使用方法"><a href="#markDown的使用方法" class="headerlink" title="markDown的使用方法"></a>markDown的使用方法</h1><h2 id="第一步：下载markdown"><a href="#第一步：下载markdown" class="headerlink" title="第一步：下载markdown"></a>第一步：下载markdown</h2><h3 id="进入markdown官网，选择download，进行下载。"><a href="#进入markdown官网，选择download，进行下载。" class="headerlink" title="进入markdown官网，选择download，进行下载。"></a>进入markdown官网，选择download，进行下载。</h3><ul><li>列表1</li><li>列表2<br>  a 子列表1<br>  b 子列表2</li><li>列表3</li></ul><p>链接举例<br><a href="http://enfang.coding.me/enfang/" target="_blank" rel="noopener">酱油哥博客</a><br><img src="https://i.loli.net/2018/08/31/5b8920fdca03d.png" alt=""><br><em>斜体</em><br><strong>字体加粗</strong></p><hr><p>分割线</p><hr><p><code>&lt;html&gt;&lt;/html&gt;</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;markdown使用&lt;/tile&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;body&gt;&lt;body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><blockquote><p>引用</p></blockquote><p> <strong>欢迎访问我的博客</strong><br><a href="https://enfangzhong.github.io/">Git托管博客效果</a> </p><p><a href="http://enfang.coding.me/" target="_blank" rel="noopener">Coding托管博客效果</a></p><p><a href="https://itjyg.gitee.io" target="_blank" rel="noopener">码云托管博客效果</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;markDown的使用方法&quot;&gt;&lt;a href=&quot;#markDown的使用方法&quot; class=&quot;headerlink&quot; title=&quot;markDown的使用方法&quot;&gt;&lt;/a&gt;markDown的使用方法&lt;/h1&gt;&lt;h2 id=&quot;第一步：下载markdown&quot;&gt;&lt;a hr
      
    
    </summary>
    
      <category term="搭建个人博客" scheme="http://enfangzhong.github.io/categories/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="个人博客" scheme="http://enfangzhong.github.io/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="github博客" scheme="http://enfangzhong.github.io/tags/github%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="hexo博客" scheme="http://enfangzhong.github.io/tags/hexo%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
</feed>
