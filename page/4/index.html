<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">

  <script type="text/javascript" src="/js/src/crash_cheat.js"></script>
  

  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/favicon-32x32-next.png?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  






  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "9494e80d"
    });
  daovoice('update');
  </script>

  <meta name="description" content="你若晴天，我便安好。">
<meta name="keywords" content="AnFrank AnFrank&#39;s blog">
<meta property="og:type" content="website">
<meta property="og:title" content="AnFrank&#39;s Blog">
<meta property="og:url" content="http://enfangzhong.github.io/page/4/index.html">
<meta property="og:site_name" content="AnFrank&#39;s Blog">
<meta property="og:description" content="你若晴天，我便安好。">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AnFrank&#39;s Blog">
<meta name="twitter:description" content="你若晴天，我便安好。">



  <link rel="alternate" href="/atom.xml" title="AnFrank's Blog" type="application/atom+xml">




  <link rel="canonical" href="http://enfangzhong.github.io/page/4/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>AnFrank's Blog</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">


<!-- 加入APlayer音乐播放器 -->
<link rel="stylesheet" href="/dist/APlayer.min.css">
<div id="aplayer"></div>

<script type="text/javascript" src="/dist/APlayer.min.js"></script>
<script type="text/javascript" src="/dist/music.js"></script>




  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>



<a href="https://github.com/enfangzhong" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><script>
 function GetRandomNum(Min,Max)
  {
    var Range = Max - Min;
    var Rand = Math.random();
    return(Min + Math.round(Rand * Range));
  }
function setSidebarMarginTop (headerOffset) {
    return $('#sidebar').css({ 'margin-top': headerOffset });
  }
 function getHeaderOffset () {
    return $('.header-inner').height() + CONFIG.sidebar.offset;
  }
  window.onload=function(){
    var subtitle = "世界真的很小，好像一转身，就不知道会遇见谁。W世界真的很大，好像一转身，就不知道谁会消失。W成熟是给你陌生人看的，逗比是给朋友看的，幼稚是给喜欢的人看的。w";
     var mytitle = subtitle.split("W");
     var max = mytitle.length-1;
     var index = GetRandomNum(0,max);
     var text = mytitle[index];
     $("#helloTitle").html(text);
     var headOffset = getHeaderOffset();
     setSidebarMarginTop(headOffset);
     //动态subtitle设置
  }
</script>

<div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AnFrank's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  
    <p style="color:#black;font-size:18px;font-family:STXingkai" id="helloTitle" class="site-subtitle"></p>
  

  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-schedule">
    <a href="/schedule/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>微言</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-links">
    <a href="/links/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-link"></i> <br>友链</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-guestbook">
    <a href="/guestbook" rel="section">
      <i class="menu-item-icon fa fa-fw fa-comments"></i> <br>留言</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-love">
    <a href="/love/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>love</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-loveshow">
    <a href="/loveshow/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>loveshow</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>


  

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <!--轮播图-->

<script src="https://cdn.staticfile.org/jquery/2.1.1/jquery.min.js"></script>
<script src="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/js/bootstrap.min.js"></script>

<style type="text/css">

.glyphicon-chevron-left:before{
	/* content: "《" */
}
.glyphicon-chevron-right:before{
	/* content: "》" */
}

.right-siders{
   border-radius: 10px;
   /*margin-top: 5px;*/
   margin-bottom: 5px;
}

.my-carousel:hover{
  margin-left: 5px;
  //padding: 5px 1px;
  border-radius: 5px;
  transform: scale(1.1);
  box-shadow: 10px 10px 15px 2px rgba(0,0,0,.12), 0 0 6px 0 rgba(104, 104, 105, 0.1);
}

@media (max-width: 767px){
	.rights{
		display: none;
	}
	.carousel{
		width: 100% !important;
		height: 100% !important;
	}
	.slide{
		width: 100% !important;
		height: 100% !important;
	}

}

.carousel{
	width: 65%;
	height: 100%;
	position: relative;
}

.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  /*width: 5%;*/
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr=#80000000, endColorstr=#00000000, GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr=#00000000, endColorstr=#80000000, GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
/*
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
*/

.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  background: url('/images/icon-slides.png');
  background-position-y: -20px;
  left: 9px;
}

/*
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
*/

.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  background: url('/images/icon-slides.png');
  background-position-x: -50px;
  background-position-y: -20px;
  right: 0px;
}

.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}

.carousel-control .icon-prev:before {
  content: 2039;
}
.carousel-control .icon-next:before {
  content: 203a;
}

.carousel-indicators {
  position: absolute;
  bottom: 2px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 30px;
  height: 3px;
  margin: 5px;
  text-indent: -999px;
  border: 1px solid #bbb;
  border-radius: 10px;
  cursor: pointer;
  background-color: rgba(0, 0, 0, .24);
}
.carousel-indicators .active {
  width: 30px;
  height: 3px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
   /* margin-right: -10px; */
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: -10px;
  }
}
</style>

<div width="100%" height="320px" style="border: 0px; overflow: hidden; border-radius: 10px; margin-bottom: 25px;" scrolling="no">

    <div id="myCarousel" class="carousel slide" data-ride="carousel" data-interval="3500" style="float:left">

      <!-- 轮播（Carousel）指标 -->
      <ol class="carousel-indicators">
      
      
        <li data-target="#myCarousel" data-slide-to="0"></li>
        
      
        <li data-target="#myCarousel" data-slide-to="1"></li>
        
      
        <li data-target="#myCarousel" data-slide-to="2"></li>
        
      
        <li data-target="#myCarousel" data-slide-to="3"></li>
        
      
        <li data-target="#myCarousel" data-slide-to="4"></li>
        
      
        <li data-target="#myCarousel" data-slide-to="5"></li>
        
      
      </ol>

      <!-- 轮播（Carousel）项目 -->
      <div class="carousel-inner" style="height: 280px; border-radius: 10px; width: 100%;">
       
       
          
          <a class="item active" href="/messageboard/" target="_blank" style="height: 100%;">
            
            <img src="/images/blog-polish.png" class="nofancybox" style="width: 100%; height: 100%">
          </a>
          
          
      
          
            <a class="item" href="https://github.com/996icu/996.ICU" target="_blank" style="height: 100%;">
              <img src="/images/blog-polish.png" class="nofancybox" style="width: 100%; height: 100%;">
            </a>
        
      
          
            <a class="item" href="/tags/JVM/" target="_blank" style="height: 100%;">
              <img src="/images/blog-polish.png" class="nofancybox" style="width: 100%; height: 100%;">
            </a>
        
      
          
            <a class="item" href="/posts/258aee07.html" target="_blank" style="height: 100%;">
              <img src="/images/blog-polish.png" class="nofancybox" style="width: 100%; height: 100%;">
            </a>
        
      
          
            <a class="item" href="/tags/博客/" target="_blank" style="height: 100%;">
              <img src="/images/blog-polish.png" class="nofancybox" style="width: 100%; height: 100%;">
            </a>
        
      
          
            <a class="item" href="https://promotion.aliyun.com/ntms/act/campus2018.html" target="_blank" style="height: 100%;">
              <img src="/images/blog-polish.png" class="nofancybox" style="width: 100%; height: 100%;">
            </a>
        
      
      </div>

      <!-- 轮播（Carousel）导航 -->
      <a class="left carousel-control" data-target="#myCarousel" href="javascript:void(0);" role="button" data-slide="prev">
          <span class="glyphicon glyphicon-chevron-left" aria-hidden="true"></span>
      </a>
      <a class="right carousel-control" data-target="#myCarousel" href="javascript:void(0);" role="button" data-slide="next">
          <span class="glyphicon glyphicon-chevron-right" aria-hidden="true"></span>
      </a>

	</div>

	<div class="rights" style="width: 30%; height: 280px; margin-right: 0px;margin-left: 20px;float: left;">
       <!-- 天气插件 -->
       <iframe scrolling="no" height="50px" frameborder="0" allowtransparency="true" src="//i.tianqi.com/index.php?c=code&id=12&color=%23&bdc=%23&icon=1&py=wuhan&num=5&site=12"></iframe>
    
       <!-- 关于博主 -->
       <div class="my-carousel">
         <a href="/about/" target="_blank">
            <img class="right-siders nofancybox" src="/images/about.png" width="100%">
         </a>
       </div>
    
       <!-- 推荐阅读 -->
       <div class="my-carousel">
         <a href="/books/" target="_blank">
           <img class="right-siders nofancybox" src="/images/read.png">
         </a>
       </div>

      <!-- 光影留念 -->
      <div class="my-carousel">
        <a href="/photos/" target="_blank">
          <img class="right-siders nofancybox" src="/images/photo.png">
        </a>
      </div>

      <!-- 留下脚印 -->
      <div class="my-carousel">
        <a href="/guestbook/" target="_blank">
          <img class="right-siders nofancybox" src="/images/foot.png">
        </a>
      </div>

   </div>

</div>



  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第08章浅层排序模型逻辑回归/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnFrank">
      <meta itemprop="description" content="你若晴天，我便安好。">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnFrank's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/01/个性化推荐算法实践第08章浅层排序模型逻辑回归/" itemprop="url">
                  个性化推荐算法实践第08章浅层排序模型逻辑回归
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-01 18:03:11" itemprop="dateCreated datePublished" datetime="2019-06-01T18:03:11+08:00">2019-06-01</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-18 14:22:18" itemprop="dateModified" datetime="2019-12-18T14:22:18+08:00">2019-12-18</time>
              
            
          </span>


          

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/推荐算法/" itemprop="url" rel="index"><span itemprop="name">推荐算法</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/01/个性化推荐算法实践第08章浅层排序模型逻辑回归/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2019/06/01/个性化推荐算法实践第08章浅层排序模型逻辑回归/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="个性化推荐算法实践第08章浅层排序模型逻辑回归"><a href="#个性化推荐算法实践第08章浅层排序模型逻辑回归" class="headerlink" title="个性化推荐算法实践第08章浅层排序模型逻辑回归"></a>个性化推荐算法实践第08章浅层排序模型逻辑回归</h1><p>本章节重点介绍点击率预估模型，逻辑回归模型，以及选取实例数据集，从特征选择到模型训练、模型评估等几个方面来代码实战逻辑回归模型。</p>
<p>本章节重点介绍一种排序模型，逻辑回归模型。从逻辑回归模型的背景知识与数学原理进行介绍。并介绍样本选择与特征选择相关知识。最后结合公开数据集。代码实战训练可用的逻辑回归模型。</p>
<h3 id="逻辑回归模型的背景介绍"><a href="#逻辑回归模型的背景介绍" class="headerlink" title="逻辑回归模型的背景介绍"></a>逻辑回归模型的背景介绍</h3><h4 id="一、LR（logistic-regression逻辑回归）背景知识介绍"><a href="#一、LR（logistic-regression逻辑回归）背景知识介绍" class="headerlink" title="一、LR（logistic regression逻辑回归）背景知识介绍"></a>一、LR（logistic regression逻辑回归）背景知识介绍</h4><p>将会介绍什么是点击率预估、什么是分类模型以及LR模型的基本使用流程、LR模型的基本训练流程，从这几个方面介绍LR的背景知识。</p>
<h5 id="1-点击率预估与分类模型"><a href="#1-点击率预估与分类模型" class="headerlink" title="1. 点击率预估与分类模型"></a>1. 点击率预估与分类模型</h5><p>什么是点击率预估呢？</p>
<p>相信点击率的概念大家都知道，在系统中，点击率 = 点击的数目 / 总展现的数目，而点击率预估就是针对特定的用户在当前上下文结合用户当前的特征给出的item可能被点击的概率，预估方法可以是一些简单的规则也可以是使用模型，目的就是得到不同的item在此时应该被展现的顺序关系。</p>
<p>点击率预估在推荐、搜索、广告领域都被广泛使用。</p>
<p>那么什么是分类模型呢？</p>
<p>用一个简单的例子进行讲解：假如去菜市场买牛肉，这个牛肉可以称之为新鲜或者不新鲜，我们之前的判断就是根据这个肉的时间。假如有一个模型可以再牛肉到来的时候就给出标签，是新鲜或者不新鲜，那么这样的一个模型就是一个分类模型，而且是一个二分类，因为这里的label只有新鲜或者不新鲜。</p>
<p>当然，这个分类模型也可以是多分类，如果说这个label有很多个，那么便是多分类。我们这里讲的点击率预估实际也是一个二分类问题，因为，点了样本就是1，没点就是0。就在这两类之间，我们会给出每一个样本预估出它倾向1的概率，基于次概率的大小决定了此item的展现顺序。</p>
<h5 id="2-什么是LR？"><a href="#2-什么是LR？" class="headerlink" title="2. 什么是LR？"></a>2. 什么是LR？</h5><p>假设二维空间中有一些数据点，我们想用一条直线来拟合这些数据点经过的路径，这便是回归。但是，LR是一个分类模型，需要在样本进入模型之后给出分类标签。</p>
<p>所以LR对于回归得到的数值会进行一个处理，使之变成0,1这样的标签。这个梳理就是将得到的数值输入到一个函数中，这个函数就是单位阶跃函数，后面会详细讲解这个函数。</p>
<p>这里说的二维空间是只有一个特征的，用在之前讲解的牛肉是不是新鲜这个案例里，这个特征就是时间。那么如果扩展到三维空间中，可以再加入一个特征，比如说这个牛肉的含水量。那么这个时候，可能就不是用一条直线，而是用一个面来拟合整体样本经过的路径。</p>
<h5 id="3-sigmoid函数：单位阶跃函数"><a href="#3-sigmoid函数：单位阶跃函数" class="headerlink" title="3. sigmoid函数：单位阶跃函数"></a>3. sigmoid函数：单位阶跃函数</h5><p>这个函数有一个特点，那就是输入为0的时候，值为0.5，但是输入&gt;0 的部分，很快便逼近于1，输入 &lt; 0 的部分，很快便逼近于0，所以称之为阶跃函数。</p>
<p>这样的函数，很少有在中间的部分，所以可以很容易的分辨出是1还是0，正好符合0、1分类模型的要求。</p>
<h5 id="4-LR模型的工作流程"><a href="#4-LR模型的工作流程" class="headerlink" title="4. LR模型的工作流程"></a>4. LR模型的工作流程</h5><p>前面介绍过LR模型的主体流程是：首先对数据进行一个拟合，不管是二维空间里 y = k <em> x + b 这样的直线，还是三维空间里y = a1 </em> x1 + b2 <em> x2 + c 这样的平面，亦或是更高维度我们需要学习更多的参数。像y = k </em> x + b 中的k、b，像三维空间里我们用平面 y = a1 <em> x1 + b2 </em> x2 + c 这里面的a1、b2、c这样的参数都是我们需要学习的。学习完这些参数之后，也就得到了模型。得到模型之后，用参数与输入的特征进行相乘，同时将得到的数值放入前面介绍过的单位阶跃函数中得到类别。</p>
<p>下面用具体的例子，详细介绍一下具体的工作流程：</p>
<p><img src="/2019/06/01/个性化推荐算法实践第08章浅层排序模型逻辑回归/1565107327628.png" alt="1565107327628"></p>
<p>这里是三个训练数据，训练数据的label就是女朋友是不是开心。我们看到这是一个二分类问题，女朋友开心，label就是1；女朋友不开心，那么label就是0。同时这里每一个训练样本都有三个特征，分别是买礼物、说早安、陪吃饭。</p>
<p>样本1 是买了礼物，说了早安，陪了吃饭，那么女朋友是开心的。</p>
<p>样本2 是没有买礼物，没有说早安，陪了吃饭，女朋友是不开心的。</p>
<p>样本3 是买了礼物，说了早安，，没有陪吃饭，女朋友是开心的。</p>
<p>这里的目标是通过这三个样本，学习出一个LR的模型，然后能够通过这个模型，帮助我们预测女朋友是否开心。我们需要学习的参数是什么呢？很明显就是 a <em> x1 + b </em> x2 + c * x3，这里的a、b、c这三个参数，在得到了这三个参数之后，在接下来的某一天，我们将买没买礼物，说没说早安，有没有陪吃饭带入到这个公式中，也便能够自己知道女朋友是否开心。</p>
<p>以上就是LR模型的工作流程</p>
<h5 id="5-LR模型的整体训练流程"><a href="#5-LR模型的整体训练流程" class="headerlink" title="5. LR模型的整体训练流程"></a>5. LR模型的整体训练流程</h5><p>（1）从log中获取训练样本与特征</p>
<p>在工业界中，样本与特征都不会是处理好的，我们需要自己从日志中，根据需要去提取。比如在推荐系统中，我们可能有展示日志，有点击日志，有基于<em>点击</em>统计User Profile,也有入库时就获得的item info信息。首先，我们需要判断，哪些展现，哪些点击是需要的，也就是样本选择。因为，有很多样本含有脏数据，我们需要去除掉。</p>
<p>其次，我们需要判断我们需要的是用户的哪些特征。比如说用户的年龄、性别，或者是item的哪些特征，比如item的种类、item的title、item的时长等等。这就是特征选择。</p>
<p>（2）模型的参数学习</p>
<p>获取了样本与特征之后，需要决定模型学习的学习率，是否正则化，以及选用哪一种正则化的方式，同样还有学习的方法等等参数设定。</p>
<p>得到了模型之后，同样需要进行离线评估。</p>
<p>这里面包含了模型本身的指标，还有模型应用到测试集中的指标，来看一下我们的模型是否可用。如果模型可用，需要将模型实例化。</p>
<p>（3）模型预测</p>
<p>这里的预测包含可能是将已经实例化的模型录入到内存中提供服务，也可能是基于已经实例化好的模型再搭建一个模型的服务。</p>
<p>预测部分需要将待预测的数据集带入到训练格式去抽取特征，抽取完特征的待预测的样本放入到模型中便能得到预测的结果。</p>
<h5 id="6-LR模型的优缺点"><a href="#6-LR模型的优缺点" class="headerlink" title="6. LR模型的优缺点"></a>6. LR模型的优缺点</h5><p>优点：易于理解，计算代价小</p>
<p>这个易于理解是指，我们在建模过程中我们把则认为对结果又影响的特征罗列出来就可以，比如说我们认为哪些因素会影响女朋友开心或不开心呢，那就是买没买礼物，说没说早安，有没有陪吃饭，有没有送回寝室…然后学习这些因素的权重就可以。</p>
<p>在推荐系统中，我们预估这个item能不能被user点击。同样也可以选取一些关键的特征，比如说item的历史点击率，item所属的类型，然后用户喜欢看的类型，这个item的长度等等。</p>
<p>由于LR模型是一个浅层模型，而且需要学习的参数的数目与特征的维度是一一对应的，比如说有100维的特征，那么就需要学习100个参数，所以计算代价是比较小的。</p>
<p>缺点：<strong>容易欠拟合</strong>，<strong>需要特征工程来补足模型容易欠拟合的缺点</strong></p>
<p><strong>欠拟合</strong>是一个专业术语，解释一下：</p>
<p>欠拟合表示模型没有从训练数据中学习到所有的规律。我们以女朋友是否开心为例，比如说陪吃饭，以及买礼物都不能单独的影响女朋友是否开心，也许这里需要一个交叉特征，比如陪吃饭和买礼物放在一起的一个特征。当这两样同时做了，女朋友才会开心，如果没做或者是只做了一样，女朋友都是不会开心的。</p>
<p>那么在这种情况下，我们发现，由于LR不能主动的去想学习这些交叉特征，所以需要我们大量的构造特征。这也就是说，特征工程来补足模型容易欠拟合的缺点。</p>
<h4 id="二、LR算法数学原理解析"><a href="#二、LR算法数学原理解析" class="headerlink" title="二、LR算法数学原理解析"></a>二、LR算法数学原理解析</h4><p>将会介绍LR模型的函数表达式、损失函数以及梯度，并且介绍什么是正则化。（LR模型参数迭代的数学原理）</p>
<p>逻辑回归模型的数学原理</p>
<h5 id="1、单位阶跃函数（sigmoid）"><a href="#1、单位阶跃函数（sigmoid）" class="headerlink" title="1、单位阶跃函数（sigmoid）"></a>1、单位阶跃函数（sigmoid）</h5><p>单位阶跃函数及其导数</p>
<p>单位阶跃函数的函数表达式：</p>
<script type="math/tex; mode=display">
f(x)=\frac{1}{1+\exp (-x)}</script><p>当 x = 0 时，f(x) = 0.5 ; 当  x = 10 时，f(x) 接近于1 ，这也就是之前说过的，当x &gt; 0 的时候，会非常快速的接近于1 ；当x &lt; 0 的时候，会非常快速的接近于0 。这完全符合LR模型，对0-1分类时的要求。</p>
<p>下面再来看一下单位阶跃函数的导数：</p>
<script type="math/tex; mode=display">
f^{\prime}(x)=\frac{\exp (-x)}{(1+\exp (-x))^{2}}</script><p>经过简单转换，上述式子转换为：</p>
<script type="math/tex; mode=display">
f^{\prime}(x)=\frac{1}{1+\exp (-x)} * \frac{1+\exp (-x)-1}{1+\exp (-x)}</script><p>也就是 f(x) 的导数 = f(x) * (1 - f(x))</p>
<h5 id="2、LR模型的函数表达式"><a href="#2、LR模型的函数表达式" class="headerlink" title="2、LR模型的函数表达式"></a>2、LR模型的函数表达式</h5><p>LR模型分为两个步骤：</p>
<ol>
<li>拟合数据点；</li>
</ol>
<p>公式：</p>
<script type="math/tex; mode=display">
w=w_{1} \times x_{1}+w_{2} \times x_{2}+\ldots+w_{n} \times x_{n}</script><p>这里的w1，w2…是需要学习的参数，这里的x1，x2…是选取的特征。</p>
<p>用上一部分中的例子来分析，x1可能是陪吃饭，x2可能是送礼物等。</p>
<ol>
<li>将第一步回归得到的数值带入到阶跃函数中，进而得到分类。</li>
</ol>
<script type="math/tex; mode=display">
y=\operatorname{sigmoid}(w)</script><p>也就是女朋友是否开心的倾向性，或者说item是否被用户点击的倾向性。</p>
<p>LR模型的函数表达式就介绍到这里，之前介绍个性化召回算法LFM的时候，曾经介绍过一种最优化的方法来学习参数—梯度下降。</p>
<p>梯度下降需要首先设定损失函数，进而得到梯度，完成参数的迭代。</p>
<p>下面看一下LR模型的损失函数。</p>
<h5 id="3、LR模型的损失函数"><a href="#3、LR模型的损失函数" class="headerlink" title="3、LR模型的损失函数"></a>3、LR模型的损失函数</h5><script type="math/tex; mode=display">
\operatorname{loss}=\log \prod_{i=1}^{n} p\left(y_{i} | x_{i}\right)</script><p>这个损失函数采用的是log损失函数，与之前介绍word2vec算法的损失函数是一致的。</p>
<p>这里没有用平方损失函数的原因是：这里LR模型需要两个步骤，第二步是将第一步拟合的值带入到单位阶跃函数中，如果此时使用平方损失函数的话，损失函数并不是下凸的，而且有很多个波谷，我们在梯度下降的过程中，很容易学习到并不是最低点的波谷，也就是不能学习到最小化的loss function。所以这里采用log 损失函数。</p>
<p>下面解释一下公式：</p>
<p>i ：样本的数目。也就是说这里有n个样本。那么对于第1个样本呢，该模型下希望预测的概率最准。</p>
<p>p：概率，也就是本来是1的label，希望也是1；如果是0，预测成0</p>
<p>为了统一得到最大化的概率，当label是0的时候，我们就预测（1-这个条件概率），那么整体对于这个损失函数，我们直接最大化这个损失函数，便能够将参数学习到。</p>
<p>下面看一下条件概率：</p>
<script type="math/tex; mode=display">
p\left(y_{i} | x_{i}\right)=h_{w}\left(x_{i}\right)^{y_{i}}\left(1-h_{w}\left(x_{i}\right)\right)^{1-y_{i}}</script><p>这个条件概率就是刚才解释过的，如果这里y = 0，我们看到是后面这一部分起作用，那么也就是我们说的来预测（1-这个概率）。这里的w就是上一篇文章介绍的LR函数表达式的第一部分，也就是所有的参数与特征相乘得到的结果。</p>
<p>下面看一下，将条件概率带入到损失函数中，得到的：</p>
<script type="math/tex; mode=display">
\operatorname{loss}=-\left(y_{i} \log h_{w}\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-h_{w}\left(x_{i}\right)\right)\right)</script><p>来看单一样本，这里不再关心 n 个样本。</p>
<p>我们来看单一样本，单一样本这里我们知道$h_{w}\left(x_{i}\right)^{y_{i}}$被log一下，$y^i$是可以提到前面去的；同时，相乘被log一下就变成了相加，就得到了上面式子。</p>
<p>之前说过，loss函数需要最大化，那么这里加了一个负号，所以上面式子中的 loss 需要最小化，也就是使用梯度下降法就可以。</p>
<p>这里再次重申，这里$x_i$表示，第 i 个样本对应的所有特征；这里$y_i$ 是第 i 个样本对应的label。如果想表示第 i 个样本的第一个特征，会在$x_i$的右上角标明$x_{i}^1$,以示区分。</p>
<h5 id="4、梯度"><a href="#4、梯度" class="headerlink" title="4、梯度"></a>4、梯度</h5><p>下面看一下loss损失函数对于参数w的梯度：</p>
<p>首先，这里选取参数的某一个，这里选择特征$x_j$对应的参数$w_j$来进行演示求偏导，这里应用链导法则（也便等于loss函数对于LR模型输出的偏导）：</p>
<script type="math/tex; mode=display">
\frac{\partial \operatorname{loss}}{\partial w_{j}}=\frac{\partial  \operatorname{loss}}{\partial h_{w}\left(x_{i}\right)} \frac{\partial h_{w}\left(x_{i}\right)}{\partial w} \frac{\partial w}{\partial w_{j}}</script><p>这里$h_{w}\left(x_{i}\right)$表示：第 i 个样本的输入到 LR 模型中，我们给出的输出。它的公式是$f(x)=\frac{1}{1+\exp (-w)}$。这里的W表示为$w=w_{1} \times x_{1}+w_{2} \times x_{2}+\ldots+w_{n} \times x_{n}$，那么这里$\frac{\partial w}{\partial w_{j}}$就是$x_j$。也就是第i个样本的第j维度的特征。</p>
<p>损失函数</p>
<script type="math/tex; mode=display">
\operatorname{loss}=-\left(y_{i} \log h_{w}\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-h_{w}\left(x_{i}\right)\right)\right)</script><p>损失函数求导</p>
<p>看一下第一部分：</p>
<script type="math/tex; mode=display">
\frac{\partial \operatorname{loss} }{\partial h_{w}\left(x_{i}\right)}=-\left(\frac{y_{i}}{h_{w}\left(x_{i}\right)}+\frac{y_{i}-1}{1-h_{w}\left(x_{i}\right)}\right)</script><p>看一下剩余部分：</p>
<p>根据sigmod阶跃函数f(x) 的导数 = f(x) * (1 - f(x))。$\frac{\partial w}{\partial w_{j}}$就是$x_{i}^j$。</p>
<script type="math/tex; mode=display">
\frac{\partial h_{w}\left(x_{i}\right)}{\partial w} \frac{\partial w}{\partial w_{j}}=h_{w}\left(x_{i}\right)\left(1-h_{w}\left(x_{i}\right)\right) x_{i}^{j}</script><p>带入梯度公式，得到：</p>
<script type="math/tex; mode=display">
\frac{\partial l o s s}{\partial w_{j}}=\left(h_{w}\left(x_{i}\right)-y_{i}\right) x_{i}^{j}</script><p>那么 i 样本对应的梯度已经得到了，如果这里有n个样本的话。同理，对每一个样本求得梯度，然后去 1/n ，就得到了平均梯度。</p>
<p>得到平均梯度之后，用梯度下降对这一维度的特征进行更新：</p>
<script type="math/tex; mode=display">
w_{j}=w_{j}-\alpha \frac{\partial loss}{\partial w_{j}}</script><p>当然，要对所有维度的特征都进行更新，即w1, w2…同样也是按照这种方式来进行更新迭代的，这里$\alpha$是指学习率。</p>
<p>四、正则化</p>
<ol>
<li>什么是过拟合？</li>
</ol>
<p>过拟合就是模型对于训练数据过分的学习，对训练数据完美的适配。有时候训练数据并不能反应事情的本质。</p>
<p>eg.也许女朋友不开心是因为考试挂科了，假如给与我们的训练数据中，全都没有挂科，那么我们后面如何买礼物，如何陪吃饭，我们都会发现，女朋友都会不开心。我们便无法学习到真正事物的本质。也就是我们所说的泛化能力减弱。</p>
<p>完全为防止过拟合，就提出了正则化的概念。</p>
<p>常用的正则化方法有两种：L1、L2</p>
<p>首先看一下<strong>L1正则化</strong>的公式：</p>
<script type="math/tex; mode=display">
\operatorname{loss}_{-} n e w=\operatorname{loss} +\alpha \sum_{i=1}^{n}\left|w_{i}\right|</script><p>L1正则化的公式，是在原来的损失函数的基础上，将所有权值的绝对值求和，这种方式下，使模型的参数变得稀疏，会产生一部分为0的参数。物理意义上说，就是起作用的特征变少了，模型变得简单了。这样也就不容易过拟合了。</p>
<p>下面看一下<strong>L2正则化</strong>的公式：</p>
<script type="math/tex; mode=display">
\operatorname{loss}_{-} n e w=\operatorname{loss}+\alpha|w|^{2}</script><p>是在原来损失函数的基础上，加上每一个权重的平方和。因为要最小化损失函数，所以这里倾向于将每一个权重学的比较小。试想一下，如果某一个权重比较大的话，面对数据分布变化的特质数据会产生结果上的非常大的扰动。这也不利于模型的泛化能力。$\alpha$是指正则化参数。</p>
<h4 id="三、样本选择与特征构建"><a href="#三、样本选择与特征构建" class="headerlink" title="三、样本选择与特征构建"></a>三、样本选择与特征构建</h4><p>将会介绍模型训练中非常重要的一步，那就是<strong>样本的选择、特征的选择</strong>与处理。<br>我们知道样本与特征决定了模型表现的天花板，而选择什么样的模型只是来逼近这个天花板。</p>
<p>回忆一下，8-1中给出的实例，当时用了3个样本，3个特征来演示LR模型的工作原理。但是，可能会有疑问，为什么只有3个样本？在实际的项目中，可能会有非常多的样本，其中有些样本是可以用的，有些样本是不可以用的，到底哪些可以用，哪些不可以用。包括我们有很多的特征，依据什么规则来判断是否对最终的结果有效都是下面要介绍的内容。</p>
<h4 id="3-1、样本选择"><a href="#3-1、样本选择" class="headerlink" title="3.1、样本选择"></a>3.1、样本选择</h4><p>下面首先看一下样本方面的知识。</p>
<p>在点击率预估过程中，需要的样本是带有label的，也就是点击或者未点击，这是大前提。也就是说，每个用户的每次刷新，我们都能对应上item是否被点击。这么多的样本都是我们训练时候的有效样本么？</p>
<p>当然不是！</p>
<p>下面首先看一下样本的选择规则。</p>
<h5 id="1、样本选择规则"><a href="#1、样本选择规则" class="headerlink" title="1、样本选择规则"></a>1、样本选择规则</h5><p>这里面主要包含两个因素，1. 采样比例； 2. 采样率。</p>
<p>①采样比例</p>
<p> 正负样本需要维持一个正常的比例，正常的比例需要符合产品的实际形式。比如说某个产品，用户三次到来就会产生一次购买，那么我们的正负样本就是1:2的比例。</p>
<p>当然，模型训练还有很多的采样规则，比如说在某些模型训练的时候，我们需要确保userid的样本达到平均水平，比如说最少要20个。这个时候，就需要做样本增强。对于该userid下的样本，我们需要给他一个特定的权重，来确保它虽然样本少，但是也能达到最低要求。</p>
<p>②采样率</p>
<p>当模型没有办法用所有的训练数据的时候，必须设定一定的采样率。常用的随机采样的方法就是其中的一种。</p>
<h5 id="2、样本过滤规则"><a href="#2、样本过滤规则" class="headerlink" title="2、样本过滤规则"></a>2、样本过滤规则</h5><p>样本过滤规则有两个大方面：</p>
<ol>
<li>结合业务情况</li>
</ol>
<p>比如在样本选取时，需要去除爬虫带来的虚假请求，测试人员构造的测试 id 数据，作弊数据等等。还需要根据特定场景下模型的目标来保证样本选取的有效性。</p>
<ol>
<li>异常点的过滤</li>
</ol>
<p>常用的方法有基于统计的方法。举个例子，比如说，某个特征，就以某item被评论的数目这个特征为例，99%的评论数目都均匀的分布在0-5000之间，而top 1% 有几十万、几百万这种数量级的评分。对于这种数据，我们选取一个阈值，大于这个阈值的直接去掉，为什么呢？因为这会给我们在特征归一化的过程中，造成极大的样本分布不均。</p>
<p>还有就是基于距离的方法。比如说，某一样本数据与其余样本点的之间的距离，有80%都超过了我们所设定的阈值，这个样本就需要被过滤掉。</p>
<p>3.例子</p>
<p>下面以具体的实例来说明如何选取有效的数据来保证模型训练的目标。</p>
<p><img src="/2019/06/01/个性化推荐算法实践第08章浅层排序模型逻辑回归/1569135721017.png" alt="1569135721017"></p>
<p>这是某个推荐系统展现给用户的推荐列表，用户依次看到的是itemid1 、itemid2 ….itemid5，同时用户对着5个item分别做了不同的处理，用户点击了itemid1和itemid4，并没有点击itemid2、itemid3、itemid5。这时，我们在构建训练样本的时候，是不是这5个样本都需要呢？</p>
<p>答案是<strong>否定的</strong>。</p>
<p>这里我们只需要前四个。为什么不要第5个呢？下面解释一下原因呢：</p>
<p>我们模型的最终学习目的是希望用户能够在最开始的位置发生点击，而不用下拉。所以，目标是将最终的推荐列表学习成1,1,0,0,0的形式。在这里我们发现，逆序对是0,0,然后1。对于最后的这个0，我们有两方面的原因不选择它作为训练样本，第一方面，我们不能确定用户是否真的看到了这个数据，对于以上的4条，我们可以确定用户真的看到了，因为最后的点击发生在第四条，用户想要点击到第4条，就需要下拉看到第4条数据；第二个原因，是因为这个数据对我们学习的目标是没有帮助的，如果选取还会增加负样本所占的比例。</p>
<p>结合刚才的分析，最终在这一次展现当中，我们得到了4条样本。</p>
<p><img src="/2019/06/01/个性化推荐算法实践第08章浅层排序模型逻辑回归/1569135751384.png" alt="1569135751384"></p>
<p>他们分别是位于位置1、位置2、位置3、位置4的样本，我们对每一条样本选取了一些特征，打上label。同时并没有选取位置5的数据。</p>
<h4 id="3-2、特征方面"><a href="#3-2、特征方面" class="headerlink" title="3.2、特征方面"></a>3.2、特征方面</h4><p>首先对特征进行一个概述，特征如果按照数值类型可以分为连续值类型和离散值类型。举个例子，连续值类型，像item的平均观看时长可能是3.75分钟，4.28分钟等等。所以说，是不可穷举的。而离散值类型是可以穷举的，比如说某人的学历，就是小学、初中、高中、大学、研究生、博士等。</p>
<p>同样，按照统计的力度同样可以分为低纬度和高纬度，低纬度的特征包含像人的年龄、性别，高纬度的特征像这个人他过去30天喜欢什么样的电影，这个人历史上喜欢什么类型的电影等等。</p>
<p>根据数值变化的幅度，可以将特征分为稳定特征和动态特征，稳定特征就想item的历史点击率，而动态特征就想item的天级别的点击率。</p>
<p>特征的概述就概述到这里。</p>
<p>下面看一下如何做特征选择。</p>
<h5 id="1-特征的统计和分析"><a href="#1-特征的统计和分析" class="headerlink" title="1. 特征的统计和分析"></a>1. 特征的统计和分析</h5><p>首先需要知道特征的获取难度，比如说，我们想使用用户的年龄和性别这两个特征，我们发现用户画像中并没有这两个维度。如果需要的话，我们需要根据用户的行为建立一个模型，预估这两个特征。显然成本是较大的，就需要放弃。</p>
<p>第二个是需要看一下覆盖率，同样是用户的年龄和性别这两个特征进行举例。如果说发现能够获取到，但是在整体的覆盖率上不足1%，我们也是不能够用的。</p>
<p>下一个就是特征的准确率，我们发现视频的平均播放时长都只有几毫秒，显然这是违背常识的，那么这个特征也是不能够使用的。</p>
<p>在我们初步分析了哪些特征可以使用之后，我们到底选取什么样的特征来完成训练呢？</p>
<h5 id="2-特征的选择"><a href="#2-特征的选择" class="headerlink" title="2. 特征的选择"></a>2. 特征的选择</h5><p>主要分为两个大方面：</p>
<p>（1）根据自己的建模常识，也就是说我们想预估一个目标的话，我们知道哪些特征与这个目标是紧密相连的。比如我们想预估这个item的点击率，那么很明显这个item的类别与这个用户喜欢观看的类别是强相关的特征。还有一些强相关的特征，比如这个item的历史点击率，在我们初步根据自己的常识选取了这么多的特征之后，我们训练出了第一版的模型。</p>
<p>（2）那么剩下的特征选择第二步就是基于模型的表现。我们训练完了基线版的模型之后，如果不能够满足我们对于目标的需要的话，我们应该不停的增减特征来发现增减特征对模型指标的影响。如果说减掉某个特征，反而指标变好的话，那么很明显这个特征就不应该是我们需要的。</p>
<p>实际上，在我们选定了特征之后，想要让模型能够识别这些特征，我们需要将特征变成数字。这也就是特征的预处理。</p>
<h5 id="3-特征的预处理"><a href="#3-特征的预处理" class="headerlink" title="3. 特征的预处理"></a>3. 特征的预处理</h5><p>特征的预处理往往包含三大步骤：</p>
<p>（1）缺省值的填充</p>
<p>缺省值是指某些样本里的某些特征是缺失的，我们应该用什么样的规则来填充呢？</p>
<p>业界常用的规则有：使用这个特征的众数，或者是平均数来填充。</p>
<p>（2）归一化</p>
<p>归一化是指将不同维度的数值特征都转化到0,1之间，这样有利于<strong>减少由于不同特征绝对数值的影响</strong>，对模型权重的影响。举个例子：比如说有一个特征是收入，这个收入可能是几千几万的大数据；还有一个特征是工作的时长，这个工作的时长可能每周都在40-60小时之间，这样可以发现数量值是不一样的。我们需要将他们归一化，这个归一化可以使用排序归一化，以及最大值归一化等等。</p>
<p>排序归一化是指：这一维度的特征按数字进行排序，排序最大的数字将变成1，排序最小的数字变成0。那么举一个例子：如果说一共有10个样本，那么这10个样本之间进行了排序，按照数字的大小，显然最小的对应0.1，第二小的对应0.2，依次类推，最大的变成了1，这样就归一化了0-1之间。如果样本的数目更大，就依此类推。</p>
<p>最大值归一化是指：我们统计出这一维度特征的最大值，然后让所有数字都除以这个最大值，显然这一维度的特征就会被归一化到0-1之间。</p>
<p>（3）离散化</p>
<p>特征的离散化并不是所有模型都需要的，但是逻辑回归LR模型是需要的。</p>
<p>下面以具体的例子讲解什么是离散化：</p>
<p>首先以一个连续值举例，这个值表示人平均每周工作的时长，这个人每周工作23小时，我们怎么离散化呢？</p>
<p><img src="/2019/06/01/个性化推荐算法实践第08章浅层排序模型逻辑回归/1569135782966.png" alt="1569135782966"></p>
<p>首先需要统计一下，我们样本当中这一维度特征的分布，比如我们想四段离散化，这里我们就需要统计一下四分之一分位点。这里就是0-18小时是一个区间，18-25小时是一个区间，25-40小时是一个区间，40-无穷大是另一个区间。那么我们总共有4个区间，这4个区间我们发现23是位于18-25这个区间，那么就离散化成了 [0,1,0,0]。如果这个值不是23，而是60，显然特征就会被离散化成[0,0,0,1]。当然，这里也可以不被4段化，而是5段，我们只需要按相应的去统计就可以了。</p>
<p>下面再以一个离散值来举例，如果说一个人的国家是中国，系统中只有三个国家，那中国对应的实例化特征就是[1,0,0]，美国便是[0,1,0]。</p>
<p><img src="/2019/06/01/个性化推荐算法实践第08章浅层排序模型逻辑回归/20190409210625322.png" alt="img"></p>
<h3 id="四、代码实战LR之样本选择"><a href="#四、代码实战LR之样本选择" class="headerlink" title="四、代码实战LR之样本选择"></a>四、代码实战LR之样本选择</h3><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第07章综述学习排序/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnFrank">
      <meta itemprop="description" content="你若晴天，我便安好。">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnFrank's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/01/个性化推荐算法实践第07章综述学习排序/" itemprop="url">
                  个性化推荐算法实践第07章综述学习排序
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-01 17:03:11" itemprop="dateCreated datePublished" datetime="2019-06-01T17:03:11+08:00">2019-06-01</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-18 14:22:08" itemprop="dateModified" datetime="2019-12-18T14:22:08+08:00">2019-12-18</time>
              
            
          </span>


          

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/推荐算法/" itemprop="url" rel="index"><span itemprop="name">推荐算法</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/01/个性化推荐算法实践第07章综述学习排序/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2019/06/01/个性化推荐算法实践第07章综述学习排序/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="个性化推荐算法实践第07章综述学习排序"><a href="#个性化推荐算法实践第07章综述学习排序" class="headerlink" title="个性化推荐算法实践第07章综述学习排序"></a>个性化推荐算法实践第07章综述学习排序</h1><p>综述学习排序的思路，并介绍工业界排序架构以及本课程重点讲解的学习排序模型。</p>
<h3 id="一、什么是学习排序（Learn-To-Rank）？"><a href="#一、什么是学习排序（Learn-To-Rank）？" class="headerlink" title="一、什么是学习排序（Learn To Rank）？"></a>一、什么是学习排序（Learn To Rank）？</h3><p>说起学习排序，首先介绍一下排序，排序是在搜索场景以及推荐场景中应用的最为广泛的。</p>
<p>传统的排序方法是基于构造相关度函数，使相关度函数对于每一个文档进行打分，得分较高的文档，排的位置就靠前。但是，随着相关度函数中特征的增多，使调参变得极其的困难。所以后来便将排序这一过程引入机器学习的概念，也就变成这里介绍的学习排序。</p>
<p>那么这里介绍的排序都是学习排序中的<strong>Pointwise</strong>，指对于单独的文档进行预估点击率，将预估点击率最大的文档排到前面。</p>
<p>所以特征的选择与模型的训练是至关重要的。</p>
<p><strong>那么什么是学习排序呢？</strong></p>
<p>学习排序：将个性化召回的物品候选集根据物品本身的属性结合用户的属性，上下文等信息给出展现优先级的过程便是学习排序。</p>
<p>下面用一个例子进行展示：</p>
<p><img src="/2019/06/01/个性化推荐算法实践第07章综述学习排序/1564992344840.png" alt="1564992344840"></p>
<p>假设这里有一个用户A，基于他的历史行为给出了召回，可能是很多种召回算法，经过合并之后得到的6个item[a,b,c,d,e,f]。经过排序，最终将这6个item的优先级固定为c、a、f、d、b、e。</p>
<p>得到优先级的过程就是由排序得到的。分别根据item本身的属性，以及user当前的一些上下文和user固定的一些属性，得到此时最佳的顺序应该是将c给展示，这样以保证最后的点击率最高。</p>
<h3 id="二、排序在个性化推荐系统中的重要作用"><a href="#二、排序在个性化推荐系统中的重要作用" class="headerlink" title="二、排序在个性化推荐系统中的重要作用"></a>二、排序在个性化推荐系统中的重要作用</h3><p>之前介绍过，在个性化的算法中后端的主要流程是：召回—&gt;排序—&gt;策略调整。</p>
<p>我们说过，召回决定了推荐效果的天花板，那么排序就决定了逼近天花板的程度。</p>
<p>1、排序决定了最终的推荐效果</p>
<p>用户看到的顺序基本就是由排序这一步骤所决定的，如果用户在前面的位置就能够看到自己感兴趣的物品，那么用户就会在推荐系统总停留较长的时间；反之，如果需要用户几次刷新之后，才能得到自己想要的物品，那么用户下一次将不会在信任推荐效果，导致在推荐系统中停留的时间较短。</p>
<p>在工业界中，排序这一部分分为三个步骤：</p>
<p><img src="/2019/06/01/个性化推荐算法实践第07章综述学习排序/1564992495527.png" alt="1564992495527"></p>
<p>（1）prerank（预排序）</p>
<p>也就是排序之前的部分，由于排序的模型由浅层模型切换到深层模型的时候，耗时在不停的增加。比如之前召回可以允许有5000个物品去做浅层模型，比如说逻辑回归，就是训练出一组参数，那么整体的打分过程耗时很短。但是，如果当排序模型切换到深层模型，比如说DNN，那么整体需要请求一次新的深度学习的服务，那么这5000个item去请求的时间显然是不能承受的。所以要先有一个<strong>粗排</strong>。这个粗排会将这5000个召回的物品进行第一次排序，将候选集缩小到一定范围之内。这样使排序模型的总处理时间满足系统的性能要求。粗排往往以一些简单的规则为主，比如说使用<strong>后验CTR</strong>或者说对于新的物品使用入库时的<strong>预估CTR</strong>等等。</p>
<p>（2）Rank（主排序）</p>
<p>主排序部分就是重点部分，现在业界比较流行的还有一次重排（ReRank）。</p>
<p>主排序模型的分类：</p>
<p>a. 单一的浅层模型：浅层模型是相较于深度模型而言的，浅层模型的代表有LR(逻辑回归)、FM。</p>
<p>这一类模型在学习排序初期是非常受欢迎的，因为模型线上处理时间较短，所以它支持特征的维度就会非常的高。但是也存在很多问题：比如像LR模型，需要研发者具有很强的样本筛选以及特征处理能力，这个包含像特征的归一化、离散化、特征的组合等等。</p>
<p>所以，后期发展了浅层模型的组合。</p>
<p>b. 浅层模型的组合</p>
<p>这里比较著名的树模型的组合：GBDT组合，LR+GBDT等等。这一类模型不需要特征的归一化、离散化，能够较强的发现特征之间的规律，所以相较于单一的浅层模型具有一定的优势。</p>
<p>c. 深度学习模型</p>
<p>随着深度学习在工业界应用的不断成熟，以及像tensorflow等深度学习框架的开源，现在工业界大部分的主排序模型都已经切换到了深度学习模型。</p>
<p>（3）Rerank（重排序）</p>
<p>这个重排是将主排序的结果再放入一个类似于session model或者说是强化学习的一个模型里面去进行一个重排序，这种主要是突出了用户最近几次行为的session特征，将与最近几次session内用户行为相近的item给优先的展示，以便获取用户行为的连续性。</p>
<blockquote>
<h1 id="KDD2018-电商搜索场景中的强化排序学习：形式化、理论分析以及应用http-www-sohu-com-a-244970525-129720"><a href="#KDD2018-电商搜索场景中的强化排序学习：形式化、理论分析以及应用http-www-sohu-com-a-244970525-129720" class="headerlink" title="KDD2018 | 电商搜索场景中的强化排序学习：形式化、理论分析以及应用http://www.sohu.com/a/244970525_129720"></a>KDD2018 | 电商搜索场景中的强化排序学习：形式化、理论分析以及应用<a href="http://www.sohu.com/a/244970525_129720" target="_blank" rel="noopener">http://www.sohu.com/a/244970525_129720</a></h1></blockquote>
<p>由于单一item在重排模型的耗时要比主模型长很多，所以重排部分只是会影响主排序头部的一些结果，比如说top 50 的结果去进行一个重排。那么既然是这样的话，可以看到，最能影响结果的还是主排序模型。</p>
<h3 id="三、工业界推荐系统中排序架构解析"><a href="#三、工业界推荐系统中排序架构解析" class="headerlink" title="三、工业界推荐系统中排序架构解析"></a>三、工业界推荐系统中排序架构解析</h3><p>工业界中排序是如何落地的。</p>
<p><img src="/2019/06/01/个性化推荐算法实践第07章综述学习排序/1565100075461.png" alt="1565100075461"></p>
<p>算法的后端主流程是：召回之后排序。</p>
<p>召回完item之后，我们将item集合传给排序部分，排序部分会调用打分框架，得到每一个item在当前上下文下，对当前user的一个得分，进而根据得分决定展现顺序。</p>
<p>下面看一下打分框架内部的构成：</p>
<p>首先会将每一个item以及user去提取特征，注意这里提取的特征要与离线训练模型的特征保持一致。提取完特征之后，我们向排序服务发出请求，排序服务会返回给我们一个得分，推荐引擎会基于此得分完成排序。经过简单的策略调整之后，展现给用户。</p>
<p>这里需要特别注意的是，排序服务与离线训练好的排序模型之间的通信。</p>
<p>如果是单一的浅层模型，像LR，那么可以直接将训练好的模型参数存入内存。当排序服务需要对外提供服务的时候，直接加载内存中模型的参数即可。像FM以及GBDT等等，我们只需要离线训练好模型，将模型实例化到硬盘当中。在在线服务当中，由于这些模型都有相应的库函数，他们提供了模型的加载以及模型对外预测等一系列接口，所以便可以完成打分。</p>
<p>但是，对于像深度学习的话，我们在训练完成之后，我们还需要提供一个深度学习的服务供排序服务调用。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第06章个性化召回算法总结与回顾/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnFrank">
      <meta itemprop="description" content="你若晴天，我便安好。">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnFrank's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/01/个性化推荐算法实践第06章个性化召回算法总结与回顾/" itemprop="url">
                  个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-01 16:03:11" itemprop="dateCreated datePublished" datetime="2019-06-01T16:03:11+08:00">2019-06-01</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-18 14:21:58" itemprop="dateModified" datetime="2019-12-18T14:21:58+08:00">2019-12-18</time>
              
            
          </span>


          

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/推荐算法/" itemprop="url" rel="index"><span itemprop="name">推荐算法</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/01/个性化推荐算法实践第06章个性化召回算法总结与回顾/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2019/06/01/个性化推荐算法实践第06章个性化召回算法总结与回顾/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍"><a href="#个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍" class="headerlink" title="个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍"></a>个性化推荐算法实践第06章个性化召回算法总结与评估方法的介绍</h1><p>本章节重点总结前面几章节介绍过的个性化召回算法。并介绍如何从离线与在线两个大方面评估新增一种个性化召回算法时的收益。</p>
<h3 id="一、个性化召回算法的总结"><a href="#一、个性化召回算法的总结" class="headerlink" title="一、个性化召回算法的总结"></a>一、个性化召回算法的总结</h3><p>这里会将之前介绍过的几种算法进行归类，并简短介绍每一种个性化召回算法的核心原理；同时演示工业界中多种召回算法共存的架构。</p>
<p>下面看一下之前讲过的个性化召回算法的分类：</p>
<ol>
<li>基于邻域的：CF、LFM、基于图的推荐personal rank</li>
</ol>
<p>item-CF是item根据user的贡献，得到item的相似度矩阵，用户根据用户点击过的item的相似item来完成推荐。</p>
<p>user-CF是user根据item的贡献，得到user的相似度矩阵，用户将根据相似用户点击过的物品完成自己的推荐。</p>
<p>LFM是根据user-item矩阵，将矩阵分解，从而得到user与item的隐向量，将两个向量点乘得到的数值取top k 便完成了推荐。</p>
<p>psersonal rank是根据user与item的二分图，在这个二分图之间随机游走，便得到物品对固定用户的倾向度，把这个倾向度得分叫做PR值。那么取PR值的top k也就完成了用户的推荐。</p>
<ol>
<li>基于内容的：content-based算法</li>
</ol>
<p>content-base算法的主要流程是：</p>
<p>首先将item进行刻画，然后将user进行刻画，然后将user的刻画与item的刻画在线上推荐的时候串联起来。</p>
<ol>
<li>基于神经网络(Neural network)的：item2vec</li>
</ol>
<p>item2vec首先根据用户的行为得到由item得到的句柄，根据训练语料得到item embedding的向量，得到这个向量之后就能得到item的相似度矩阵。从而根据用户的历史点击推荐相似的item给用户，也就完成了推荐。</p>
<p>下面看一下工业界中推荐系统中多种召回并存的架构：</p>
<p>后端算法的核心逻辑：</p>
<p><img src="/2019/06/01/个性化推荐算法实践第06章个性化召回算法总结与回顾/1564988526824.png" alt="1564988526824"></p>
<p>首先是召回，召回之后是排序，排序之后是策略调整，然后就将结果返回给web层。</p>
<p>接下来看一下，具体在召回阶段是如何多种算法并存的。</p>
<p><img src="/2019/06/01/个性化推荐算法实践第06章个性化召回算法总结与回顾/1564988665653.png" alt="1564988665653"></p>
<p>比如这里的算法A，召回了两个item，分别是a、b；算法B召回了3个item，分别是a、c、d；同理算法C召回了4个item，分别是e、f、d、c。</p>
<p>那么每一种算法召回的数目是如何确定的呢？</p>
<p>这里有两种形式：</p>
<p>形式一：为了满足rank阶段的性能要求，这里指定召回阶段召回的数目，比如说50个，那么各种算法根据以往的表现来平分这50个，每一个算法有一个比例，比如说算法A是0.2，算法B是0.3，算法C是0.5。这样每一个算法也就有了自己召回的上限。 </p>
<p>形式二：rank阶段毫无性能压力，我们给算法A写了多少个推荐都能全部召回，其余算法也是相同的处理。</p>
<p>在召回完成之后，我们需要进行一个合并。合并完成之后，我们得到item a~f，将重复召回的进行去重，但是也会给item a标记上它同时是属于算法A和算法B召回的。召回完成之后，这些item进入排序阶段。</p>
<h3 id="二、个性化召回算法的评价"><a href="#二、个性化召回算法的评价" class="headerlink" title="二、个性化召回算法的评价"></a>二、个性化召回算法的评价</h3><p>在现有的个性化召回体系下，如果要新增一种个性化召回算法，需要知道这种个性化召回算法会对系统造成怎样的影响，是正向收益还是负向收益。所以需要从离线和在线两个方面对个性化召回算法进行评价。</p>
<p>离线评价准入：</p>
<p>也就是说，在我们新增一种个性化召回算法的时候，我们离线选取了一部分训练文件来训练个性化召回算法的模型。我们根据这个模型得到了一些推荐结果，同时有必要保留一些测试集。在测试集上评价推荐结果的可靠程度。这个可靠程度首先是要有一个预期，这个算法会给线上带来正向还是负向的收益。</p>
<p>当然，最终的结果仍然需要在线上生产环境中去评价真实的受益，也就是做A/B test。</p>
<p>如何在离线进行评价的？</p>
<p>评价方法：评测新增算法推荐结果在测试集上的表现。</p>
<p>这里用一个例子来具体说明：</p>
<p><img src="/2019/06/01/个性化推荐算法实践第06章个性化召回算法总结与回顾/1564989385352.png" alt="1564989385352"></p>
<p>如果新增了某种个性化召回算法，对于user A我们给出了推荐结果a、b、c。恰巧这里我们获得了user A在测试集上的展现数据，就是a、b、c、m，那么在这里我们发现有3个是重合的，也就是a、b、c，那么这3个就是分母，如果我们在得到了用户A在测试集上的点击数据，这个点击数据恰好是a、c。我们发现这里的推荐结果是a、b、c是分母，然后有两个被点击了，那么a、c就是分子，最后的点击率就是 2/3。</p>
<p>如果这个数据是高于基线的点击率的话，那么就可以将这种推荐算法放到线上做A/B test(A/B测试)。</p>
<p>当然了，我们知道线下的评价结果与线上真实环境中的结果是有差异的，但是这种方式是能够给我们一个最基础的、直观的评判，是否可以准入到线上。</p>
<p>这里简单解释一下什么是测试集？</p>
<p>举例：如果我们要使用itemCF这种个性化召回算法，那么我们首先需要计算item的相似度矩阵，我们这里以过去一周的用户的真实的展现与点击数据为依据来训练这个相似度矩阵。我们只使用周一到周五的数据来训练，周六、周日的数据便是这里的测试集。</p>
<p>在线评价收益：A/B test</p>
<p>线上的评价分为两步：</p>
<p>（1）定义指标：</p>
<p>这里需要根据不同的情况，比如说在信息流场景下，我们最关心的就是点击率，平均阅读时长等等指标；但是在电商系统中，我们可能更加关注的是转化率及总的交易额度。</p>
<p>总之要根据自己的产品，来找到最能够评价产品的核心指标。</p>
<p>（2）生产环境A/B test：</p>
<p>往往采用以划分user id尾号的形式，比如说分出1%的流量在原来的个性化召回体系框架上增加要实验的个性化召回算法。实验几天之后，与基线去比较核心指标的优劣。如果收益是正向的，我们就保留。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第05章基于内容的推荐方法ContentBased/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnFrank">
      <meta itemprop="description" content="你若晴天，我便安好。">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnFrank's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/01/个性化推荐算法实践第05章基于内容的推荐方法ContentBased/" itemprop="url">
                  个性化推荐算法实践第05章基于内容的推荐方法ContentBased
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-01 15:03:11" itemprop="dateCreated datePublished" datetime="2019-06-01T15:03:11+08:00">2019-06-01</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-18 14:21:46" itemprop="dateModified" datetime="2019-12-18T14:21:46+08:00">2019-12-18</time>
              
            
          </span>


          

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/推荐算法/" itemprop="url" rel="index"><span itemprop="name">推荐算法</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/01/个性化推荐算法实践第05章基于内容的推荐方法ContentBased/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2019/06/01/个性化推荐算法实践第05章基于内容的推荐方法ContentBased/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="个性化推荐算法实践第05章基于内容的推荐方法Content-Based"><a href="#个性化推荐算法实践第05章基于内容的推荐方法Content-Based" class="headerlink" title="个性化推荐算法实践第05章基于内容的推荐方法Content Based"></a>个性化推荐算法实践第05章基于内容的推荐方法Content Based</h1><p>本章节重点介绍一种基于内容的推荐方法content based。从content based算法的背景与主体流程进行介绍。并代码实战content based算法。</p>
<h1 id="第一部分：基于内容的推荐的理论知识部分"><a href="#第一部分：基于内容的推荐的理论知识部分" class="headerlink" title="第一部分：基于内容的推荐的理论知识部分"></a>第一部分：基于内容的推荐的理论知识部分</h1><h3 id="一、个性化召回算法Content-based背景介绍"><a href="#一、个性化召回算法Content-based背景介绍" class="headerlink" title="一、个性化召回算法Content based背景介绍"></a>一、个性化召回算法Content based背景介绍</h3><p>之前博文讲到的CF、LFM、Personal Rank都同属于基于领域的推荐。item2vec属于基于深度学习的推荐。</p>
<p>基于内容推荐不同于之前讲过的任何一种个性化召回算法，属于独立的分支。我们有必要去了解该算法出现的背景。</p>
<ol>
<li>思路简单，可解释性强</li>
</ol>
<p>任何一个推荐系统的初衷，都是推荐出用户喜欢的item。</p>
<p>基于内容的推荐，恰恰是根据用户的喜好之后，给予用户喜欢的物品。</p>
<p>eg：某一个用户经常点击体育类的新闻，那么在这个用户下一次访问这个网站系统的时候，自然而然的给用户推荐体育类型的新闻。那么对于推荐结果可解释性非常的强。</p>
<ol>
<li>用户推荐的独立性</li>
</ol>
<p>基于内容的推荐，推荐的结果只与该用户本身的行为有关系，其余用户的行为是影响不到该用户的推荐结果的。但是，联想一下，之前提到的无论是CF还是LFM、personal rank以及item2vec，其余用户的行为都会一定程度上，或多或少的干预到最后的推荐结果。</p>
<ol>
<li>问世较早，流行度高</li>
</ol>
<p>由于基于内容推荐思路的极简性，可解释性，所以它出现的非常早；并且，无论是在工业界，还是研究界，都最为一种基础的算法，流行度非常的高。</p>
<p>但是，任何事物都是有两面性的，基于内容的推荐不是说是完美的，它同样有一些非常明显的缺点。</p>
<p>（1）对于推荐的扩展性较差：也就是说，如果一个用户之前经常访问体育类型的新闻，那么在之后的推荐之中，倾向于在体育范围内不断地挖掘；<strong>很难完成跨领域的物品推荐。</strong></p>
<p>（2）<strong>需要积累一定量的用户的行为</strong>，才能够完成基于内容的推荐。</p>
<h3 id="二、Content-based算法的主体流程介绍"><a href="#二、Content-based算法的主体流程介绍" class="headerlink" title="二、Content-based算法的主体流程介绍"></a>二、Content-based算法的主体流程介绍</h3><p>实际上该算法的主体流程大部分不属于个性化推荐的范畴，应该从属于NLP或者用户画像的范畴。只有极小部分属于个性化推荐算法实战的范畴。</p>
<ol>
<li>item profile：对item的刻画</li>
</ol>
<p>针对于基于内容的推荐下，对item的刻画大体可以分为两大类：（1）关键词刻画；（2）类别的刻画。</p>
<p>比如，在信息流场景下，我们需要刻画出这篇新闻属于财经还是娱乐；那么在电商场景下也是一样的，我们需要刻画出这个物品它属于图书还是说属于母婴，具体的关键词上也会有这个图书是数据机器学习的还是人文情感的，这个物品是参与满减的，还是参与包邮的等等。</p>
<p>第一步完成内容的物品刻画之后，第二步需要对用户进行刻画。</p>
<ol>
<li>user profile</li>
</ol>
<p>传统范畴的用户画像是比较宽泛的，它不仅包含了用户的动态特征，还包含了它的一些静态特征。</p>
<p>而我们用在基于内容推荐里的更多的是聚焦在用户的长期、短期行为，进而通过行为的分析将用户感兴趣的topic、或者用户感兴趣的类别给予刻画。</p>
<p>那么，有了item的刻画，有了user的刻画，第三步就是在线上完成个性化推荐的过程。</p>
<ol>
<li>online recommendation</li>
</ol>
<p>给用户推荐他最感兴趣的一些topic，或者说一些类别。</p>
<p>假设某个用户经常点击明星新闻，当用户访问系统的时候，我们应该明星最新的新闻最及时的推荐给用户，这样点击率自然很高。</p>
<p>那么经过这三步流程的分析，可以发现：实际上，前两步更多的同属于NLP或者说是用户画像的范畴，第三步更多的是我们个性化推荐的内容实战范畴。</p>
<p>下面将每一部分的技术要点进行解析：</p>
<p>（1）item profile </p>
<p>a. Topic finding（Topic 发现）：首先要选定特征，这里的特征是title和内容主体的分词，那么得到词语的分词之后，针对于topic的发掘采用命名实体识别的方式。这个命名实体识别的方式可以去匹配关键词词表，那么得到了关键词之后，我们需要对这些关键词进行一定的排名，那么将排名最高的top 3 或者top 5给 item 完成label。</p>
<p>至于这里的排名，会使用一些算法和规则，算法诸如：TF-IDF，规则是基于自己的场景总结出来的修正错误case的一些规则。</p>
<p>b. Genre Classify（类别的划分）：首先选定好特征，这里同样是利用一些文本信息，比如说title，分词（正文中所有的去过标点，去过停用词）得到的词向量，这里词向量在浅层模型中可以直接one-hot编码，在深层模型中，首先可以先进行一个embedding，这里使用的分类模型主要是像LR、GBDT、CNN等等。</p>
<p>分类器的使用，是使用多种分类器，分别占不同的权重，然后对结果进行一个线性的加权，从而得到正确的分类。</p>
<p>以上是针对于文档的topic 发掘或者说是类别的分类进行的叙述。那么对于短视频，实际上现在引入了一些更多的特征，比如关键帧所对应图像的分类识别，以及音频所对应的语音识别后，文字的处理等一些有意义的尝试。</p>
<p>（2）user profile</p>
<p>a. Genre/Topic（类别的划分）（Topic 发现）： </p>
<p>一个层面是用户对哪些<strong>种类</strong>的新闻或者是物品感兴趣；另一个层面是对哪些<strong>关键词</strong>感兴趣。</p>
<p>现在多是基于统计的方式，业界也在做一些尝试，比如引入分类器等等。</p>
<p>b. Time Decay：</p>
<p>注意时间衰减，不同时期的行为所占权重是不同的。</p>
<p>最终，针对于某个用户最想想刻画得到的结果是，用户对于不同种类item的倾向性，eg,比如这个用户对于娱乐倾向性是0.7，对于财经的倾向性是0.3。</p>
<p>（3）线上推荐部分</p>
<p>a. find top k Genre/Topic ；</p>
<p>b. get the best n item for fix genre/topic</p>
<p>第一步，基于用户的刻画，找到用户最感兴趣的top k个分类，由于这top k个分类都是带有权重的，那么第二步，相应给每个分类得到n个最好的分类下的item.</p>
<p>这里有两点说明，</p>
<p>a. 由于权重的不同，从种类下召回的数目是不同的。比如某人对财经感兴趣，对娱乐也感兴趣，但是对娱乐感兴趣的程度更高。那么对娱乐召回的数目就要多于财经召回的数目。</p>
<p>b. best的理解：这里的best对于不是新item来讲，就是它的后验CTR；如果是新的item，在入库的时候，都会给出一个预估的CTR，那么就用这个预估的CTR来作为衡量的标准。</p>
<p>第二部分：基于内容的推荐的代码实战部分</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnFrank">
      <meta itemprop="description" content="你若晴天，我便安好。">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnFrank's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/" itemprop="url">
                  个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-01 14:03:11" itemprop="dateCreated datePublished" datetime="2019-06-01T14:03:11+08:00">2019-06-01</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-18 14:21:37" itemprop="dateModified" datetime="2019-12-18T14:21:37+08:00">2019-12-18</time>
              
            
          </span>


          

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/推荐算法/" itemprop="url" rel="index"><span itemprop="name">推荐算法</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec"><a href="#个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec" class="headerlink" title="个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec"></a>个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec</h1><p>本章节重点介绍一种基于深度学习的个性化召回算法item2vec。从item2vec的背景与物理意义以及算法的主流程进行介绍。并对该算法依赖的模型word2vec数学原理进行浅析。最后结合公开数据集代码实战item2vec算法。</p>
<p>本章节重点介绍基于深度学习的个性化召回算法item2vec。分为两部分：第一部分，item2vec背景和物理意义，论文依据，算法流程，以及item2vec的依据的模型word2vec的数学原理等等理论部分解析。第二部分，根据示例数据，item2vec的算法流程，编程实战，训练模型得到item对应的向量完成推荐并分析推荐结果。</p>
<h2 id="一、基于深度学习的个性化召回算法item2vec"><a href="#一、基于深度学习的个性化召回算法item2vec" class="headerlink" title="一、基于深度学习的个性化召回算法item2vec"></a>一、基于深度学习的个性化召回算法item2vec</h2><h3 id="1、个性化召回算法item2vec背景与物理意义"><a href="#1、个性化召回算法item2vec背景与物理意义" class="headerlink" title="1、个性化召回算法item2vec背景与物理意义"></a>1、个性化召回算法item2vec背景与物理意义</h3><h4 id="个性化召回算法item2vec背景"><a href="#个性化召回算法item2vec背景" class="headerlink" title="个性化召回算法item2vec背景"></a>个性化召回算法item2vec背景</h4><ol>
<li>Item2item的推荐方式效果显著：</li>
</ol>
<p>很多场景下item2item的推荐方式要优于user2item；</p>
<p>item2item的推荐方式：在获取item相似度矩阵之后，根据用户的最近的行为，根据行为过的item找到相似的item，完成推荐，如itemCF。</p>
<p>user2item：根据用户的基本属性和历史行为等基于一定的模型，算出最可能喜欢的item列表写在KV存储中；当用户访问系统的时候，将这些item列表推荐给用户，像userCF、LFM、personal rank算法等都是这种方式。</p>
<ol>
<li>NN model（神经网络）模型的特征抽象能力</li>
</ol>
<p>神经网络的特征抽象能力是要比浅层的模型特征抽象能力更强，主要有两方面原因：</p>
<p>（1）输入层与隐含层，隐层与输入层之间，所有的网络都是全连接的网络；</p>
<p>（2）激活函数的去线性化；</p>
<p>基于上述，基于神经网络的item2item的个性化召回算法item2vec也就在这个大背景下产生了。</p>
<p>3、依据的算法论文：<a href="http://www.researchgate.net/publication/298205072_Item2Vec_Neural_Item_Embedding_for_Collaborative_Filtering?ev=auth_pub" target="_blank" rel="noopener">Item2Vec: Neural Item Embedding for Collaborative Filtering</a></p>
<p>核心内容1：论文首先介绍了item2vec落地场景是类似于相关推荐的场景。也就是说用户点击了某item a APP,那么推荐一款类似的APP给用户。</p>
<p>核心内容2：介绍了item2vec所选的model，也就是word2vec算法原理，文中采用了负采样的训练方法进行介绍，因为我们知道word2vec还有一种也就所说的哈夫曼树的方式进行训练。那么我们后面也用负采样的方式进行介绍word2vec的算法原理。</p>
<p>核心内容3：论文抽象了算法的整体流程。</p>
<p>核心内容4：论文将给出了与之前流行的item2item算法进行结果对比。</p>
<h4 id="个性化召回算法item2vec物理意义"><a href="#个性化召回算法item2vec物理意义" class="headerlink" title="个性化召回算法item2vec物理意义"></a>个性化召回算法item2vec物理意义</h4><p>在介绍item2item之前，先介绍一下原型word2vec。</p>
<ol>
<li>word2vec</li>
</ol>
<p>根据所提供的语料，语料可以想象成一段一段的文字，将语料中的词embedding成词向量，embedding成词向量之间的距离远近可以表示成词与词之间的远近。（word2vec原理中详细介绍怎么样做到可以表征词与词距离的远近）</p>
<ol>
<li>item2item</li>
</ol>
<p>（1）将用户行为序列转换成item组成的句子。</p>
<p>解释：在系统中，无论是用户的评分系统，还是信息流场景下用户的浏览行为，或者是电商场景下用户的购买行为。在某一天内，用户会进行一系列的行为，那么将这一系列的行为抽象出来。每一个用户组成的item与item之间的这种序列的连接关系就变成了之前所说的文字组成的一段一段的句子，那么这里的每一个word相当于这里的item。</p>
<p>（2）模仿word2vec训练word embedding 过程，将item embedding。</p>
<p>word embedding的过程只需要提供语料，也就是一段一段的文字，那么训练得到的word embedding可以表征词语义的远近，那么同样希望表征item之间内涵的远近。</p>
<p>所以，可以将第一步构成的item语料放到word2vec中，也能够完成item embedding。embedding完成的向量同样可以表示item之间的隐语义的远近，也就是说，可以表示item之间的相似性。</p>
<p>以上就是item2vec的物理意义。</p>
<h4 id="个性化召回算法item2vec缺陷"><a href="#个性化召回算法item2vec缺陷" class="headerlink" title="个性化召回算法item2vec缺陷"></a>个性化召回算法item2vec缺陷</h4><p>（1）用户的行为序列时序性缺失：</p>
<p>在介绍物理意义的时候，说过将用户的行为转化成由item组成的句子，这里句子之间词与词之间的顺序与按照用户行为顺序进行排列和不按照用户行为顺序进行排列的结果是几乎一致的。</p>
<p>也就是用户的行为顺序性，模型是丢失的。</p>
<p>（2）用户行为序列中的item强度是无区分性的：</p>
<p>这里比如说，在信息流场景中，观看短视频的50%或者80%或者100%，在用item组成的句子当中，同样都是出现一次的，而不是说观看100%就会出现2次。</p>
<p>再如，在电商场景中，可能你购买一件商品，或者说你加了购物车，都会出现一次；而不会说，你购买了就会出现两次。加了购物车，在句子当中出现一次。在item cf算法中，item 相似度矩阵计算 ，当时引入了用户行为item的总数，用户行为item的时间这两个特征，来分别将公式进行升级。所以在item2vec算法中，是没有这个消息的。所以是item2vec的缺陷之一。</p>
<h3 id="2、item2vec算法应用的主流程"><a href="#2、item2vec算法应用的主流程" class="headerlink" title="2、item2vec算法应用的主流程"></a>2、item2vec算法应用的主流程</h3><p>（1）从log中抽取用户行为序列</p>
<p>按照每个用户“天”级别，行为过的item，构成一个完整的句子，这里的行为根据不同的推荐系统所指的不同。比如：在信息流场景下，用户点击就可以认为是行为；那么在评分系统中，我们可能需要评分大于几分；在电商系统中，可能希望用户购买，得到用户行为序列所构成的item句子。</p>
<p>（2）将用户序列当做语料训练word2vec得到item embedding</p>
<p>在训练过程中，word2vec代码不需要书写，但是有很多参数是需要我们具体设定的。</p>
<p>（3）得到item sim关系用于推荐</p>
<p>根据第二个步骤中得到每一个item所embedding的向量，可以计算每一个item最相似的top k个，然后将相似度矩阵离线写入到KV当中，当用户访问我们的推荐系统的时候，用户点击了哪些item，推出这些item所最相似的top k个给用户，就完成了推荐。</p>
<p>eg：</p>
<p><img src="/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/1564895184811.png" alt="1564895184811"></p>
<p>1、首先第一步：这是我们从推荐系统log中获得的，也就是说User A行为过item a、item b、item d，User B行为过item a、item c，User C行为过item b、item e。</p>
<p>2、继而我们需要将这些转化成句子。句子1就是a、b、d。句子2就是a、c。句子3就是b、e。这里我们已经没有了user之间的关系，只留下item组成的句子。</p>
<p>3、将句子放入到放入word2vec模型，这里就是一个输入（word2vec模型是一个三层的神经网络：输入层、隐含层、输出层，具体每一层的作用，公式原理后面详细介绍。）</p>
<p>经过word2vec模型的训练，会得到每一个item对应的embedding层向量。这里只写了item a [0.1,0.2,0.14……..0.3] ,item b [0.4,0.6,0.14……..0.3]。item c，item d,item e也是可以得到的。</p>
<p>4、得到item向量之后，就得到item的sim关系。基于item sim的关系，我们便完成了用户的推荐。</p>
<h3 id="3、item2vec依赖模型word2vec介绍（连续词袋模型和跳字模型-skip-gram-）"><a href="#3、item2vec依赖模型word2vec介绍（连续词袋模型和跳字模型-skip-gram-）" class="headerlink" title="3、item2vec依赖模型word2vec介绍（连续词袋模型和跳字模型(skip-gram)）"></a>3、item2vec依赖模型word2vec介绍（连续词袋模型和跳字模型(skip-gram)）</h3><p>item2vec依赖模型word2vec的数学原理详细介绍</p>
<p>word2vec model</p>
<p>是围绕负采样的训练方法进行介绍，因为我们知道word2vec还有一种也就所说的哈夫曼树的方式进行训练（这里就不做介绍了）。</p>
<p><strong>word2vec有两种形式：</strong>连续词袋模型和跳字模型(skip-gram)</p>
<ol>
<li><strong>CBOW（continuous bag of words）连续词袋模型</strong></li>
<li><strong>skip gram()</strong>跳字模型</li>
</ol>
<h2 id="二、item2vec依赖模型word2vec之CBOW数学原理介绍"><a href="#二、item2vec依赖模型word2vec之CBOW数学原理介绍" class="headerlink" title="二、item2vec依赖模型word2vec之CBOW数学原理介绍"></a>二、item2vec依赖模型word2vec之CBOW数学原理介绍</h2><h3 id="1、CBOW网络结构"><a href="#1、CBOW网络结构" class="headerlink" title="1、CBOW网络结构"></a>1、CBOW网络结构</h3><p><img src="https://img-blog.csdnimg.cn/20190331193124458.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY0MDU4Mw==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>网络分为三层：输入层、投影层、输出层。</p>
<p>输入层：上下文；比如说这里有五个词W(t+2)、W(t+1)、W(t)、W(t-1)、W(t-2)。这里我们需要输入的训练数据是W(t)的上下文，即是W(t+2)、W(t+1)、W(t-1)、W(t-2)。 </p>
<p>投影层：将上下文的词输入的向量加起来；因为给每个词都初始化了一个向量。比如说我们需要让它的长度是16。那么刚开始的时候，这些词W(t+2)、W(t+1)、W(t-1)、W(t-2)，再包括了W(t)都有一个初始化的向量。把这些向量加到了投影层。</p>
<p>输出层：当前词；</p>
<p>投影层与输出层之间是全连接，如果输出的这个词是这里的W(t)的话，希望最大化的就是这个概率。而除了W(t)，词典（词典指训练语料包含的所有的词）中所有的其他的词，其余词的概率我们都希望最小那个概率。</p>
<p>这就引申出一个问题，如果其余词都是负样本的话，负样本太多，训练太慢。</p>
<p>所以，采用负采样（后面介绍）的方法。这里我们先不要关心负采样的具体是怎么做的，后面我们会详细的介绍</p>
<p>也就是说，有了正样本W(t)与它的上下文，负样本是通过负采样得到的，我们希望最小化负样本所对应的模型的输出。这里实际上，最后一层的输出不仅仅是只有W(t)，实际上是有字典中每一个词。因为每一个词都有一个与输出层之间的全连接，这也是模型需要训练的参数之一。</p>
<p>然后，模型需要得到的是每一个词，我们初始化的那个向量，让它训练，根据我们传入的训练样本，训练完成的向量，就是最后模型输出。我们就是依赖每一个词对应的向量，完成item相似度矩阵之间的运算。</p>
<p>与传统的监督模型有所不同，传统的模型在训练完成之后，需要将它保存，然后对外提供服务，当我们传入真实样本的时候，希望得到一个输出值；而这里不是。下面我们来看下一中形式。</p>
<h3 id="2、CBOW的数学公式"><a href="#2、CBOW的数学公式" class="headerlink" title="2、CBOW的数学公式"></a>2、CBOW的数学公式</h3><ol>
<li>问题抽象</li>
</ol>
<script type="math/tex; mode=display">
g(w)=\prod_{u \in w \cup N E G(w)} p(u | \operatorname{Context}(w))</script><p>上式是想最大化的条件概率函数。</p>
<p>下面我来解释一下这个公式：</p>
<p>$\text { Context }(w)$：某一词W(t)的上下文的词，已知上下文，想预测中间词。</p>
<p>可能有的同学对于这个训练样本还不是很了解，下面我举一个最简单的例子，有一句话是我是中国人，那么经过分词之后呢，变成了（我    是    中国    人）这么四个字也就是w1,w2,w3,w4，如果我们选窗口是1的话，在这里第1组的训练样本变成了（我   是     中国  人），其中这个”是”就是公式这里的w，它的上下文呢，w(t+1)呢是’中国’，w（t-1）是”我”这个词。</p>
<p>我们知道了”中国”，”我”这个词。如果u是W的话（这个u是公式中W”是”的话），则是需要最大化条件概率；如果是W的负样本（NEG(w)）（除了”是”，以外的其他词），我们想要把这个概率最小化，最小化这个条件概率$p(u | \text { Context }(w))$，也就是最大化$1-p(u | \text { Context }(w))$条件概率。那么无论是u是w,或者u是我们选择的负采样的负样本都能统一起来了，实际上这个式子由两部分组成。</p>
<p>这里的条件概率是指：</p>
<script type="math/tex; mode=display">
p(u | \text { Context }(w))=\sigma\left(X_{w}^{T} \theta^{u}\right)^{L^{w}(u)}\left(1-\sigma\left(X_{w}^{T} \theta^{u}\right)\right)^{\left(1-L^{w}(u)\right)}</script><p>这个式子由两部分组成：</p>
<p>（1）当u=w时，也就是说label $L^{w}(u)$=1，起作用的是前一部分，因为后面的指数变为零次幂（$1-L^{w}(u)$=0），零次幂的话，后面部分就等于1了。起作用的是前一部分，我们是想最大化的正样本的概率。解释一下这里的$X_{w}^{T}$和$\theta^{u}$：</p>
<p>$X_{w}^{T}$：CBOW的时候，投影层是将w对应的上下文的词向量加和。也就是我们这里举例子的”我”和”中国”对应的向量加和，便是这里的$X_{w}^{T}$。</p>
<p>$\theta^{u}$：隐含层（投影层）和输出层对应的词为u的时候，它们之间的全连接。</p>
<p>所以，想通过模型训练，让这两个参数$X_{w}^{T}$和$\theta^{u}$相乘得到的结果为1。（按照我们的距离就是16维的横竖向量相乘应该是一个常数，这里我们希望通过训练，让他最终相乘得到的数字是1）</p>
<p>（2）负采样部分，也就是后面那一部分，我们这里负采样选取的负样本，希望这一部分是0。也就是希望这一部分$1-\sigma\left(X_{w}^{T} \theta^{u}\right)$为1，也就是最大化$1-\sigma\left(X_{w}^{T} \theta^{u}\right)$这一部分。</p>
<ol>
<li>损失函数</li>
</ol>
<p>$\operatorname{Loss}=\log (g(w))$</p>
<p>这里采用对数损失函数，之前LFM采用的是平方损失函数。</p>
<p>公式带入：</p>
<script type="math/tex; mode=display">
\text {Loss}=\sum\left(L^{w}(u) * \log \left(\sigma\left(x_{w}^{T} \theta^{u}\right)\right)+\left(1-L^{w}(u)\right) * \log \left(1-\sigma\left(x_{w}^{T} \theta^{u}\right)\right)\right)</script><p>取对数之后，简单讲解一下，之前的累乘，由于我们取对数，就变成了累加。而之前里面是两部分相乘，，由于我们取对数，就变成了两部分相加。而且对数里面的幂次，可以直接变成了这里的系数。</p>
<p>对$X_{w}^{T}$和$\theta^{u}$求偏导，求完偏导之后，使用梯度上升法不断迭代这里我们这里需要的参数$X_{w}^{T}$和$\theta^{u}$，继而便能够去迭代每一个词对应的词向量。</p>
<ol>
<li>梯度：</li>
</ol>
<p>$\frac{\partial L o s s}{\partial \theta^{u}}=\left(L^{w}(u)-\delta\left(x_{w}^{T} \theta^{u}\right)\right) x_{w}$               $\theta^{u}=\theta^{u}+\alpha * \frac{\partial L o s s}{\partial \theta^{u}}$</p>
<p>$\frac{\partial L o s s}{\partial x_{w}}=\left(L^{w}(u)-\delta\left(x_{w}^{T} \theta^{u}\right)\right) \theta^{u}$        $v(w_{context}) = v(w_{context}) + \sum_{u \in w \cup NEG(w)}\alpha *\frac{\partial L o s s}{\partial x_{w}}$</p>
<p>梯度公式1：</p>
<p>首先对$\theta^{u}$求偏导得到了上述的结果。我们先不在此处讲解推导过程。推导过程和排序部分的逻辑回归的部分完全一样。（推导过程也并不是很复杂，只需要记住链式求导法以及加上激活函数。sigmod函数的导数是等于他的本身乘以（1-他的本身），即是s(x)*(1-s(x))）。这两个小技巧比较容易得到。</p>
<p>公式中，$L^{w}(u)$是label，值是1或者0，如果当这里的词是中心词的时候就是1，如果这里的词是负采样中选取的负样本，那么就是0。$\delta\left(x_{w}^{T} \theta^{u}\right)$这个是模型的输出,实际上是投影层对应的向量$x_{w}^{T}$，并且乘以$\theta^{u}$向量得到的一个值，我们在用激活函数激活一下，也得到了一个零一之间的值。这里的$x_{w}$便是投影层上下文向量的加和。</p>
<p>由于我们之前看到的损失函数里$x_{w}$与$\theta^{u}$是对偶的，所以loss函数对$x_{w}$的偏导也便是与上面对偶的形式，只不过括号外面是乘以$\theta^{u}$。</p>
<p>梯度公式2：</p>
<p>既然分别都得到偏导之后呢，我们如果去更新呢。对于$\theta^{u}$我们根据学习率去对于$\theta^{u}$更新就可以了。但是对于$x_{w}$更新，我们看到由于这里损失函数对$x_{w}$求偏导呢，是与这里的$\theta^{u}$有关系的。这个u我们知道它有可能是中心词，也有可能是负采样所选出来的负样本，所以他是一系列的，我们将这一系列的词，或者说是正负样本对。学习完之后我们得到一个总的梯度。得到这个总的梯度之后呢，是$x_{w}^{T}$的梯度，也就是$x_{w}^{T}$可以去更新它自己。这里$x_{w}^{T}$是所有上下文词向量的加和。这里也采用了上下文的每一个词都共享这个梯度，来更新自己的向量。</p>
<p>这里就是$x_{w}^{T}$对于正负样本对他的梯度的加和。然后我们将上下文中的每一词对应的词向量都以这个梯度去更新。</p>
<h3 id="3、训练的主流程"><a href="#3、训练的主流程" class="headerlink" title="3、训练的主流程"></a>3、训练的主流程</h3><ol>
<li><p>选取中心词w以及负采样出NEG(w)</p>
<p>根据训练的语料，选取中心词w与上下文的词构成的正样本以及负采样选取出的负样本。</p>
</li>
<li><p>分别获得损失函数对于$X_w$和的$\theta^u$梯度</p>
</li>
</ol>
<p>$X_w$：隐含层(投影层)的向量，是上下层向量的一个累加和；</p>
<p>$\theta^u$：正负样本的每一个词都有一个$\theta^u$；</p>
<ol>
<li>更新$\theta^u$以及中心词对应的上下文context(w)中的每一个词的词向量。</li>
</ol>
<p>这里更新的时候需要注意：</p>
<p>以一个实例来说明：</p>
<p>中心词所对应的负样本，假使我们选了5个，加上中心词与上下文组成的正样本，这里一共有6个样本。在$X_w$的梯度的过程当中，实际上是6个梯度的加和，构成了它自己的梯度。在每一词所需要更新的$\theta^u$以及$X_w$的1/6的时候，首先先更新$X_w$的1/6。因为$X_w$是依赖于$\theta^u$的。如果这一次我们将$\theta^u$更新呢，再更新$X_w$的话，就错了。</p>
<p>故需要先更新$X_w$，在更新$\theta^u$。</p>
<h2 id="三、item2vec依赖模型word2vec之skip-gram数学原理介绍"><a href="#三、item2vec依赖模型word2vec之skip-gram数学原理介绍" class="headerlink" title="三、item2vec依赖模型word2vec之skip gram数学原理介绍"></a>三、item2vec依赖模型word2vec之skip gram数学原理介绍</h2><h3 id="1、skip-gram网络结构"><a href="#1、skip-gram网络结构" class="headerlink" title="1、skip gram网络结构"></a>1、skip gram网络结构</h3><p>我们首先来看一下它的网络结构，这里同样有三层构成(输入层、投影层、输出层)。</p>
<p><img src="/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/1564901411917.png" alt="1564901411917"></p>
<p>与CBOW网络结构不同的是，这里的投影层与输入层完成是一样的，也就是说，投影层是W(t)的输入向量。</p>
<p>这里的核心目标是说，当W(t)已知的情况下，去预测它的周围词，我们将这个条件概率最大化就是我们的目标。所以，这里也是有两组参数需要更新的：</p>
<p>（1）W(t)与词典中的每一个词所对应的这种全连接网络，这个参数需要更新；</p>
<p>（2）W(t)本身对应的初始化的向量。比如说我们想要把每个词映射成16维，这个向量也是需要更新的，</p>
<p>这里与之前有两点不同：</p>
<p>（1）这里每一次更新，只能更新W(t)一个词语对应的向量；而CBOW模型一次可以更新4个（4：针对上图）。也就是对应的这个上下文，如果我们上下文的窗口选的更长一点的话，可能会更新的更多一次。</p>
<p>（2）在对W(t)的每一个词进行上下文训练的时候，都需要对输出的词进行一次负采样，来构成训练的负样本。</p>
<h3 id="2、skip-gram的数学公式"><a href="#2、skip-gram的数学公式" class="headerlink" title="2、skip gram的数学公式"></a>2、skip gram的数学公式</h3><ol>
<li>问题抽象</li>
</ol>
<script type="math/tex; mode=display">
G=\prod_{u \in \text { Contert }(w)} \prod_{z \in {u} \cup NEG(u)} p(z | w)</script><p>skip gram是已知中间词，去最大化它相邻词的概率。</p>
<p>举个栗子：”我   是  中国”为例子，这个w就是”是”这个词。这里的z就是”我”或者”中国”他们对应的正样本，以及通过负采样选取的负样本，最大化正样本的输出概率，并且最小化负样本的输出概率，也就是最大化(1-负样本）的输出概率。</p>
<p>与CBOW的不同：CBOW的时候，是选取一次负采样；而这里对于中间词的上下文的每一个词，也就是”我”或者”中国”,每一次都需要进行一个负采样。</p>
<p>下面看一下条件概率：</p>
<script type="math/tex; mode=display">
p(z | w)=\left(\delta\left(v(w)^{T} \theta^{z}\right)\right)^{L^{u}(z)} *\left(1-\delta\left(v(w)^{T} \theta^{z}\right)\right)^{1-L^{u}(z)}</script><p>这个条件概率与之前的CBOW大体形式一样，也就是说当label $L^{w}(u)$=1的时候，我们还是希望最大化这个条件概率。当label $L^{w}(u)$=0的时候（看后半部分），我们需要最大化$(1-\delta\left(v(w)^{T} \theta^{z}\right))$,即最大化1减去模型输出。</p>
<p>这个条件概率与之前的CBOW，不同之处：</p>
<p>（1）隐含层（投影层）输出的是中间词对应的词向量；而CBOW是输出的所有中间词上下文词向量对应的和；</p>
<p>（2）这里的$\theta^{z}$：上下文的词，或者是上下文的词选出来的负样本的词与输出层之间的全连接；目标是中间词$v(w)^{T}$对应的向量以及$\theta^{z}$进行参数学习。进而得到中间词词向量的最佳表示。</p>
<ol>
<li>损失函数<script type="math/tex; mode=display">
\text {Loss}=\sum_{u \in \text {Context}(w)} \sum_{z \in {u} \cup N E G(u)} L^{u}(z) * \log \left(\delta\left(v(w)^{T} \theta^{z}\right)\right)+\left(1-L^{u}(z)\right) * \log \left(1-\delta\left(v(w)^{T} \theta^{z}\right)\right)</script></li>
</ol>
<p>采用log损失函数。将上述的式子log一下，两个连乘，变成了两个连加（之前的乘变成了加）。幂次也可以放到log的前面。</p>
<p>但是，可以发现如果按照这个loss去对$v(w)^t$或者$\theta^z$求偏导，在每一轮迭代的时候，只能够对词向量$v(w)^t$进行一次迭代。这里需要进行上下文窗口次的负采样才能对一个词的词向量进行迭代。显然，效率有些低。</p>
<p>在真正的word2vec实现的时候，需要变换一下思路：</p>
<script type="math/tex; mode=display">
\text {G}=\sum_{w^c \in \text {context}(w)} \sum_{u \in {w} \cup N E G(u)} p(u|w^c)</script><p>同样也是基于像CBOW一样的思想，已知上下文$(w^c)$的情况下，最大化中间词u。但是这里上下文$(w^c)$的每一个词都是独立的，不像CBOW是对上下文中所有的词向量进行累加。</p>
<p>下面重新看一下损失函数：</p>
<script type="math/tex; mode=display">
\text {Loss}=\sum_{w^c \in \text {Context}(w)} \sum_{u \in {w} \cup N E G(u)} L^{w}(u) * \log \left(\delta\left(v(w^c)^{T} \theta^{z}\right)\right)+\left(1-L^{w}(u)\right) * \log \left(1-\delta\left(v(w^c)^{T} \theta^{u}\right)\right)</script><p>这个log损失函数，如果我们忽略掉前半部分的累加。我们只看后面这部分。如果把上下文中的单个的词，变成了$w^x$的话，这一部分的损失函数与上一节讲过的GBOW的损失函数就一样了。也就是说每一次不是用所有上下文的累加和向量来进行梯度的学习。而是对每一个词单独学习梯度，并进行单独更新。实际上梯度形式也比较容易。只是比上一节求出来的多了一次累加。</p>
<ol>
<li>Skip Gram训练主流程</li>
</ol>
<ul>
<li><p>对于中心词上下文词context(w)中的每一个词$w^c$，都需要选取一次负采样，也就是选取词w的正负样本，构造出正负样本；</p>
</li>
<li><p>计算loss对于theta以及$w^c$的偏导；($w^c$指的是我们举例的”我 是  中国”的”我”  或者”中国”)，计算偏导也是有顺序的，像CBOW首先更新loss对于$w^c$的偏导，因为这个偏导是最后我们需要更新词向量偏导的 1/n （n=负采样的数目 + 正样本 + 1）；</p>
</li>
</ul>
<ul>
<li>更新$w^c$对应的词向量；</li>
</ul>
<p>skip gram与CBOW相比，每一次负采样skip gram只能更新一个词对应的词向量；而CBOW在一次负采样，可以更新n（n指窗口）个词。</p>
<ol>
<li>负采样的算法</li>
</ol>
<p>假设词典（训练样本中所有的词）中有n个词，每一个词都会计算出一个长度，这个长度是一个0-1之间的长度，有的短，有的长，所有词的长度加起来的长度=1。</p>
<p><img src="/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/1564945861541.png" alt="1564945861541"></p>
<p>1、每一词的长度计算：</p>
<script type="math/tex; mode=display">
\operatorname{len}(\text {word})=\frac{(\text {counter }(\text {word}))^{\alpha}}{\sum_{w \in D}(\text {counter }(w))^{\alpha}}</script><p>分子：该单词在所有语料中出现的次数；幂次：相当于做了一个平缓；源码中这个平缓是3/4。</p>
<p>分母：语料中（字典中）所有词出现的次数的累加和。</p>
<p>显而易见这个长度是一个0-1之间的长度。而且所有的词的长度累加便是1。这样每一个词都有自己一个值域。比如说这里的w1（词1）可能是0-0.05，w2可能是0.05-0.11。每一个词的值域都是采用前开后闭的区间。</p>
<p>2、然后，初始一个非常大的数，源码中采用10^8，将0-1进行等分。然后每一段都会对应一个词（w1,w2,….,wn）的值域。</p>
<p><img src="/2019/06/01/个性化推荐算法实践第04章基于深度学习的个性化召回算法item2vec/1564946420820.png" alt="1564946420820"></p>
<p>eg：比如这里的m1属于了w1,m2也属于了w1,但是m4和m5属于w2。</p>
<p>在每次进行负采样的过程中，会随机一个0-M之间的数，随机完数字之后，也就知道了随机的哪一个词。eg：随机到了1，那么m1就对应了词1（w1），那么也就是词1。如果我们随机到了4和5，那么m4和m5对应的是w2，那么就是词2是负样本。</p>
<p>注意：如果随机到的词和中心词相同，那么就跳过这次，再进行一次随机。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enfangzhong.github.io/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnFrank">
      <meta itemprop="description" content="你若晴天，我便安好。">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnFrank's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/" itemprop="url">
                  个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-01 13:03:11" itemprop="dateCreated datePublished" datetime="2019-06-01T13:03:11+08:00">2019-06-01</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-18 14:21:28" itemprop="dateModified" datetime="2019-12-18T14:21:28+08:00">2019-12-18</time>
              
            
          </span>


          

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/推荐算法/" itemprop="url" rel="index"><span itemprop="name">推荐算法</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="个性化推荐算法实践第03章基于图推荐的个性化推荐召回算法基于随机游走的Personal-Rank"><a href="#个性化推荐算法实践第03章基于图推荐的个性化推荐召回算法基于随机游走的Personal-Rank" class="headerlink" title="个性化推荐算法实践第03章基于图推荐的个性化推荐召回算法基于随机游走的Personal Rank"></a>个性化推荐算法实践第03章基于图推荐的个性化推荐召回算法基于随机游走的Personal Rank</h1><p>本章节重点介绍一种基于图的个性化推荐召回算法personal rank。从personal rank算法的理论知识与数学原理进行介绍。并结合公开数据集，代码实战personal rank算法的基础版本与矩阵升级版本。</p>
<p><strong>基于图的推荐—基于随机游走的personal rank算法实现</strong></p>
<p>博客第一部分：理论部分主要介绍该算法的背景、物理意义、数学公式推导，以及结合在大数据量实际推荐系统开发工作中为了满足训练速度等方面的要求，对数学公式的矩阵化升级。</p>
<p>博客第二大部分：主要介绍结合第一大部分的数学公式与虚拟log数据为大家编程实战该算法。</p>
<h3 id="1、个性化召回算法Personal-Rank背景与物理意义"><a href="#1、个性化召回算法Personal-Rank背景与物理意义" class="headerlink" title="1、个性化召回算法Personal Rank背景与物理意义"></a>1、个性化召回算法Personal Rank背景与物理意义</h3><p>1、首先介绍基于图的个性化召回算法—personal rank的背景。</p>
<p>（1）用户行为很容易表示为图</p>
<p>图这种数据结构有两个基本的概念—顶点和边。</p>
<p>在实际的个性化推荐系统中，无论是信息流场景、电商场景或者是O2O场景，用户无论是点击、购买、分享、评论等等的行为都是在user和item两个顶点之间搭起了一条连接边，构成了图的基本要素。</p>
<p>实际上这里user与item构成的图是二分图，后面会介绍二分图的概念以及结合具体的例子展示如何将用户行为转换为图。</p>
<p>（2）图推荐在个性化推荐领域效果显著</p>
<p>2、二分图<br>二分图又称为二部图，是图论中的一种特殊模型。设G=(V,E)是一个无向图，如果顶点V可分割为两个互不相交的子集（A,B），并且图中的每条边（i,j）所关联的两个顶点 i 和 j 分别属于这两个不同的顶点集（i in A, j in B）,则称图G为一个二分图。</p>
<p>（如果有一种无向图，它的定点可以分成两个独立的集合，并且互不相交，且所有的边关联顶点，都从属于这个集合。那么这样的图可以称为二分图。）</p>
<p>则，推荐系统中，user、item恰好满足两种独立的集合，并且用户行为总是从user顶点到item顶点集合，所以由推荐系统中user和item之间构成的图就是二分图。</p>
<p>接下来结合具体实例讲解如何将用户的行为转化为二分图。</p>
<p>假设某推荐系统中有4个用户：A B C D，以及从日志（log）中发现对如下item有过行为：</p>
<p><img src="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/1564726845935.png" alt="1564726845935"></p>
<p>即：user A 对 item a、b、d有过行为，userB 对 item a、c有过行为，userC对 item b、e有过行为，userD 对 item c、d有过行为。</p>
<p>首先将user、item分成两组不相交的集合，如下：</p>
<p><img src="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/1564726875995.png" alt="1564726875995"></p>
<p>然后，将所有user 对 item 有过行为的进行连线，就可以得到二分图，如下：</p>
<p><img src="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/1564727091910.png" alt="1564727091910"></p>
<p>此时问题也就抽象出来了，对于userA 来说，item c 和item e哪个更值得推荐？</p>
<p>这里共有5个item，其中userA 已经对item a、b、d有过行为，这里行为是指信息流产品中的点击或者电商产品中的购买等表示user对item喜欢的这种操作。</p>
<p>那么personal rank恰恰是这么一种算法，它能够结合用户行为构成的二分图，对于固定用户对item集合的重要程度给出排序，也就是说将user A 没有对item c 和item e有过行为，但是personal rank算法可以给出item c 和item e对于user A来说，哪个更值得推荐。</p>
<p>下面从物理意义的角度来分析一下，从二分图上如何分析出来item集合对user的重要程度。</p>
<p>3、物理意义<br>（1）两个顶点之间连通的路径数</p>
<p>如果要比较两个item顶点对固定user的重要程度，只需分别看一下user到两个item顶点的路径数，路径数越多的顶点越重要。</p>
<p>（2）两个顶点之间连通的路径长度</p>
<p>同样路径数的情况下，总路径长度越短的顶点越重要。</p>
<p>（3）两个顶点之间连通路径经过顶点的出度</p>
<p>这里解释一下出度的概念：出度是指顶点对外连接边的数目。如user A对item a、b、d有过行为，即为有条连接边，则A的出度为3。如果前两项都相同，则两个item对固定user 的重要程度则比较经过顶点所有的出度和，如果出度和越小则越重要。</p>
<p>结合刚才所举的具体二分图的例子，给大家介绍—对于user A来说，item c 和item e哪个更值得推荐？</p>
<h3 id="2、Personal-Rank算法example解析"><a href="#2、Personal-Rank算法example解析" class="headerlink" title="2、Personal Rank算法example解析"></a>2、Personal Rank算法example解析</h3><p> 例子分析</p>
<p><img src="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/1564729370493.png" alt="1564729370493"></p>
<p>1.分别有几条路径连通？</p>
<p>首先看A-c 之间有几条路径连通：分别是A-a-B-c，A-d-D-c 两条路径连通。</p>
<p>再来看A-e 之间有几条路径连通：A-b-C-e一条路径</p>
<p>从这一角度出发，可以知道 c 比 e 重要。</p>
<p>2.连通路径的长度分别是多少？</p>
<p>首先看A-c 之间有几条路径连通：分别是A-a-B-c，A-d-D-c ，长度都为3</p>
<p>再来看A-e 之间有几条路径连通：A-b-C-e长度为3</p>
<p>3.连通路径的经过顶点出度分别是多少?</p>
<p>首先看A-a-B-c这条路径：A出度是3，a出度是2，B出度是2，c出度是2</p>
<p>再看A-d-D-c这条路径：A出度是3，d出度是2，D出度是2，c出度是2</p>
<p>再看A-b-C-e这条路劲：A出度是3，b出度是2，C出度是2，e出度是1</p>
<p>实例中这里我们物理意义得到的结果。接下来使用程序来完成person Rank算法的时候同样可以得到相同的结论。</p>
<p>虽然 e 的出度和更小，但是由于1中 c 有两条路径，且1的优先级更高，所以还是应该推荐 c。</p>
<h3 id="3、Personal-Rank算法公式解析"><a href="#3、Personal-Rank算法公式解析" class="headerlink" title="3、Personal Rank算法公式解析"></a>3、Personal Rank算法公式解析</h3><p>personal rank是可以通过用户行为划分二分图为固定user得到item重要程度排序的一种算法。</p>
<p>1.算法的文字阐述</p>
<p>随机游走算法PersonalRank实现基于图的推荐对用户A进行个性化推荐，从用户A节点开始在用户-物品二分图random walk，以alpha的概率从A的出边中，等概率选择一条游走过去，到达该顶点后（举例顶点a），由alpha的概率继续从顶点a的出边中，等概率选择一条继续游走到下一个节点，或者（1-alpha）的概率回到顶点A，多次迭代。直到各顶点对于用户A的重要度收敛。</p>
<p><img src="/2019/06/01/个性化推荐算法实践第03章基于图的个性化推荐召回算法PersonalRank/1564729376053.png" alt="1564729376053"></p>
<p>后续我们在实现person rank算法的时候用不同的alpha值来做实验，熟悉是Google的pageRank算法的童鞋们可以发现PageRank与person rank算法有极大的相似性。只不过PageRank算法没有固定的起点。</p>
<p>2.算法的数学公式</p>
<script type="math/tex; mode=display">
PR(v)=\left\{\begin{matrix}
\alpha * \sum_{v^{\sim} \in i n(v)} \frac{P R\left(v^{\sim}\right)}{\left|o u t\left(v^{\sim}\right)\right|} \ldots\left(v !=v_{A}\right) & \\ 
(1-\alpha)+\alpha * \sum_{v^{\sim} \in i n(v)} \frac{P R\left(v^{\sim}\right)}{\left|o u t\left(v^{\sim}\right)\right|} \cdots\left(v=v_{A}\right) & 
\end{matrix}\right.</script><p>把不同item对user的重要程度描述为PR值。</p>
<p>为了便于理解，同样适用A作为固定起点。user A的PR值初始化为1，其余节点的PR值初始化为0。</p>
<p>这里使用 a 节点和 A 节点阐述公式的上半部分和公式的下半部分：</p>
<p>首先看公式的上部分，根据person rank的算法描述，节点a只可能是节点A与节点B,以alpha概率从他们的出边中等概率的选择了与节点a相互连的这条边。</p>
<p>具体来看，从user A出发有3条边，以3条边中等概率的选择了节点a连接的这条边，以1/3的概率选择连接节点a；user B以1/2的概率选择了连接节点a。</p>
<p>结合阐述看一下公式的上半部分：对于不是A节点的PR值，也就是 a 的PR值，那么首先要找到连接该顶点节点，同时分别计算他们PR值得几分之几贡献到要求节点的PR值。那么A将自己PR值得1/3贡献给了 a ，B将自己PR值得1/2贡献给了 a，分别求和，乘alpha，得到 a 的PR值。</p>
<p>接下来看下半部分：如果要求A节点本身的PR值，首先知道任意节点都会以（1-alpha）的概率回到本身，那么对于一些本来就与A节点相连的节点，比如这里的 a 节点或者 b 节点，它们除了以（1-alpha）的概率直接回到A以外；还可以以alpha的概率从自己的出边中等概率的选择与A相邻的这条边，比如这里的 a 节点，可以以1/2的概率选择回到A节点，所以就构成了下半部分的前后两个部分。</p>
<p>经过分析可以发现，personal rank算法求item对固定user的PR值，需要每次迭代在全图范围内迭代，时间复杂度在工业界实际算法落地的时候是不能接受的，所以要让尽可能多的user并行迭代。结合之前许多其他算法训练的工业界实现，很容易想到矩阵化实现，下面看personal rank算法的矩阵化实现。</p>
<p>3.算法抽象—矩阵式</p>
<p>$r=(1-\alpha) r_{0}+\alpha M^{T} r$</p>
<p>$M_{i j}=\frac{1}{|o u t(i)|} j \in \operatorname{out}(i) e l s e 0$</p>
<p>假设这里共有m个user，n个item。</p>
<p>R矩阵是m+n行，1列矩阵，表示其余顶点对该固定顶点的PR值。当然得到了这个，就得到了固定顶点下，其余所有顶点的重要程度排序，这里只需要排出m个user节点。只看n个item节点对该固定顶点的排序。也就得到了该固定顶点下推荐的item。</p>
<p>r0 是m+n行，1列的矩阵，负责选取某一节点是固定节点，它的数值只有1行唯一，其余行全为0。唯一的行，即为选取了该行对应的顶点为固定顶点。那么得到的就是该固定顶点下，其余节点对该固定节点的重要程度的排序。</p>
<p>M 是 m+n行 * m+n列的矩阵，也就是行包含了所有的节点，列也包含了所有的节点。 它是转移矩阵，数值定义如下：1.第一行第二列的数值距离，如果第一行对应的数值顶点由出边连接到了第二列的顶点，那么该值就为第一行顶点的出度的倒数；如果没有连接边，那么就是0。</p>
<p>我们很容易联想到，第一个式子包含了刚才所说的非矩阵化的personal rank的公式的上下两部分。</p>
<script type="math/tex; mode=display">
\left(E-\alpha M^{T}\right) * r=(1-\alpha) r_{0}</script><p>上述公式是本部分中第一个公式，移项、合并同类项之后得到的。</p>
<script type="math/tex; mode=display">
r=\left(E-\alpha M^{T}\right)^{-1}(1-\alpha) r_{0}</script><p>该公式是上一公式两个同时乘以$\left(E-\alpha M^{T}\right)$转置的之后得到的。</p>
<p>刚才说过，r0是m+n行，1列的矩阵，它能够选取固定的顶点，得到固定顶点的推荐结果。如果将r0变为（m+n）*（m+n）的矩阵，也就得到了所有顶点的推荐结果。</p>
<p>由于得到的推荐结果是考虑顶点之间的PR值的顺序关系，并非一个绝对数值，所以可以将$(1-\alpha) $舍去。所以$\left(E-\alpha M^{T}\right)^{-1}$即为所有顶点的推荐结果。每一列表示该顶点下，其余顶点对于该顶点的PR值。</p>
<p>但是，需要注意的是，每一个user能够行为的item毕竟是少数，所以这里的M矩阵是稀疏矩阵，$\left(E-\alpha M^{T}\right)^{-1}$同样也是稀疏矩阵。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>



  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="AnFrank">
            
              <p class="site-author-name" itemprop="name">AnFrank</p>
              <p class="site-description motion-element" itemprop="description">你若晴天，我便安好。</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">40</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">16</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/enfangzhong" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://blog.csdn.net/qintian888" target="_blank" title="CSDN"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://weibo.com/5604852325" target="_blank" title="weibo"><i class="fa fa-fw fa-weibo"></i>weibo</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://twitter.com/anfrankit" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.facebook.com/enfangzhong" target="_blank" title="FB Page"><i class="fa fa-fw fa-facebook"></i>FB Page</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:enfangzhong@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://lruihao.cn/" title="博采众长" target="_blank">博采众长</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.leridy.pw/" title="乐余地" target="_blank">乐余地</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://tmy123.com/" title="同盟源" target="_blank">同盟源</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://yremp.live" title="Yremp" target="_blank">Yremp</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          <marquee>
	<a href="/links">
	<font size="4" face="STXingkai">
		<font color="#FF0000">欢</font>
		<font color="#E80017">迎</font>
		<font color="#D1002E">点</font>
		<font color="#BA0045">我</font>
		<font color="#A3005C">互</font>
		<font color="#A00073">换</font>
		<font color="#A0008A">友</font>
		<font color="#A000A1">链</font>
		<font color="#A000B8">哦</font>
		<font color="#A000CF">~</font>
		<font color="#A000E6">~</font>
	</font>
	</a>
</marquee>

        </div>
      </section>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      


      

<script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
<script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
<div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div id="myCanvasContainer" class="widget tagcloud">
        <canvas width="250" height="250" id="resCanvas" style="width=100%">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JVM/">JVM</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java虚拟机/">Java虚拟机</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/">Redis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/">SQL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spring-Boot/">Spring Boot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spring-Cloud/">Spring Cloud</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java编程思想/">java编程思想</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rabbitmq/">rabbitmq</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/写作标签/">写作标签</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/博客/">博客</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/微服务/">微服务</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/推荐算法/">推荐算法</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/标签云/">标签云</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/消息中间件/">消息中间件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深入理解Java虚拟机/">深入理解Java虚拟机</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/秋招/">秋招</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/自定义友链页面/">自定义友链页面</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/设计模式/">设计模式</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/音乐播放器/">音乐播放器</a><span class="tag-list-count">1</span></li></ul>
        </canvas>
    </div>
</div>




  <!-- canvas粒子时钟 -->
  
<div style="">
  <canvas id="canvas" style="width:60%;">当前浏览器不支持canvas，请更换浏览器后再试</canvas>
</div>
<script>
(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();
</script>





    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AnFrank</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://github.com/enfangzhong">GitHub</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>


<!--
  <div class="theme-info">主题 – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Pisces</a> v6.3.0</div>
-->


  <div class="theme-info">主题 – <a class="theme-link" target="_blank" href="https://github.com/enfangzhong">Enfang</a> v6.3.0</div>





<font color="MediumPurple" face="STLiti">
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共154.1k字|</span>
</div>
</font>

<font color="MediumPurple" face="STLiti"><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span></font>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("05/28/2018 20:01:01");//此处为建站时间 
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "我在此等候你："+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>


<div class="weixin-box">
  <div class="weixin-menu">
    <div class="weixin-hover">
      <div class="weixin-description">微信扫一扫，联系博主</div>
    </div>
  </div>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      我的第<span class="busuanzi-value" id="busuanzi_value_site_uv"></span>位朋友
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      经历<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次回眸才与你相遇
    </span>
  
</div>









        
      </div>
    </footer>

    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.3.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  








  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="/js/src/Valine.min.js"></script>
  // <script src="/js/src/valine1.3.4.js"></script>

  <!-- https://deserts.io/diy-a-comment-system/ -->
  <script type="text/javascript">
    new Valine({
        lang: 'zh-cn',
        admin_email: '944727327@qq.com', //博主邮箱
        el: '#comments' ,
        appId: 'SACSC0biGE5t01ErmVSxYvVt-gzGzoHsz',
        appKey: 'td1GAOLqHNMnQ4BtJENstUVW',
        emoticon_url: 'https://cloud.panjunwen.com/alu',
        emoticon_list: ["吐.png","喷血.png","狂汗.png","不说话.png","汗.png","坐等.png","献花.png","不高兴.png","中刀.png","害羞.png","皱眉.png","小眼睛.png","中指.png","尴尬.png","瞅你.png","想一想.png","中枪.png","得意.png","肿包.png","扇耳光.png","亲亲.png","惊喜.png","脸红.png","无所谓.png","便便.png","愤怒.png","蜡烛.png","献黄瓜.png","内伤.png","投降.png","观察.png","看不见.png","击掌.png","抠鼻.png","邪恶.png","看热闹.png","口水.png","抽烟.png","锁眉.png","装大款.png","吐舌.png","无奈.png","长草.png","赞一个.png","呲牙.png","无语.png","阴暗.png","不出所料.png","咽气.png","期待.png","高兴.png","吐血倒地.png","哭泣.png","欢呼.png","黑线.png","喜极而泣.png","喷水.png","深思.png","鼓掌.png","暗地观察.png"],
        placeholder: 'ヾﾉ≧∀≦)o来啊，快活啊!填写邮箱能很快收到我的回复喔！老铁，你不想说两句吗？',
  });


  // if(window.location.hash){
  //       var checkExist = setInterval(function() {
  //          if ($(window.location.hash).length) {
  //             $('html, body').animate({scrollTop: $(window.location.hash).offset().top-90}, 1000);
  //             clearInterval(checkExist);
  //          }
  //       }, 100);
  //   }

  </script>


  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  
  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('复制成功')
          else $(this).text('复制失败')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('复制')
        }, 300)
      }).append(e)
    })
  </script>



  
   <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;"></canvas> 
   <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
   <script type="text/javascript" src="/js/src/fireworks.js"></script>


<!--知乎卡片链接-->
<script type="text/javascript" src="/js/src/linkcard.js"></script>

</body>
</html>
